{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "374b8e1e",
   "metadata": {},
   "source": [
    "# Model Predictions (qubit_TransmonCross_cap_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e7659a-fe1b-4fdf-8c09-a398e498373b",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9418886-6a3f-4473-ae89-53bab6428eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parameter file is where the hyperparameters are set. \n",
    "# It's reccomended to look at that file first, its interesting and you can set stuff there\n",
    "\n",
    "from parameters import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d17135-58ce-45e4-9c16-1d1b76f34ea3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa89948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable some console warnings\n",
    "import os\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import tensorflow as tf# Disable some console warnings so you can be free of them printing. \n",
    "# Comment the next two lines if you are a professional and like looking at warnings.\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import os, gc\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4ad03d-ae5d-4bc6-b055-3e8579849c5d",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13d0b8c-6699-4caf-b257-abc4f8e49b99",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "667c238f-0e0f-4e4c-b185-f76d1ca261ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all of the nice data you saved from the previous notebook, or downloaded from the drive\n",
    "\n",
    "if DATA_AUGMENTATION:\n",
    "    if 'Try Both' not in ENCODING_TYPE:\n",
    "        encoding = ENCODING_TYPE.replace(' ','_')\n",
    "        X_train = np.load('{}/npy/x_train_{}_encoding_augmented.npy'.format(DATA_DIR, encoding), allow_pickle=True)\n",
    "        X_val = np.load('{}/npy/x_val_{}_encoding_augmented.npy'.format(DATA_DIR, encoding), allow_pickle=True)\n",
    "        X_test = np.load('{}/npy/x_test_{}_encoding_augmented.npy'.format(DATA_DIR, encoding), allow_pickle=True)\n",
    "        y_train = np.load('{}/npy/y_train_{}_encoding_augmented.npy'.format(DATA_DIR, encoding), allow_pickle=True)\n",
    "        y_val = np.load('{}/npy/y_val_{}_encoding_augmented.npy'.format(DATA_DIR, encoding), allow_pickle=True)\n",
    "        y_test = np.load('{}/npy/y_test_{}_encoding_augmented.npy'.format(DATA_DIR, encoding), allow_pickle=True)\n",
    "    \n",
    "    elif 'Try Both' in ENCODING_TYPE:\n",
    "        X_train_one_hot_encoding = np.load('{}/npy/x_train_one_hot_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        X_val_one_hot_encoding = np.load('{}/npy/x_val_one_hot_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        X_test_one_hot_encoding = np.load('{}/npy/x_test_one_hot_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_train_one_hot_encoding = np.load('{}/npy/y_train_one_hot_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_val_one_hot_encoding = np.load('{}/npy/y_val_one_hot_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_test_one_hot_encoding = np.load('{}/npy/y_test_one_hot_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "\n",
    "        X_train_linear_encoding = np.load('{}/npy/x_train_linear_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        X_val_linear_encoding = np.load('{}/npy/x_val_linear_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        X_test_linear_encoding = np.load('{}/npy/x_test_linear_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_train_linear_encoding = np.load('{}/npy/y_train_linear_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_val_linear_encoding = np.load('{}/npy/y_val_linear_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_test_linear_encoding = np.load('{}/npy/y_test_linear_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "\n",
    "else:\n",
    "    if 'Try Both' not in ENCODING_TYPE:\n",
    "        X_train = np.load('{}/npy/x_train_{}_encoding.npy'.format(DATA_DIR, encoding), allow_pickle=True)\n",
    "        X_val = np.load('{}/npy/x_val_{}_encoding.npy'.format(DATA_DIR, encoding), allow_pickle=True)\n",
    "        X_test = np.load('{}/npy/x_test_{}_encoding.npy'.format(DATA_DIR, encoding), allow_pickle=True)\n",
    "        y_train = np.load('{}/npy/y_train_{}_encoding.npy'.format(DATA_DIR, encoding), allow_pickle=True)\n",
    "        y_val = np.load('{}/npy/y_val_{}_encoding.npy'.format(DATA_DIR, encoding), allow_pickle=True)\n",
    "        y_test = np.load('{}/npy/y_test_{}_encoding.npy'.format(DATA_DIR, encoding), allow_pickle=True)\n",
    "    \n",
    "    elif 'Try Both' in ENCODING_TYPE:\n",
    "        X_train_one_hot_encoding = np.load('{}/npy/x_train_one_hot_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        X_val_one_hot_encoding = np.load('{}/npy/x_val_one_hot_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        X_test_one_hot_encoding = np.load('{}/npy/x_test_one_hot_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_train_one_hot_encoding = np.load('{}/npy/y_train_one_hot_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_val_one_hot_encoding = np.load('{}/npy/y_val_one_hot_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_test_one_hot_encoding = np.load('{}/npy/y_test_one_hot_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "\n",
    "        X_train_linear_encoding = np.load('{}/npy/x_train_linear_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        X_val_linear_encoding = np.load('{}/npy/x_val_linear_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        X_test_linear_encoding = np.load('{}/npy/x_test_linear_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_train_linear_encoding = np.load('{}/npy/y_train_linear_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_val_linear_encoding = np.load('{}/npy/y_val_linear_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_test_linear_encoding = np.load('{}/npy/y_test_linear_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ec9efd-ee93-429d-95b4-4e6efef296db",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd39ffbc-5524-4ce0-bcc1-4fd67727e652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide which model file & test set to use\n",
    "chosen_path = \"model/mlp_2_1000_3800_300_400_3_best_model.keras\"      \n",
    "X_test_cur = np.asarray(X_test)\n",
    "y_test_cur = np.asarray(y_test)\n",
    "y_encoding_format_name = encoding    \n",
    "\n",
    "# Load y headers for labeling columns\n",
    "y_headers_csv = f'y_characteristics_{y_encoding_format_name}_encoding.csv'\n",
    "with open(y_headers_csv, 'r') as f:\n",
    "    headers = f.readline().strip().split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66a01e10-6cc5-4940-8091-8dadea6062ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "—— mlp_2_1000_3800_300_400_3_best_model.keras ——\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ fc0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_relu0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ fc1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3800</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,803,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_relu1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3800</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3800</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ fc2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,140,300</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_relu2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ fc3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">120,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_relu3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ fc_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,203</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ fc0 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)           │         \u001b[38;5;34m3,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_relu0 (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout0 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ fc1 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3800\u001b[0m)           │     \u001b[38;5;34m3,803,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_relu1 (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3800\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout1 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3800\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ fc2 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │     \u001b[38;5;34m1,140,300\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_relu2 (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout2 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ fc3 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m)            │       \u001b[38;5;34m120,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_relu3 (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout3 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ fc_output (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │         \u001b[38;5;34m1,203\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,068,703</span> (19.34 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,068,703\u001b[0m (19.34 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,068,703</span> (19.34 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,068,703\u001b[0m (19.34 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: 291 | Targets dim: 3\n"
     ]
    }
   ],
   "source": [
    "#run on CPU\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "try:\n",
    "    tf.config.experimental.reset_memory_stats('GPU:0')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    chosen_model = load_model(chosen_path, compile=False)  #dont compile it because we just need to predict\n",
    "    y_pred = chosen_model.predict(X_test_cur, verbose=0)\n",
    "\n",
    "print(f\"\\n—— {os.path.basename(chosen_path)} ——\")\n",
    "chosen_model.summary()\n",
    "print(f\"Samples: {len(X_test_cur)} | Targets dim: {y_test_cur.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6e42f1-a716-4df1-b092-386ca784f727",
   "metadata": {},
   "source": [
    "# Scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bee83ea-6fdb-484e-9dae-e2d619708fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved CSV -> /home/abouzahr/ML_qubit_design/model_predict_qubit_TransmonCross_cap_matrix/predictions_and_errors_one_hot.csv\n",
      "\n",
      "— Sample 0 — X: qubit_frequency_GHz=0.229409493, anharmonicity_MHz=0.858337674\n",
      "                                                param      ref     pred  abs_error  sq_error\n",
      "   design_options.connection_pads.readout.claw_length 0.333333 0.281250   0.052083  0.002713\n",
      "design_options.connection_pads.readout.ground_spacing 0.152542 0.484131   0.331588  0.109951\n",
      "                          design_options.cross_length 0.515152 0.543945   0.028794  0.000829\n",
      "\n",
      "— Sample 1 — X: qubit_frequency_GHz=0.189596398, anharmonicity_MHz=0.886035915\n",
      "                                                param      ref     pred  abs_error  sq_error\n",
      "   design_options.connection_pads.readout.claw_length 0.090909 0.314209   0.223300  0.049863\n",
      "design_options.connection_pads.readout.ground_spacing 1.000000 0.484863   0.515137  0.265366\n",
      "                          design_options.cross_length 0.606061 0.614258   0.008197  0.000067\n",
      "\n",
      "— Sample 2 — X: qubit_frequency_GHz=0.467937738, anharmonicity_MHz=0.662520901\n",
      "                                                param      ref     pred  abs_error     sq_error\n",
      "   design_options.connection_pads.readout.claw_length 0.121212 0.145508   0.024296 5.902806e-04\n",
      "design_options.connection_pads.readout.ground_spacing 0.000000 0.471924   0.471924 2.227121e-01\n",
      "                          design_options.cross_length 0.272727 0.272217   0.000510 2.605856e-07\n",
      "\n",
      "Global scaled error stats:\n",
      "  min abs_error: 1.4796401515138058e-05\n",
      "  median abs_error: 0.13827607125946967\n",
      "  max abs_error: 0.5334472656249999\n",
      "\n",
      "Here onehot/linear encoding and the MLP which maps categorical data to 1s and 0s is probably throwing off the global average. These will be rounded in the future and will probably always  round to the right number to reconstruct the correct category-- but for now it might throw off  the overall average error. In the future we might want to just have it consider the non-categorical data when finding an overall average and reporting that number.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#use a smaller view if you want\n",
    "N_SAMPLES_TO_SHOW = 3\n",
    "\n",
    "n_samples = min(N_SAMPLES_TO_SHOW, len(X_test_cur))\n",
    "n_params  = y_test_cur.shape[1]\n",
    "\n",
    "# scaled errors\n",
    "sq_errors  = (y_test_cur - y_pred) ** 2\n",
    "abs_errors = np.abs(y_test_cur - y_pred)\n",
    "\n",
    "# scaled dataframe\n",
    "rows = []\n",
    "for i in range(n_samples):\n",
    "    qubit_frequency_GHz, anharmonicity_MHz = X_test_cur[i, 0], X_test_cur[i, 1]\n",
    "    for j in range(n_params):\n",
    "        rows.append({\n",
    "            \"sample_idx\": i,\n",
    "            \"qubit_frequency_GHz\": qubit_frequency_GHz,\n",
    "            \"anharmonicity_MHz\": anharmonicity_MHz,\n",
    "            \"param\": headers[j],\n",
    "            \"ref\":  y_test_cur[i, j],\n",
    "            \"pred\": y_pred[i, j],\n",
    "            \"abs_error\": abs_errors[i, j],\n",
    "            \"sq_error\":  sq_errors[i, j],\n",
    "        })\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# save scaled predictions\n",
    "out_csv = Path(f\"predictions_and_errors_{y_encoding_format_name}.csv\")\n",
    "df.to_csv(out_csv, index=False, float_format=\"%.6g\")\n",
    "print(f\"\\nSaved CSV -> {out_csv.resolve()}\\n\")\n",
    "\n",
    "#pretty print per-sample (scaled)\n",
    "for i in range(n_samples):\n",
    "    sub = df[df[\"sample_idx\"] == i].copy()\n",
    "    sub = sub[[\"param\", \"ref\", \"pred\", \"abs_error\", \"sq_error\"]]\n",
    "    header_line = (\n",
    "        f\"— Sample {i} — \"\n",
    "        f\"X: qubit_frequency_GHz={X_test_cur[i,0]:.9g}, anharmonicity_MHz={X_test_cur[i,1]:.9g}\"\n",
    "    )\n",
    "    print(header_line)\n",
    "    print(sub.to_string(index=False))\n",
    "    print()\n",
    "\n",
    "#print global stats (scaled)\n",
    "print(\"Global scaled error stats:\")\n",
    "print(\"  min abs_error:\", float(abs_errors.min()))\n",
    "print(\"  median abs_error:\", float(np.median(abs_errors)))\n",
    "print(\"  max abs_error:\", float(abs_errors.max()))\n",
    "print(\"\\nHere onehot/linear encoding and the MLP which maps categorical data to 1s and 0s is probably throwing off the global average. These will be rounded in the future and will probably always  round to the right number to reconstruct the correct category-- but for now it might throw off  the overall average error. In the future we might want to just have it consider the non-categorical data when finding an overall average and reporting that number.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375b3b8b-8dc3-4512-9363-14e32cabeea7",
   "metadata": {},
   "source": [
    "# Unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3f3bc71-1fbe-4438-bcd9-efac2dc55911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved CSV -> /home/abouzahr/ML_qubit_design/model_predict_qubit_TransmonCross_cap_matrix/predictions_and_errors_unscaled_one_hot.csv\n",
      "\n",
      "— Sample 0 (Unscaled) — X: qubit_frequency_GHz=4.11507752, anharmonicity_MHz=-150.844392\n",
      "                                                param  ref_unscaled  pred_unscaled  abs_error_unscaled  sq_error_unscaled\n",
      "   design_options.connection_pads.readout.claw_length      0.000180       0.000163            0.000017       2.944694e-10\n",
      "design_options.connection_pads.readout.ground_spacing      0.000005       0.000007            0.000002       3.895663e-12\n",
      "                          design_options.cross_length      0.000260       0.000269            0.000009       8.860446e-11\n",
      "\n",
      "— Sample 1 (Unscaled) — X: qubit_frequency_GHz=3.95948584, anharmonicity_MHz=-138.74412\n",
      "                                                param  ref_unscaled  pred_unscaled  abs_error_unscaled  sq_error_unscaled\n",
      "   design_options.connection_pads.readout.claw_length       0.00010       0.000174            0.000074       5.429912e-09\n",
      "design_options.connection_pads.readout.ground_spacing       0.00001       0.000007            0.000003       9.158229e-12\n",
      "                          design_options.cross_length       0.00029       0.000293            0.000003       7.717368e-12\n",
      "\n",
      "— Sample 2 (Unscaled) — X: qubit_frequency_GHz=5.04725852, anharmonicity_MHz=-236.389034\n",
      "                                                param  ref_unscaled  pred_unscaled  abs_error_unscaled  sq_error_unscaled\n",
      "   design_options.connection_pads.readout.claw_length      0.000110       0.000118        8.017197e-06       6.427544e-11\n",
      "design_options.connection_pads.readout.ground_spacing      0.000004       0.000007        2.814139e-06       7.919377e-12\n",
      "                          design_options.cross_length      0.000180       0.000180        1.131821e-07       1.281018e-14\n",
      "\n",
      "Global unscaled error stats:\n",
      "  min abs_error: 1.1318206787110509e-07\n",
      "  median abs_error: 5.224075317382799e-06\n",
      "  max abs_error: 0.00015899955749511718\n",
      "\n",
      "Here onehot/linear encoding and the MLP which maps categorical data to 1s and 0s is probably throwing off the global average. These will be rounded in the future and will probably always  round to the right number to reconstruct the correct category-- but for now it might throw off  the overall average error. In the future we might want to just have it consider the non-categorical data when finding an overall average and reporting that number.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load X feature names for the X scalers\n",
    "with open('X_names', 'r') as f:\n",
    "    X_index_names = f.read().splitlines()\n",
    "\n",
    "#unscale X\n",
    "X_test_unscaled = np.asarray(X_test_cur.copy())\n",
    "for i in range(X_test_unscaled.shape[0]):\n",
    "    for j in range(X_test_unscaled.shape[1]):\n",
    "        scaler = joblib.load(f'scalers/scaler_X_{X_index_names[j]}.save')\n",
    "        X_test_unscaled[i, j] = scaler.inverse_transform([[X_test_unscaled[i, j]]])[0][0]\n",
    "\n",
    "#unscale y (refs and preds)\n",
    "y_test_unscaled = np.asarray(y_test_cur.copy())\n",
    "y_pred_unscaled = np.asarray(y_pred.copy())\n",
    "for i in range(y_test_unscaled.shape[0]):\n",
    "    for j in range(y_test_unscaled.shape[1]):\n",
    "        scaler = joblib.load(f'scalers/scaler_y_{headers[j]}_{y_encoding_format_name}_encoding.save')\n",
    "        y_test_unscaled[i, j] = scaler.inverse_transform([[y_test_unscaled[i, j]]])[0][0]\n",
    "        y_pred_unscaled[i, j] = scaler.inverse_transform([[y_pred_unscaled[i, j]]])[0][0]\n",
    "\n",
    "# Errors (unscaled)\n",
    "sq_errors_unscaled  = (y_test_unscaled - y_pred_unscaled) ** 2\n",
    "abs_errors_unscaled = np.abs(y_test_unscaled - y_pred_unscaled)\n",
    "\n",
    "#build dataframe (unscaled)\n",
    "rows_unscaled = []\n",
    "for i in range(min(N_SAMPLES_TO_SHOW, len(X_test_unscaled))):\n",
    "    qubit_frequency_GHz, anharmonicity_MHz = X_test_unscaled[i, 0], X_test_unscaled[i, 1]\n",
    "    for j in range(n_params):\n",
    "        rows_unscaled.append({\n",
    "            \"sample_idx\": i,\n",
    "           \"qubit_frequency_GHz\": qubit_frequency_GHz,\n",
    "            \"anharmonicity_MHz\": anharmonicity_MHz,\n",
    "            \"param\": headers[j],\n",
    "            \"ref_unscaled\":  y_test_unscaled[i, j],\n",
    "            \"pred_unscaled\": y_pred_unscaled[i, j],\n",
    "            \"abs_error_unscaled\": abs_errors_unscaled[i, j],\n",
    "            \"sq_error_unscaled\":  sq_errors_unscaled[i, j],\n",
    "        })\n",
    "df_unscaled = pd.DataFrame(rows_unscaled)\n",
    "\n",
    "# save (unscaled)\n",
    "out_csv_unscaled = Path(f\"predictions_and_errors_unscaled_{y_encoding_format_name}.csv\")\n",
    "df_unscaled.to_csv(out_csv_unscaled, index=False, float_format=\"%.6g\")\n",
    "print(f\"\\nSaved CSV -> {out_csv_unscaled.resolve()}\\n\")\n",
    "\n",
    "# Pretty print per-sample (unscaled)\n",
    "for i in range(min(N_SAMPLES_TO_SHOW, len(X_test_unscaled))):\n",
    "    sub = df_unscaled[df_unscaled[\"sample_idx\"] == i].copy()\n",
    "    sub = sub[[\"param\", \"ref_unscaled\", \"pred_unscaled\", \"abs_error_unscaled\", \"sq_error_unscaled\"]]\n",
    "    header_line = (\n",
    "        f\"— Sample {i} (Unscaled) — \"\n",
    "        f\"X: qubit_frequency_GHz={X_test_unscaled[i,0]:.9g}, anharmonicity_MHz={X_test_unscaled[i,1]:.9g}\"\n",
    "    )\n",
    "    print(header_line)\n",
    "    print(sub.to_string(index=False))\n",
    "    print()\n",
    "\n",
    "#print global stats (unscaled)\n",
    "print(\"Global unscaled error stats:\")\n",
    "print(\"  min abs_error:\", float(abs_errors_unscaled.min()))\n",
    "print(\"  median abs_error:\", float(np.median(abs_errors_unscaled)))\n",
    "print(\"  max abs_error:\", float(abs_errors_unscaled.max()))\n",
    "print(\"\\nHere onehot/linear encoding and the MLP which maps categorical data to 1s and 0s is probably throwing off the global average. These will be rounded in the future and will probably always  round to the right number to reconstruct the correct category-- but for now it might throw off  the overall average error. In the future we might want to just have it consider the non-categorical data when finding an overall average and reporting that number.\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
