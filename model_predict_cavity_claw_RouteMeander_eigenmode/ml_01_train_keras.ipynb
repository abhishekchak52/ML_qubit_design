{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "374b8e1e",
   "metadata": {},
   "source": [
    "# Model Training (cavity_claw_RouteMeander_eigenmode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e7659a-fe1b-4fdf-8c09-a398e498373b",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9418886-6a3f-4473-ae89-53bab6428eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parameter file is where the hyperparameters are set. \n",
    "# It's reccomended to look at that file first, its interesting and you can set stuff there\n",
    "\n",
    "from parameters import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d17135-58ce-45e4-9c16-1d1b76f34ea3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa89948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, joblib\n",
    "\n",
    "# Disable some console warnings so you can be free of them printing. \n",
    "# Comment the next two lines if you are a professional and like looking at warnings.\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "# os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "os.environ.pop(\"TF_XLA_FLAGS\", None)      # disable XLA \n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"  # show warnings/errors while debugging\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "\n",
    "import tensorflow as tf, gc\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for g in gpus:\n",
    "    tf.config.experimental.set_memory_growth(g, True)\n",
    "\n",
    "tf.keras.backend.set_floatx(\"float32\") # make the backend use float32 which will be the same as the data--helps speed it up\n",
    "\n",
    "\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Dropout, LeakyReLU\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from keras_tuner import HyperModel, RandomSearch\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Dropout, LeakyReLU\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b35ed7bf-9c4f-41d4-8652-1fe11dd8c013",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "\n",
    "# Input seed value. If this value is the same, the random number generator \n",
    "# will generate the same set of random values every time. We like reproducibility:)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Set the seed value for reproducibility in tensorflow\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c9e65e-f247-4388-a274-6041e8cdcc27",
   "metadata": {},
   "source": [
    "## Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17dc079f-6430-41bc-8342-0cc5ffbe4a8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10278896956963876842\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 40298217472\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 891141395771909798\n",
      "physical_device_desc: \"device: 0, name: NVIDIA A100 80GB PCIe MIG 4g.40gb, pci bus id: 0000:00:10.0, compute capability: 8.0\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1768594246.551703  143622 gpu_process_state.cc:208] Using CUDA malloc Async allocator for GPU: 0\n",
      "I0000 00:00:1768594246.551985  143622 gpu_device.cc:2020] Created device /device:GPU:0 with 38431 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe MIG 4g.40gb, pci bus id: 0000:00:10.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# Look at what you are working with. If you dont have a nice GPU I highly reccomend finding one\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4ad03d-ae5d-4bc6-b055-3e8579849c5d",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13d0b8c-6699-4caf-b257-abc4f8e49b99",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "667c238f-0e0f-4e4c-b185-f76d1ca261ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all of the nice data you saved from the previous notebook, or downloaded from the drive\n",
    "\n",
    "if DATA_AUGMENTATION:\n",
    "    if 'Try Both' not in ENCODING_TYPE:\n",
    "        encoding = ENCODING_TYPE.replace(' ','_')\n",
    "        if 'one hot' in ENCODING_TYPE:\n",
    "            X_train = np.load('{}/npy/x_train_one_hot_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "            X_val = np.load('{}/npy/x_val_one_hot_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "            X_test = np.load('{}/npy/x_test_one_hot_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "\n",
    "            y_value_train = np.load('{}/npy/y_value_train_one_hot_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "            y_value_val = np.load('{}/npy/y_value_val_one_hot_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "            y_value_test = np.load('{}/npy/y_value_test_one_hot_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "\n",
    "            y_exists_train = np.load('{}/npy/y_exists_train_one_hot_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "            y_exists_val = np.load('{}/npy/y_exists_val_one_hot_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "            y_exists_test = np.load('{}/npy/y_exists_test_one_hot_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "\n",
    "        elif 'linear' in ENCODING_TYPE:\n",
    "            X_train = np.load('{}/npy/x_train_linear_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "            X_val = np.load('{}/npy/x_val_linear_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "            X_test = np.load('{}/npy/x_test_linear_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "\n",
    "            y_value_train = np.load('{}/npy/y_value_train_linear_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "            y_value_val = np.load('{}/npy/y_value_val_linear_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "            y_value_test = np.load('{}/npy/y_value_test_linear_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "\n",
    "            y_exists_train = np.load('{}/npy/y_exists_train_linear_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "            y_exists_val = np.load('{}/npy/y_exists_val_linear_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "            y_exists_test = np.load('{}/npy/y_exists_test_linear_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "\n",
    "    elif 'Try Both' in ENCODING_TYPE:\n",
    "        # one-hot branch\n",
    "        X_train_one_hot_encoding = np.load('{}/npy/x_train_one_hot_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        X_val_one_hot_encoding = np.load('{}/npy/x_val_one_hot_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        X_test_one_hot_encoding = np.load('{}/npy/x_test_one_hot_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "\n",
    "        y_value_train_one_hot_encoding = np.load('{}/npy/y_value_train_one_hot_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_value_val_one_hot_encoding = np.load('{}/npy/y_value_val_one_hot_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_value_test_one_hot_encoding = np.load('{}/npy/y_value_test_one_hot_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "\n",
    "        y_exists_train_one_hot_encoding = np.load('{}/npy/y_exists_train_one_hot_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_exists_val_one_hot_encoding = np.load('{}/npy/y_exists_val_one_hot_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_exists_test_one_hot_encoding = np.load('{}/npy/y_exists_test_one_hot_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "\n",
    "        # linear branch\n",
    "        X_train_linear_encoding = np.load('{}/npy/x_train_linear_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        X_val_linear_encoding = np.load('{}/npy/x_val_linear_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        X_test_linear_encoding = np.load('{}/npy/x_test_linear_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "\n",
    "        y_value_train_linear_encoding = np.load('{}/npy/y_value_train_linear_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_value_val_linear_encoding = np.load('{}/npy/y_value_val_linear_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_value_test_linear_encoding = np.load('{}/npy/y_value_test_linear_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "\n",
    "        y_exists_train_linear_encoding = np.load('{}/npy/y_exists_train_linear_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_exists_val_linear_encoding = np.load('{}/npy/y_exists_val_linear_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_exists_test_linear_encoding = np.load('{}/npy/y_exists_test_linear_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "\n",
    "else:\n",
    "    if 'Try Both' not in ENCODING_TYPE:\n",
    "        if 'one hot' in ENCODING_TYPE:\n",
    "            X_train = np.load('{}/npy/x_train_one_hot_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "            X_val = np.load('{}/npy/x_val_one_hot_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "            X_test = np.load('{}/npy/x_test_one_hot_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "\n",
    "            y_value_train = np.load('{}/npy/y_value_train_one_hot_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "            y_value_val = np.load('{}/npy/y_value_val_one_hot_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "            y_value_test = np.load('{}/npy/y_value_test_one_hot_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "\n",
    "            y_exists_train = np.load('{}/npy/y_exists_train_one_hot_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "            y_exists_val = np.load('{}/npy/y_exists_val_one_hot_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "            y_exists_test = np.load('{}/npy/y_exists_test_one_hot_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "\n",
    "        elif 'linear' in ENCODING_TYPE:\n",
    "            X_train = np.load('{}/npy/x_train_linear_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "            X_val = np.load('{}/npy/x_val_linear_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "            X_test = np.load('{}/npy/x_test_linear_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "\n",
    "            y_value_train = np.load('{}/npy/y_value_train_linear_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "            y_value_val = np.load('{}/npy/y_value_val_linear_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "            y_value_test = np.load('{}/npy/y_value_test_linear_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "\n",
    "            y_exists_train = np.load('{}/npy/y_exists_train_linear_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "            y_exists_val = np.load('{}/npy/y_exists_val_linear_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "            y_exists_test = np.load('{}/npy/y_exists_test_linear_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "\n",
    "    elif 'Try Both' in ENCODING_TYPE:\n",
    "        # one-hot branch\n",
    "        X_train_one_hot_encoding = np.load('{}/npy/x_train_one_hot_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        X_val_one_hot_encoding = np.load('{}/npy/x_val_one_hot_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        X_test_one_hot_encoding = np.load('{}/npy/x_test_one_hot_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "\n",
    "        y_value_train_one_hot_encoding = np.load('{}/npy/y_value_train_one_hot_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_value_val_one_hot_encoding = np.load('{}/npy/y_value_val_one_hot_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_value_test_one_hot_encoding = np.load('{}/npy/y_value_test_one_hot_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "\n",
    "        y_exists_train_one_hot_encoding = np.load('{}/npy/y_exists_train_one_hot_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_exists_val_one_hot_encoding = np.load('{}/npy/y_exists_val_one_hot_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_exists_test_one_hot_encoding = np.load('{}/npy/y_exists_test_one_hot_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "\n",
    "        # linear branch\n",
    "        X_train_linear_encoding = np.load('{}/npy/x_train_linear_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        X_val_linear_encoding = np.load('{}/npy/x_val_linear_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        X_test_linear_encoding = np.load('{}/npy/x_test_linear_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "\n",
    "        y_value_train_linear_encoding = np.load('{}/npy/y_value_train_linear_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_value_val_linear_encoding = np.load('{}/npy/y_value_val_linear_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_value_test_linear_encoding = np.load('{}/npy/y_value_test_linear_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "\n",
    "        y_exists_train_linear_encoding = np.load('{}/npy/y_exists_train_linear_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_exists_val_linear_encoding = np.load('{}/npy/y_exists_val_linear_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_exists_test_linear_encoding = np.load('{}/npy/y_exists_test_linear_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ec9efd-ee93-429d-95b4-4e6efef296db",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcb45684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (851, 2)\n",
      "X_val.shape: (182, 2)\n",
      "y_value_train.shape: (851, 16)\n",
      "y_value_val.shape: (182, 16)\n",
      "y_exists_train.shape: (851, 16)\n",
      "y_exists_val.shape: (182, 16)\n",
      "y_value_train[0]: [0.1773399  0.         0.         1.         1.         1.\n",
      " 0.35714286 1.         0.65217391 0.         0.5        0.\n",
      " 0.         1.         1.         0.        ]\n",
      "y_exists_train[0]: [1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.]\n",
      "y_exists_val[0]: [1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Look at the shapes of training and test sets in case you want to orient yourself\n",
    "\n",
    "if 'Try Both' not in ENCODING_TYPE:\n",
    "    print('X_train.shape:', X_train.shape)\n",
    "    print('X_val.shape:', X_val.shape)\n",
    "    print('y_value_train.shape:', y_value_train.shape)\n",
    "    print('y_value_val.shape:', y_value_val.shape)\n",
    "    print('y_exists_train.shape:', y_exists_train.shape)\n",
    "    print('y_exists_val.shape:', y_exists_val.shape)\n",
    "    print('y_value_train[0]:', y_value_train[0])\n",
    "    print('y_exists_train[0]:', y_exists_train[0])\n",
    "    print('y_exists_val[0]:', y_exists_val[0])\n",
    "\n",
    "else:\n",
    "    print('X_train_linear_encoding.shape:', X_train_linear_encoding.shape)\n",
    "    print('X_val_linear_encoding.shape:', X_val_linear_encoding.shape)\n",
    "    print('y_value_train_linear_encoding.shape:', y_value_train_linear_encoding.shape)\n",
    "    print('y_value_val_linear_encoding.shape:', y_value_val_linear_encoding.shape)\n",
    "    print('y_exists_train_linear_encoding.shape:', y_exists_train_linear_encoding.shape)\n",
    "    print('y_exists_val_linear_encoding.shape:', y_exists_val_linear_encoding.shape)\n",
    "    print('y_value_train_linear_encoding[0]:', y_value_train_linear_encoding[0])\n",
    "    print('y_exists_train_linear_encoding[0]:', y_exists_train_linear_encoding[0])\n",
    "\n",
    "    print('X_train_one_hot_encoding.shape:', X_train_one_hot_encoding.shape)\n",
    "    print('X_val_one_hot_encoding.shape:', X_val_one_hot_encoding.shape)\n",
    "    print('y_value_train_one_hot_encoding.shape:', y_value_train_one_hot_encoding.shape)\n",
    "    print('y_value_val_one_hot_encoding.shape:', y_value_val_one_hot_encoding.shape)\n",
    "    print('y_exists_train_one_hot_encoding.shape:', y_exists_train_one_hot_encoding.shape)\n",
    "    print('y_exists_val_one_hot_encoding.shape:', y_exists_val_one_hot_encoding.shape)\n",
    "    print('y_value_train_one_hot_encoding[0]:', y_value_train_one_hot_encoding[0])\n",
    "    print('y_exists_train_one_hot_encoding[0]:', y_exists_train_one_hot_encoding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8e90f9a-14a8-41e2-ad9a-cfe4e3000d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33514963, 0.00078676],\n",
       "       [0.01809174, 0.15265725],\n",
       "       [0.01032841, 0.14763822],\n",
       "       ...,\n",
       "       [0.05754841, 0.25609945],\n",
       "       [0.57781303, 0.0013409 ],\n",
       "       [0.06168676, 0.26742149]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if 'Try Both' not in ENCODING_TYPE:\n",
    "    display(X_train) #can check this in previous script as well after loading to make sure it matches\n",
    "else:\n",
    "    display(X_train_one_hot_encoding)\n",
    "    display(X_train_linear_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95debf17-6de3-49fc-9064-b01c985c3665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Train set shape x:                851, 69.98%\n",
      "Validation set shape x:           182, 14.97%\n",
      "Test set shape x:                 183, 15.05%\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Train set shape y_value:          851, 69.98%\n",
      "Validation set shape y_value:     182, 14.97%\n",
      "Test set shape y_value:           183, 15.05%\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Look at how it was split and decide if you like the split\n",
    "\n",
    "if 'Try Both' not in ENCODING_TYPE:\n",
    "    total = len(X_train) + len(X_test) + len(X_val)\n",
    "    print('---------------------------------------')\n",
    "    print('Train set shape x:                {}, {:.2f}%'.format(len(X_train), (len(X_train) * 100.) / total))\n",
    "    print('Validation set shape x:           {}, {:.2f}%'.format(len(X_val), (len(X_val) * 100.) / total))\n",
    "    print('Test set shape x:                 {}, {:.2f}%'.format(len(X_test), (len(X_test) * 100.) / total))\n",
    "    print('---------------------------------------')\n",
    "\n",
    "    total = len(y_value_train) + len(y_value_test) + len(y_value_val)\n",
    "    print('---------------------------------------')\n",
    "    print('Train set shape y_value:          {}, {:.2f}%'.format(len(y_value_train), (len(y_value_train) * 100.) / total))\n",
    "    print('Validation set shape y_value:     {}, {:.2f}%'.format(len(y_value_val), (len(y_value_val) * 100.) / total))\n",
    "    print('Test set shape y_value:           {}, {:.2f}%'.format(len(y_value_test), (len(y_value_test) * 100.) / total))\n",
    "    print('---------------------------------------')\n",
    "\n",
    "else:\n",
    "    total = len(X_train_one_hot_encoding) + len(X_test_one_hot_encoding) + len(X_val_one_hot_encoding)\n",
    "    print('---------------------------------------')\n",
    "    print('Train set shape x one_hot_encoding:      {}, {:.2f}%'.format(len(X_train_one_hot_encoding), (len(X_train_one_hot_encoding) * 100.) / total))\n",
    "    print('Validation set shape x one_hot_encoding: {}, {:.2f}%'.format(len(X_val_one_hot_encoding), (len(X_val_one_hot_encoding) * 100.) / total))\n",
    "    print('Test set shape x one_hot_encoding:       {}, {:.2f}%'.format(len(X_test_one_hot_encoding), (len(X_test_one_hot_encoding) * 100.) / total))\n",
    "    print('---------------------------------------')\n",
    "\n",
    "    total = len(y_value_train_one_hot_encoding) + len(y_value_test_one_hot_encoding) + len(y_value_val_one_hot_encoding)\n",
    "    print('---------------------------------------')\n",
    "    print('Train set shape y_value one_hot_encoding:      {}, {:.2f}%'.format(len(y_value_train_one_hot_encoding), (len(y_value_train_one_hot_encoding) * 100.) / total))\n",
    "    print('Validation set shape y_value one_hot_encoding: {}, {:.2f}%'.format(len(y_value_val_one_hot_encoding), (len(y_value_val_one_hot_encoding) * 100.) / total))\n",
    "    print('Test set shape y_value one_hot_encoding:       {}, {:.2f}%'.format(len(y_value_test_one_hot_encoding), (len(y_value_test_one_hot_encoding) * 100.) / total))\n",
    "\n",
    "    total = len(X_train_linear_encoding) + len(X_test_linear_encoding) + len(X_val_linear_encoding)\n",
    "    print('---------------------------------------')\n",
    "    print('Train set shape x linear_encoding:      {}, {:.2f}%'.format(len(X_train_linear_encoding), (len(X_train_linear_encoding) * 100.) / total))\n",
    "    print('Validation set shape x linear_encoding: {}, {:.2f}%'.format(len(X_val_linear_encoding), (len(X_val_linear_encoding) * 100.) / total))\n",
    "    print('Test set shape x linear_encoding:       {}, {:.2f}%'.format(len(X_test_linear_encoding), (len(X_test_linear_encoding) * 100.) / total))\n",
    "    print('---------------------------------------')\n",
    "\n",
    "    total = len(y_value_train_linear_encoding) + len(y_value_test_linear_encoding) + len(y_value_val_linear_encoding)\n",
    "    print('---------------------------------------')\n",
    "    print('Train set shape y_value linear_encoding:      {}, {:.2f}%'.format(len(y_value_train_linear_encoding), (len(y_value_train_linear_encoding) * 100.) / total))\n",
    "    print('Validation set shape y_value linear_encoding: {}, {:.2f}%'.format(len(y_value_val_linear_encoding), (len(y_value_val_linear_encoding) * 100.) / total))\n",
    "    print('Test set shape y_value linear_encoding:       {}, {:.2f}%'.format(len(y_value_test_linear_encoding), (len(y_value_test_linear_encoding) * 100.) / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f6e25c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78b32e05-b5df-4647-8dae-6ee5f8cddcca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Bin the data and look at how its distributed, probably the more random/spread out the better \n",
    "# for training, but this will improve as the database fills out\n",
    "\n",
    "if 'Try Both' not in ENCODING_TYPE:\n",
    "    #--------------------Training Set---------------------\n",
    "    save_encoding = ENCODING_TYPE.replace(' ','_')\n",
    "    \n",
    "    num_cols = X_train.shape[1]\n",
    "    num_rows = math.ceil(num_cols / 3)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_rows, 3, figsize=(10, 3 * num_rows))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    with open('X_names', 'r') as f:\n",
    "        column_labels = f.read().splitlines()\n",
    "    \n",
    "    \n",
    "    for i in range(num_cols):\n",
    "        axes[i].hist(X_train[:, i], bins=30, edgecolor='black')\n",
    "        axes[i].set_title(f'Training Set {column_labels[i]}')\n",
    "        axes[i].set_xlabel(f'{column_labels[i]}')\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "    \n",
    "    # Remove unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/training_set_data_distribution{save_encoding}.pdf')\n",
    "    plt.show()\n",
    "\n",
    "    #--------------------Validation Set---------------------\n",
    "    num_cols = X_val.shape[1]\n",
    "    num_rows = math.ceil(num_cols / 3)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_rows, 3, figsize=(10, 3 * num_rows))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    with open('X_names', 'r') as f:\n",
    "        column_labels = f.read().splitlines()\n",
    "    \n",
    "    \n",
    "    for i in range(num_cols):\n",
    "        axes[i].hist(X_val[:, i], bins=30, edgecolor='black')\n",
    "        axes[i].set_title(f'Validation Set {column_labels[i]}')\n",
    "        axes[i].set_xlabel(f'{column_labels[i]}')\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "    \n",
    "    # Remove unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/validation_set_data_distribution{save_encoding}.pdf')\n",
    "    plt.show()\n",
    "    \n",
    "    #--------------------Test Set---------------------\n",
    "    num_cols = X_test.shape[1]\n",
    "    num_rows = math.ceil(num_cols / 3)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_rows, 3, figsize=(10, 3 * num_rows))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    with open('X_names', 'r') as f:\n",
    "        column_labels = f.read().splitlines()\n",
    "    \n",
    "    \n",
    "    for i in range(num_cols):\n",
    "        axes[i].hist(X_test[:, i], bins=30, edgecolor='black')\n",
    "        axes[i].set_title(f'Test Set {column_labels[i]}')\n",
    "        axes[i].set_xlabel(f'{column_labels[i]}')\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "    \n",
    "    # Remove unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/test_set_data_distribution{save_encoding}.pdf')\n",
    "    plt.show()\n",
    "    \n",
    "else: #just plot linear encoding for now to not get plot overwhelm\n",
    "    #--------------------Training Set---------------------\n",
    "    num_cols = X_train_linear_encoding.shape[1]\n",
    "    num_rows = math.ceil(num_cols / 3)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_rows, 3, figsize=(10, 3 * num_rows))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    with open('X_names', 'r') as f:\n",
    "        column_labels = f.read().splitlines()\n",
    "    \n",
    "    \n",
    "    for i in range(num_cols):\n",
    "        axes[i].hist(X_train_linear_encoding[:, i], bins=30, edgecolor='black')\n",
    "        axes[i].set_title(f'Training Set {column_labels[i]}')\n",
    "        axes[i].set_xlabel(f'{column_labels[i]}')\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "    \n",
    "    # Remove unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/training_set_data_distribution_linear_encoding.pdf')\n",
    "    plt.show()\n",
    "\n",
    "    #--------------------Validation Set---------------------\n",
    "    num_cols = X_val_linear_encoding.shape[1]\n",
    "    num_rows = math.ceil(num_cols / 3)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_rows, 3, figsize=(10, 3 * num_rows))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    with open('X_names', 'r') as f:\n",
    "        column_labels = f.read().splitlines()\n",
    "    \n",
    "    \n",
    "    for i in range(num_cols):\n",
    "        axes[i].hist(X_val_linear_encoding[:, i], bins=30, edgecolor='black')\n",
    "        axes[i].set_title(f'Validation Set {column_labels[i]}')\n",
    "        axes[i].set_xlabel(f'{column_labels[i]}')\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "    \n",
    "    # Remove unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/validation_set_data_distribution_linear_encoding.pdf')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    #--------------------Test Set---------------------\n",
    "    num_cols = X_test_linear_encoding.shape[1]\n",
    "    num_rows = math.ceil(num_cols / 3)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_rows, 3, figsize=(10, 3 * num_rows))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    with open('X_names', 'r') as f:\n",
    "        column_labels = f.read().splitlines()\n",
    "    \n",
    "    for i in range(num_cols):\n",
    "        axes[i].hist(X_test_linear_encoding[:, i], bins=30, edgecolor='black')\n",
    "        axes[i].set_title(f'Test Set {column_labels[i]}')\n",
    "        axes[i].set_xlabel(f'{column_labels[i]}')\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "    \n",
    "    # Remove unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/test_set_data_distribution_linear_encoding.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e8215af-8030-4399-bbc5-b235f2ca3488",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = int(np.ceil(len(X_train) / TRAIN_BATCH_SIZE))\n",
    "LR_DECAY_STEPS = steps_per_epoch * 20   # decay every ~10 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ecc581",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251cf19c",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb4f8c5",
   "metadata": {},
   "source": [
    "Create a classical multi-layer perceptron for regression. Taking some inspiration from [Deep learning-based I-V Global Parameter Extraction for BSIM-CMG](https://www.sciencedirect.com/science/article/pii/S003811012300179X), Solid-State Electronics, Vol. 209, November 2023.\n",
    "\n",
    "The above publication predicted parameters for BSIM, which is a physics model for advanced transistors that is complicated and might be a similar complexity to the physics we are trying to target/map with these SC qubit hamiltonian values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d945bd67-9923-4f7c-8e32-56672a6c3a4a",
   "metadata": {},
   "source": [
    "Reccomended to download a third party app like \"Sleep control Center\" or \"Amphetamine\" to prevent computer from sleeping during the many hour/day long training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a779f125-60c6-470e-8d2c-df4418633746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elementwise_value_loss(loss_name):\n",
    "    name = str(loss_name).lower()\n",
    "\n",
    "    if name in [\"mse\", \"mean_squared_error\", \"mean square error\"]:\n",
    "        def _loss(y_true, y_pred):\n",
    "            y_true = tf.cast(y_true, y_pred.dtype)\n",
    "            return tf.math.squared_difference(y_true, y_pred)  # (N,D)\n",
    "        return _loss\n",
    "\n",
    "    if name in [\"mae\", \"mean_absolute_error\", \"mean absolute error\"]:\n",
    "        def _loss(y_true, y_pred):\n",
    "            y_true = tf.cast(y_true, y_pred.dtype)\n",
    "            return tf.abs(y_true - y_pred)  # (N,D)\n",
    "        return _loss\n",
    "\n",
    "    print(f\"[WARN] TRAIN_LOSS='{loss_name}' not recognized; defaulting to elementwise MSE.\")\n",
    "    def _loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "        return tf.math.squared_difference(y_true, y_pred)\n",
    "    return _loss\n",
    "\n",
    "\n",
    "VALUE_LOSS_FN = elementwise_value_loss(TRAIN_LOSS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc4f485-e3da-4663-832c-3bc917660e45",
   "metadata": {},
   "source": [
    "### Create Model by Hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce671ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not KERAS_TUNER  and not SWEEP_PARAM_NUM and not SWEEP_DATA_AMOUNT:\n",
    "    # n output neurons for n parameters (value and exists heads both use this size)\n",
    "    if 'Try Both' not in ENCODING_TYPE:\n",
    "        # Multilayer perceptron (MLP)\n",
    "        model_shape = f'mlp_{len(X_test[0])}_'\n",
    "        # inner layer sizes\n",
    "        model_shape += '_'.join(str(l) for l in NEURONS_PER_LAYER)\n",
    "        print(len(y_value_train[0]))\n",
    "        model_shape += f'_{len(y_value_train[0])}'\n",
    "    else:\n",
    "        # Multilayer perceptron (MLP) for both encodings\n",
    "        model_shape_one_hot_encoding = f'mlp_{len(X_test_one_hot_encoding[0])}_'\n",
    "        model_shape_linear_encoding = f'mlp_{len(X_test_linear_encoding[0])}_'\n",
    "        model_shape_one_hot_encoding += '_'.join(str(l) for l in NEURONS_PER_LAYER)\n",
    "        model_shape_linear_encoding += '_'.join(str(l) for l in NEURONS_PER_LAYER)\n",
    "\n",
    "        print('one hot:', len(y_value_train_one_hot_encoding[0]))\n",
    "        model_shape_one_hot_encoding += f'_{len(y_value_train_one_hot_encoding[0])}'\n",
    "        print('linear:', len(y_value_train_linear_encoding[0]))\n",
    "        model_shape_linear_encoding += f'_{len(y_value_train_linear_encoding[0])}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85ba881d-a212-402c-86d8-ed5ab2bed357",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not KERAS_TUNER  and not SWEEP_PARAM_NUM and not SWEEP_DATA_AMOUNT:\n",
    "    if 'Try Both' not in ENCODING_TYPE:\n",
    "        #initialize a model, which lets us build a stack of layers\n",
    "        inputs = Input(shape=(len(X_test[0]),), name='input1')\n",
    "        x = inputs\n",
    "\n",
    "        #iterate over the configuration of neurons for each hidden layer specified in NEURONS_PER_LAYER\n",
    "        for i, n in enumerate(NEURONS_PER_LAYER):\n",
    "            # add a fully connected (dense) hidden layer with specified number of neurons\n",
    "            # the LeCun uniform initializer is used when initializing weights, this makes the model more stable\n",
    "            # l2 regularization is used in each layer to penalizing large weights, which prevents overfitting\n",
    "            x = Dense(n, name='fc{}'.format(i), kernel_initializer='lecun_uniform', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "\n",
    "            # apply a Leaky ReLU activation function to the outputs of the dense layer\n",
    "            # this introduces non-linearities, allowing the network to learn complex functions\n",
    "            #leaky ReLU is chosen over standard ReLU to help mitigate the \"dying ReLU\" problem:\n",
    "            #     - this problem is when neurons using the ReLU activation function output zero for all inputs and stop learning\n",
    "            #     - can be mitigated by using variations like Leaky ReLU or proper initialization\n",
    "            x = LeakyReLU(negative_slope=0.01, name='leaky_relu{}'.format(i))(x)\n",
    "            \n",
    "            # add a dropout layer to reduce overfitting -- randomly drops a set fraction (like 30%) of outputs from the layer\n",
    "            x = Dropout(rate=TRAIN_DROPOUT_RATE, name='dropout{}'.format(i))(x)\n",
    "        \n",
    "        # add the output layers consisting of # neurons, corresponding to the # target variables we aim to predict.\n",
    "        # value_out predicts the numerical parameter values; exists_out predicts if each parameter is defined (0/1).\n",
    "        value_out = Dense(len(y_value_train[0]), activation='linear', name='value_out', kernel_initializer='lecun_uniform')(x)\n",
    "        exists_out = Dense(len(y_value_train[0]), activation='sigmoid', name='exists_out', kernel_initializer='lecun_uniform')(x)\n",
    "\n",
    "        model = tf.keras.Model(\n",
    "            inputs=inputs,\n",
    "            outputs={'value_out': value_out, 'exists_out': exists_out},\n",
    "            name='mlp_multi_output'\n",
    "        )\n",
    "\n",
    "    \n",
    "    else:\n",
    "        # One-hot encoding model\n",
    "        inputs_one_hot = Input(shape=(len(X_test_one_hot_encoding[0]),), name='input1')\n",
    "        x_oh = inputs_one_hot\n",
    "        for i, n in enumerate(NEURONS_PER_LAYER):\n",
    "            x_oh = Dense(n, name='fc{}'.format(i), kernel_initializer='lecun_uniform', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x_oh)\n",
    "            x_oh = LeakyReLU(negative_slope=0.01, name='leaky_relu{}'.format(i))(x_oh)\n",
    "            x_oh = Dropout(rate=TRAIN_DROPOUT_RATE, name='dropout{}'.format(i))(x_oh)\n",
    "        value_out_oh = Dense(len(y_value_train_one_hot_encoding[0]), activation='linear', name='value_out', kernel_initializer='lecun_uniform')(x_oh)\n",
    "        exists_out_oh = Dense(len(y_value_train_one_hot_encoding[0]), activation='sigmoid', name='exists_out', kernel_initializer='lecun_uniform')(x_oh)\n",
    "        model_one_hot_encoding = tf.keras.Model(\n",
    "            inputs=inputs_one_hot,\n",
    "            outputs={'value_out': value_out_oh, 'exists_out': exists_out_oh},\n",
    "            name='mlp_multi_output_one_hot'\n",
    "        )\n",
    "\n",
    "        # Linear encoding model\n",
    "        inputs_lin = Input(shape=(len(X_test_linear_encoding[0]),), name='input1')\n",
    "        x_lin = inputs_lin\n",
    "        for i, n in enumerate(NEURONS_PER_LAYER):\n",
    "            x_lin = Dense(n, name='fc{}'.format(i), kernel_initializer='lecun_uniform', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x_lin)\n",
    "            x_lin = LeakyReLU(negative_slope=0.01, name='leaky_relu{}'.format(i))(x_lin)\n",
    "            x_lin = Dropout(rate=TRAIN_DROPOUT_RATE, name='dropout{}'.format(i))(x_lin)\n",
    "        value_out_lin = Dense(len(y_value_train_linear_encoding[0]), activation='linear', name='value_out', kernel_initializer='lecun_uniform')(x_lin)\n",
    "        exists_out_lin = Dense(len(y_value_train_linear_encoding[0]), activation='sigmoid', name='exists_out', kernel_initializer='lecun_uniform')(x_lin)\n",
    "        model_linear_encoding = tf.keras.Model(\n",
    "            inputs=inputs_lin,\n",
    "            outputs={'value_out': value_out_lin, 'exists_out': exists_out_lin},\n",
    "            name='mlp_multi_output_linear'\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0181fd2-e4ee-4e41-aaae-4603c1a853cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not KERAS_TUNER  and not SWEEP_PARAM_NUM and not SWEEP_DATA_AMOUNT:\n",
    "    # The exponential decay learning rate schedule gradually reduces the learning rate, fine-tuning the learning process for better convergence\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=LR_INITIAL,\n",
    "        decay_steps=LR_DECAY_STEPS,\n",
    "        decay_rate=LR_DECAY_RATE,\n",
    "        staircase=LR_STAIRCASE\n",
    "    )\n",
    "    \n",
    "    if 'Try Both' not in ENCODING_TYPE:\n",
    "        # Set model to minimize loss specified by TRAIN_LOSS, and also to report the loss during training\n",
    "        model.compile(\n",
    "            optimizer=tf.optimizers.Adam(learning_rate=lr_schedule),\n",
    "            loss={'value_out': VALUE_LOSS_FN, 'exists_out': 'binary_crossentropy'},\n",
    "            loss_weights={'value_out': 1.0, 'exists_out': 1.0},\n",
    "            metrics={'value_out': [VALUE_LOSS_FN], 'exists_out': ['accuracy']}\n",
    "        )\n",
    "    else:\n",
    "        # Linear encoding model\n",
    "        model_linear_encoding.compile(\n",
    "            optimizer=tf.optimizers.Adam(learning_rate=lr_schedule),\n",
    "            loss={'value_out': VALUE_LOSS_FN, 'exists_out': 'binary_crossentropy'},\n",
    "            loss_weights={'value_out': 1.0, 'exists_out': 1.0},\n",
    "            metrics={'value_out': [VALUE_LOSS_FN], 'exists_out': ['accuracy']}\n",
    "        )\n",
    "        # One-hot encoding model\n",
    "        model_one_hot_encoding.compile(\n",
    "            optimizer=tf.optimizers.Adam(learning_rate=lr_schedule),\n",
    "            loss={'value_out': VALUE_LOSS_FN, 'exists_out': 'binary_crossentropy'},\n",
    "            loss_weights={'value_out': 1.0, 'exists_out': 1.0},\n",
    "            metrics={'value_out': [VALUE_LOSS_FN], 'exists_out': ['accuracy']}\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44e3657d-d0ac-40e6-9829-694436270bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not KERAS_TUNER  and not SWEEP_PARAM_NUM and not SWEEP_DATA_AMOUNT:\n",
    "    !mkdir -p model\n",
    "    if 'Try Both' not in ENCODING_TYPE:\n",
    "        best_model_file = 'model/{}_best_model.keras'.format(model_shape)\n",
    "        last_model_file = 'model/{}_last_model.keras'.format(model_shape)\n",
    "    else:\n",
    "        best_model_file_one_hot_encoding = 'model/{}_best_model_one_hot_encoding.keras'.format(model_shape_one_hot_encoding)\n",
    "        last_model_file_one_hot_encoding = 'model/{}_last_model_one_hot_encoding.keras'.format(model_shape_one_hot_encoding)\n",
    "    \n",
    "        best_model_file_linear_encoding = 'model/{}_best_model_linear_encoding.keras'.format(model_shape_linear_encoding)\n",
    "        last_model_file_linear_encoding = 'model/{}_last_model_linear_encoding.keras'.format(model_shape_linear_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49516260-5078-4162-ab99-cdb45f9f9827",
   "metadata": {},
   "source": [
    "Enable training (`train_and_save`) to overwrite the model file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c95e4501-3aea-4053-9788-30b4532d7f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_save = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2957910-f200-4e9f-91c3-6e7ed0926ceb",
   "metadata": {},
   "source": [
    "We use Adam optimizer, minimize the Mean Squared Logarithmic Error, and early stop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a00680-7b7f-4877-babb-481f0682b7b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96b4fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "# Set up monitors and plots for later tracking purposes\n",
    "\n",
    "if not KERAS_TUNER  and not SWEEP_PARAM_NUM and not SWEEP_DATA_AMOUNT:\n",
    "    class TrainingPlot(tf.keras.callbacks.Callback):\n",
    "         \n",
    "        # This function is called when the training begins\n",
    "        def on_train_begin(self, logs={}):\n",
    "            # Initialize the lists for holding the logs, losses \n",
    "            self.losses = []\n",
    "            self.val_losses = []\n",
    "            self.logs = []\n",
    "        \n",
    "        # This function is called at the end of each epoch\n",
    "        def on_epoch_end(self, epoch, logs={}):\n",
    "            \n",
    "            # Append the logs, losses  to the lists\n",
    "            self.logs.append(logs)\n",
    "            self.losses.append(logs.get('loss'))\n",
    "            self.val_losses.append(logs.get('val_loss'))\n",
    "            \n",
    "            # Before plotting ensure at least 2 epochs have passed\n",
    "            if len(self.losses) > 1:\n",
    "                \n",
    "                # Clear the previous plot\n",
    "                clear_output(wait=True)\n",
    "                N = np.arange(0, len(self.losses))\n",
    "                \n",
    "                # Plot train loss, train acc, val loss and val acc against epochs passed\n",
    "                plt.figure()\n",
    "                plt.plot(N, self.losses, label = \"train_loss\")\n",
    "                plt.plot(N, self.val_losses, label = \"val_loss\")\n",
    "                plt.title(\"Training Loss [Epoch {}]\".format(epoch))\n",
    "                plt.xlabel(\"Epoch #\")\n",
    "                plt.ylabel(\"Loss/Accuracy\")\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "           \n",
    "\n",
    "class LearningRateMonitor(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.learning_rates = []\n",
    "\n",
    "    #we have to do some checking for versions here or else we will get an Adam error when using this monitor\n",
    "    def _current_lr(self, optimizer):\n",
    "        # look and see if you get \"lr\" ir \"learning_rate\" depending on the version\n",
    "        lr = getattr(optimizer, \"lr\", None) or getattr(optimizer, \"learning_rate\", None)\n",
    "\n",
    "        # if this is a shecdule then evaluate it at current iteration step\n",
    "        if isinstance(lr, tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "            return float(lr(optimizer.iterations).numpy())\n",
    "\n",
    "        # if not a schedule, its a scalar/variable/tensor\n",
    "        return float(tf.keras.backend.get_value(lr))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        try:\n",
    "            lr_val = self._current_lr(self.model.optimizer)\n",
    "        except Exception:\n",
    "            # for anything else fallback\n",
    "            lr_val = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        self.learning_rates.append(lr_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef223c28-cc33-4721-b049-be7023eb13ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17 s, sys: 2 s, total: 19 s\n",
      "Wall time: 26.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train the model\n",
    "history = None  \n",
    "if not KERAS_TUNER  and not SWEEP_PARAM_NUM and not SWEEP_DATA_AMOUNT:\n",
    "    if train_and_save: \n",
    "        # Set up early stopping to prevent overfitting by halting training when validation loss stops improving\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_value_out_loss',                      # Monitor validation loss for stopping criteria \n",
    "            mode='min',                              # Stop when the monitored quantity has stopped decreasing\n",
    "            patience=TRAIN_EARLY_STOPPING_PATIENCE,  # Number of epochs to wait after last improvement\n",
    "            verbose=1                                # Enable logging when early stopping happens\n",
    "        )\n",
    "    \n",
    "        # Train the model on the training data and validate on a portion of it\n",
    "        if 'Try Both' not in ENCODING_TYPE:\n",
    "            plot_callback = TrainingPlot()      # Plot training progress\n",
    "            lr_monitor = LearningRateMonitor()  # Watch learning rate changes\n",
    "            \n",
    "            # sample weights: use exists mask for value_out, ones for exists_out\n",
    "            value_sample_weight_train = np.asarray(y_exists_train)\n",
    "            value_sample_weight_val = np.asarray(y_exists_val)  # not used directly but handy to keep\n",
    "            exists_sample_weight_train = np.ones_like(value_sample_weight_train)\n",
    "            \n",
    "            # Set up model checkpointing to save the model at its best validation loss:\n",
    "            model_checkpoint = ModelCheckpoint(\n",
    "                filepath=best_model_file,          \n",
    "                monitor='val_value_out_loss',            # Save the model based on validation loss improvement\n",
    "                mode='min',                    # Favor lower validation loss values for saving (minimize)\n",
    "                save_best_only=True,           # Save only when validation loss improves\n",
    "                verbose=0                      # No logging for model saving\n",
    "            )\n",
    "   \n",
    "            history = model.fit(\n",
    "            np.asarray(X_train),\n",
    "            {\n",
    "                \"value_out\": np.asarray(y_value_train),\n",
    "                \"exists_out\": np.asarray(y_exists_train),\n",
    "            },\n",
    "            sample_weight={\n",
    "                \"value_out\": np.asarray(y_exists_train).astype(\"float32\"),  # (N,) OR (N,16) depending on your value loss setup\n",
    "                \"exists_out\": np.ones((len(y_exists_train),), dtype=\"float32\"),  # <-- MUST be (N,)\n",
    "            },\n",
    "            validation_data=(\n",
    "                np.asarray(X_val),\n",
    "                {\n",
    "                    \"value_out\": np.asarray(y_value_val),\n",
    "                    \"exists_out\": np.asarray(y_exists_val),\n",
    "                },\n",
    "                {\n",
    "                    \"value_out\": np.asarray(y_exists_val).astype(\"float32\"),\n",
    "                    \"exists_out\": np.ones((len(y_exists_val),), dtype=\"float32\"),  # <-- MUST be (N,)\n",
    "                },\n",
    "            ),\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=TRAIN_BATCH_SIZE,\n",
    "            callbacks=[early_stopping, model_checkpoint, plot_callback, lr_monitor],\n",
    "            verbose=1,\n",
    "        )\n",
    "                    \n",
    "\n",
    "\n",
    "            \n",
    "            model.save(last_model_file)  # Save the final model when done training!\n",
    "        \n",
    "        else:\n",
    "            #-----------------------------------------linear--------------------------------------------\n",
    "            plot_callback_linear_encoding = TrainingPlot()      # Plot training progress\n",
    "            lr_monitor_linear_encoding = LearningRateMonitor()  # Watch learning rate changes\n",
    "            \n",
    "            value_sample_weight_train_linear = np.asarray(y_exists_train_linear_encoding)\n",
    "            value_sample_weight_val_linear = np.asarray(y_exists_val_linear_encoding)\n",
    "            exists_sample_weight_train_linear = np.ones_like(value_sample_weight_train_linear)\n",
    "            \n",
    "            # Set up model checkpointing to save the model at its best validation loss:\n",
    "            model_checkpoint_linear_encoding = ModelCheckpoint(\n",
    "                filepath=best_model_file_linear_encoding,          \n",
    "                monitor='val_value_out_loss',            # Save the model based on validation loss improvement\n",
    "                mode='min',                    # Favor lower validation loss values for saving (minimize)\n",
    "                save_best_only=True,           # Save only when validation loss improves\n",
    "                verbose=0                      # No logging for model saving\n",
    "            )\n",
    "            \n",
    "            history_linear_encoding = model_linear_encoding.fit(\n",
    "                np.asarray(X_train_linear_encoding),\n",
    "                [\n",
    "                    np.asarray(y_value_train_linear_encoding),\n",
    "                    np.asarray(y_exists_train_linear_encoding)\n",
    "                ],\n",
    "                epochs=EPOCHS,\n",
    "                batch_size=TRAIN_BATCH_SIZE,\n",
    "                validation_data=(\n",
    "                    np.asarray(X_val_linear_encoding),\n",
    "                    [\n",
    "                        np.asarray(y_value_val_linear_encoding),\n",
    "                        np.asarray(y_exists_val_linear_encoding),\n",
    "                    ],\n",
    "                    [\n",
    "                        np.asarray(y_exists_val_linear_encoding).astype(\"float32\"),   # (N,16) mask for value_out\n",
    "                        np.ones((len(y_exists_val_linear_encoding),), dtype=\"float32\")# (N,) weights for exists_out\n",
    "                    ],\n",
    "                ),\n",
    "\n",
    "                sample_weight=[\n",
    "                    np.asarray(y_exists_train_linear_encoding).astype(\"float32\"),              # (N,16) mask for value_out\n",
    "                    np.ones((len(y_exists_train_linear_encoding),), dtype=\"float32\"),          # (N,)   weights for exists_out\n",
    "                ]\n",
    "\n",
    "                ,\n",
    "                callbacks=[early_stopping, model_checkpoint_linear_encoding, plot_callback_linear_encoding, lr_monitor_linear_encoding],\n",
    "                verbose=1\n",
    "            )\n",
    "\n",
    "\n",
    "            \n",
    "            model_linear_encoding.save(last_model_file_linear_encoding)  # Save the final model when done training!\n",
    "            \n",
    "            #-----------------------------------------one hot--------------------------------------------\n",
    "            plot_callback_one_hot_encoding = TrainingPlot()      # Plot training progress\n",
    "            lr_monitor_one_hot_encoding = LearningRateMonitor()  # Watch learning rate changes\n",
    "            \n",
    "            value_sample_weight_train_oh = np.asarray(y_exists_train_one_hot_encoding)\n",
    "            value_sample_weight_val_oh = np.asarray(y_exists_val_one_hot_encoding)\n",
    "            exists_sample_weight_train_oh = np.ones_like(value_sample_weight_train_oh)\n",
    "            \n",
    "            # Set up model checkpointing to save the model at its best validation loss:\n",
    "            model_checkpoint_one_hot_encoding = ModelCheckpoint(\n",
    "                filepath=best_model_file_one_hot_encoding,          \n",
    "                monitor='val_value_out_loss',            # Save the model based on validation loss improvement\n",
    "                mode='min',                    # Favor lower validation loss values for saving (minimize)\n",
    "                save_best_only=True,           # Save only when validation loss improves\n",
    "                verbose=0                      # No logging for model saving\n",
    "            )\n",
    "            \n",
    "            history_one_hot_encoding = model_one_hot_encoding.fit(\n",
    "                np.asarray(X_train_one_hot_encoding),\n",
    "                {\n",
    "                    'value_out': np.asarray(y_value_train_one_hot_encoding),\n",
    "                    'exists_out': np.asarray(y_exists_train_one_hot_encoding)\n",
    "                },\n",
    "                sample_weight={\n",
    "                    \"value_out\": np.asarray(y_exists_train_one_hot_encoding).astype(\"float32\"),   # (N,16)\n",
    "                    \"exists_out\": np.ones((len(y_exists_train_one_hot_encoding),), dtype=\"float32\"),\n",
    "                },\n",
    "                validation_data=(\n",
    "                    np.asarray(X_val_one_hot_encoding),\n",
    "                    {\n",
    "                        'value_out': np.asarray(y_value_val_one_hot_encoding),\n",
    "                        'exists_out': np.asarray(y_exists_val_one_hot_encoding)\n",
    "                    },\n",
    "                    {\n",
    "                        \"value_out\": np.asarray(y_exists_val_one_hot_encoding).astype(\"float32\"), # (N,16)\n",
    "                        \"exists_out\": np.ones((len(y_exists_val_one_hot_encoding),), dtype=\"float32\"),\n",
    "                    }\n",
    "                ),\n",
    "                epochs=EPOCHS,\n",
    "                batch_size=TRAIN_BATCH_SIZE,\n",
    "                callbacks=[early_stopping, model_checkpoint_one_hot_encoding, plot_callback_one_hot_encoding, lr_monitor_one_hot_encoding],\n",
    "                verbose=1\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            model_one_hot_encoding.save(last_model_file_one_hot_encoding)  # Save the final model when done training!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a4074c-a8ef-4a69-a8cd-5714edb90166",
   "metadata": {},
   "source": [
    "Load the saved best model and use it from now on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2447145e-91b5-4070-aefe-38f8e6bc33de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.saving import load_model\n",
    "\n",
    "if not KERAS_TUNER and not SWEEP_PARAM_NUM and not SWEEP_DATA_AMOUNT:\n",
    "    if 'Try Both' not in ENCODING_TYPE:\n",
    "        model = load_model(best_model_file, compile=False)\n",
    "        model.compile(\n",
    "            optimizer=model.optimizer if hasattr(model, \"optimizer\") else \"adam\",\n",
    "            loss={'value_out': VALUE_LOSS_FN, 'exists_out': 'binary_crossentropy'},\n",
    "            loss_weights={'value_out': 1.0, 'exists_out': 1.0},\n",
    "            metrics={'value_out': [VALUE_LOSS_FN], 'exists_out': ['accuracy']},\n",
    "        )\n",
    "    else:\n",
    "        model_one_hot_encoding = load_model(best_model_file_one_hot_encoding, compile=False)\n",
    "        model_one_hot_encoding.compile(\n",
    "            optimizer=model_one_hot_encoding.optimizer if hasattr(model_one_hot_encoding, \"optimizer\") else \"adam\",\n",
    "            loss={'value_out': VALUE_LOSS_FN, 'exists_out': 'binary_crossentropy'},\n",
    "            loss_weights={'value_out': 1.0, 'exists_out': 1.0},\n",
    "            metrics={'value_out': [VALUE_LOSS_FN], 'exists_out': ['accuracy']},\n",
    "        )\n",
    "\n",
    "        model_linear_encoding = load_model(best_model_file_linear_encoding, compile=False)\n",
    "        model_linear_encoding.compile(\n",
    "            optimizer=model_linear_encoding.optimizer if hasattr(model_linear_encoding, \"optimizer\") else \"adam\",\n",
    "            loss={'value_out': VALUE_LOSS_FN, 'exists_out': 'binary_crossentropy'},\n",
    "            loss_weights={'value_out': 1.0, 'exists_out': 1.0},\n",
    "            metrics={'value_out': [VALUE_LOSS_FN], 'exists_out': ['accuracy']},\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b002f4a5-df05-4ff2-be6d-24f8027ca4ea",
   "metadata": {},
   "source": [
    "### Sweep total number of parameters to find the right range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03c72fea-fbf1-48fc-8e8d-7353f631e405",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SWEEP_PARAM_NUM:\n",
    "    \n",
    "    def build_masked_mlp(neurons_per_layer, input_dim, output_dim):\n",
    "        x_in = Input(shape=(input_dim,), name=\"input1\")\n",
    "        x = x_in\n",
    "\n",
    "        for i, n in enumerate(neurons_per_layer):\n",
    "            x = Dense(\n",
    "                n,\n",
    "                name=f\"fc{i}\",\n",
    "                kernel_initializer=\"lecun_uniform\",\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(1e-5),\n",
    "            )(x)\n",
    "            x = LeakyReLU(negative_slope=0.01, name=f\"leaky_relu{i}\")(x)\n",
    "            x = Dropout(rate=TRAIN_DROPOUT_RATE, name=f\"dropout{i}\")(x)\n",
    "\n",
    "        value_out = Dense(\n",
    "            output_dim,\n",
    "            activation=\"linear\",\n",
    "            name=\"value_out\",\n",
    "            kernel_initializer=\"lecun_uniform\",\n",
    "            dtype=\"float32\",\n",
    "        )(x)\n",
    "\n",
    "        exists_out = Dense(\n",
    "            output_dim,\n",
    "            activation=\"sigmoid\",\n",
    "            name=\"exists_out\",\n",
    "            kernel_initializer=\"lecun_uniform\",\n",
    "            dtype=\"float32\",\n",
    "        )(x)\n",
    "\n",
    "        return Model(inputs=x_in, outputs={\"value_out\": value_out, \"exists_out\": exists_out})\n",
    "\n",
    "    def make_optimizer():\n",
    "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=LR_INITIAL,\n",
    "            decay_steps=LR_DECAY_STEPS,\n",
    "            decay_rate=LR_DECAY_RATE,\n",
    "            staircase=LR_STAIRCASE\n",
    "        )\n",
    "        return tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    def train_one_config(neurons_per_layer, seed=0):\n",
    "        tf.keras.backend.clear_session()\n",
    "        tf.random.set_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        model = build_masked_mlp(\n",
    "            neurons_per_layer=neurons_per_layer,\n",
    "            input_dim=X_train.shape[1],\n",
    "            output_dim=y_value_train.shape[1],\n",
    "        )\n",
    "\n",
    "        bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)  # returns (batch,)\n",
    "        \n",
    "        def elementwise_mae(y_true, y_pred):\n",
    "            # returns (batch, output_dim) so your (batch, output_dim) mask works correctly\n",
    "            return tf.abs(y_true - y_pred)\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=make_optimizer(),\n",
    "            loss={\n",
    "                \"value_out\": elementwise_mae,\n",
    "                \"exists_out\": bce,\n",
    "            },\n",
    "            metrics={\n",
    "                \"value_out\": [tf.keras.metrics.MeanAbsoluteError(name=\"mae\")],\n",
    "                \"exists_out\": [tf.keras.metrics.BinaryAccuracy(name=\"bin_acc\")],\n",
    "            },\n",
    "            # If your Keras supports it, this helps avoid XLA weirdness:\n",
    "            jit_compile=False,\n",
    "        )\n",
    "\n",
    "\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor= 'val_value_out_loss', \n",
    "            mode=\"min\",\n",
    "            patience=TRAIN_EARLY_STOPPING_PATIENCE,\n",
    "            verbose=0,\n",
    "            restore_best_weights=True,\n",
    "        )\n",
    "\n",
    "        Xtr = np.asarray(X_train)\n",
    "        Xva = np.asarray(X_val)\n",
    "\n",
    "        yv_tr = np.asarray(y_value_train)\n",
    "        ye_tr = np.asarray(y_exists_train).astype(\"float32\")\n",
    "\n",
    "        yv_va = np.asarray(y_value_val)\n",
    "        ye_va = np.asarray(y_exists_val).astype(\"float32\")\n",
    "        \n",
    "        history = model.fit(\n",
    "            Xtr,\n",
    "            {\"value_out\": yv_tr, \"exists_out\": ye_tr},\n",
    "            sample_weight={\n",
    "                \"value_out\": ye_tr.astype(\"float32\"),                  # (batch, 16) mask\n",
    "                \"exists_out\": np.ones((len(Xtr),), dtype=\"float32\"),   # (batch,) neutral\n",
    "            },\n",
    "            validation_data=(\n",
    "                Xva,\n",
    "                {\"value_out\": yv_va, \"exists_out\": ye_va},\n",
    "                {\n",
    "                    \"value_out\": ye_va.astype(\"float32\"),\n",
    "                    \"exists_out\": np.ones((len(Xva),), dtype=\"float32\"),\n",
    "                },\n",
    "            ),\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=TRAIN_BATCH_SIZE,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        best_val_loss = float(np.min(history.history[\"val_loss\"]))\n",
    "        best_epoch = int(np.argmin(history.history[\"val_loss\"]) + 1)\n",
    "\n",
    "        return {\n",
    "            \"neurons_per_layer\": tuple(neurons_per_layer),\n",
    "            \"depth\": len(neurons_per_layer),\n",
    "            \"width\": int(neurons_per_layer[0]),\n",
    "            \"total_params\": int(model.count_params()),\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "            \"best_epoch\": best_epoch,\n",
    "        }\n",
    "    configs = []\n",
    "    for depth in [1, 2, 3, 4, 5]:\n",
    "        for width in [4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 5000, 6000]:\n",
    "            configs.append([width] * depth)\n",
    "    for depth in [1, 2]:\n",
    "        for width in [2048, 4096]:\n",
    "            configs.append([width] * depth)\n",
    "\n",
    "    # remove duplicates if any\n",
    "    configs = [list(x) for x in {tuple(c) for c in configs}]\n",
    "\n",
    "    results = []\n",
    "    for cfg in sorted(configs, key=lambda c: (len(c), c[0])):\n",
    "        out = train_one_config(cfg, seed=0)\n",
    "        results.append(out)\n",
    "        print(out)\n",
    "\n",
    "    df = pd.DataFrame(results).sort_values(\"total_params\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "584b9bf8-f2b1-45cd-811f-e0ab2981101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SWEEP_PARAM_NUM:\n",
    "    # save the data\n",
    "\n",
    "    from datetime import datetime\n",
    "    \n",
    "    run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_dir = \"sweep_outputs\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    csv_path = os.path.join(out_dir, f\"sweep_results_{run_id}.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(\"Saved:\", csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d3c698f-d105-46fb-84a6-60ad839aa046",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SWEEP_PARAM_NUM:\n",
    "    plt.figure()\n",
    "    plt.scatter(df[\"total_params\"], df[\"best_val_loss\"])\n",
    "    plt.xscale(\"log\")  # will be helpful if params grow fast\n",
    "    plt.xlabel(\"Total parameters (log scale)\")\n",
    "    plt.ylabel(\"Best val_loss\")\n",
    "    plt.title(\"Best val_loss vs model size\")\n",
    "    plt.savefig('plots/params_vs_loss.png')\n",
    "    plt.show()\n",
    "\n",
    "    df2 = df.copy()\n",
    "    \n",
    "    plt.figure()\n",
    "    sc = plt.scatter(\n",
    "        df2[\"total_params\"],\n",
    "        df2[\"best_val_loss\"],\n",
    "        c=df2[\"depth\"],\n",
    "        s=20 + 10*np.log2(df2[\"width\"]),\n",
    "    )\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlabel(\"Total parameters (log scale)\")\n",
    "    plt.ylabel(\"Best val_loss\")\n",
    "    plt.title(\"Best val_loss vs model size (color=depth, size=width)\")\n",
    "    plt.colorbar(sc, label=\"Depth\")\n",
    "    plt.savefig(\"plots/params_vs_loss_width_color_coded.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48f20c5e-2bb5-4847-a8ee-da10db0e828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SWEEP_PARAM_NUM:\n",
    "    df.to_csv(\"sweep_results.csv\", index=False)\n",
    "\n",
    "    old = pd.read_csv(\"sweep_results.csv\")\n",
    "    combined = pd.concat([old, df], ignore_index=True).drop_duplicates(\n",
    "        subset=[\"neurons_per_layer\"], keep=\"last\"\n",
    "    )\n",
    "    combined.to_csv(\"sweep_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a202f375-0326-4767-96f0-9a5252e5799f",
   "metadata": {},
   "source": [
    "### Sweep amount of data used in training to determine if data amount is limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d64c0f2f-1316-4e29-b09c-6d99ce882cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SWEEP_DATA_AMOUNT:\n",
    "\n",
    "    FIXED_DEPTH = 5\n",
    "    FIXED_WIDTH = 64\n",
    "    FIXED_NEURONS = [FIXED_WIDTH] * FIXED_DEPTH\n",
    "\n",
    "    TRAIN_FRACTIONS = np.linspace(0.3, 1.0, 20)\n",
    "\n",
    "    # avg over many seeds for error bars\n",
    "    SWEEP_SEEDS = [0, 1, 2, 3, 4]\n",
    "\n",
    "    def build_masked_mlp(neurons_per_layer, input_dim, output_dim):\n",
    "        x_in = Input(shape=(input_dim,), name=\"input1\")\n",
    "        x = x_in\n",
    "\n",
    "        for i, n in enumerate(neurons_per_layer):\n",
    "            x = Dense(\n",
    "                n,\n",
    "                name=f\"fc{i}\",\n",
    "                kernel_initializer=\"lecun_uniform\",\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(1e-5),\n",
    "            )(x)\n",
    "            x = LeakyReLU(negative_slope=0.01, name=f\"leaky_relu{i}\")(x)\n",
    "            x = Dropout(rate=TRAIN_DROPOUT_RATE, name=f\"dropout{i}\")(x)\n",
    "\n",
    "        value_out = Dense(\n",
    "            output_dim,\n",
    "            activation=\"linear\",\n",
    "            name=\"value_out\",\n",
    "            kernel_initializer=\"lecun_uniform\",\n",
    "            dtype=\"float32\",\n",
    "        )(x)\n",
    "\n",
    "        exists_out = Dense(\n",
    "            output_dim,\n",
    "            activation=\"sigmoid\",\n",
    "            name=\"exists_out\",\n",
    "            kernel_initializer=\"lecun_uniform\",\n",
    "            dtype=\"float32\",\n",
    "        )(x)\n",
    "\n",
    "        return Model(inputs=x_in, outputs={\"value_out\": value_out, \"exists_out\": exists_out})\n",
    "\n",
    "    def make_optimizer():\n",
    "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=LR_INITIAL,\n",
    "            decay_steps=LR_DECAY_STEPS,\n",
    "            decay_rate=LR_DECAY_RATE,\n",
    "            staircase=LR_STAIRCASE,\n",
    "        )\n",
    "        return tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    def elementwise_mae(y_true, y_pred):\n",
    "        return tf.abs(y_true - y_pred)  \n",
    "\n",
    "    def make_subset(X, y_value, y_exists, frac, seed):\n",
    "        assert 0 < frac <= 1.0\n",
    "        n = len(X)\n",
    "        m = max(1, int(np.floor(frac * n)))\n",
    "        rng = np.random.default_rng(seed)\n",
    "        idx = rng.choice(n, size=m, replace=False)\n",
    "        return X[idx], y_value[idx], y_exists[idx], m\n",
    "\n",
    "    def train_one_fraction(neurons_per_layer, train_frac, seed=0):\n",
    "        tf.keras.backend.clear_session()\n",
    "        tf.random.set_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # Full arrays\n",
    "        Xtr_full = np.asarray(X_train)\n",
    "        Xva = np.asarray(X_val)\n",
    "\n",
    "        yv_tr_full = np.asarray(y_value_train)\n",
    "        ye_tr_full = np.asarray(y_exists_train).astype(\"float32\")\n",
    "\n",
    "        yv_va = np.asarray(y_value_val)\n",
    "        ye_va = np.asarray(y_exists_val).astype(\"float32\")\n",
    "\n",
    "        Xtr, yv_tr, ye_tr, n_sub = make_subset(Xtr_full, yv_tr_full, ye_tr_full, train_frac, seed)\n",
    "\n",
    "        model = build_masked_mlp(\n",
    "            neurons_per_layer=neurons_per_layer,\n",
    "            input_dim=Xtr.shape[1],\n",
    "            output_dim=yv_tr.shape[1],\n",
    "        )\n",
    "\n",
    "        bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=make_optimizer(),\n",
    "            loss={\"value_out\": elementwise_mae, \"exists_out\": bce},\n",
    "            loss_weights={\"value_out\": 1.0, \"exists_out\": 1.0},\n",
    "            metrics={\n",
    "                \"value_out\": [tf.keras.metrics.MeanAbsoluteError(name=\"mae\")],\n",
    "                \"exists_out\": [tf.keras.metrics.BinaryAccuracy(name=\"bin_acc\")],\n",
    "            },\n",
    "            jit_compile=False,\n",
    "        )\n",
    "\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            patience=TRAIN_EARLY_STOPPING_PATIENCE,\n",
    "            verbose=0,\n",
    "            restore_best_weights=True,\n",
    "        )\n",
    "\n",
    "        history = model.fit(\n",
    "            Xtr,\n",
    "            {\"value_out\": yv_tr, \"exists_out\": ye_tr},\n",
    "            sample_weight={\n",
    "                \"value_out\": ye_tr,                                # (batch, output_dim) mask\n",
    "                \"exists_out\": np.ones((len(Xtr),), dtype=\"float32\") # (batch,) neutral\n",
    "            },\n",
    "            validation_data=(\n",
    "                Xva,\n",
    "                {\"value_out\": yv_va, \"exists_out\": ye_va},\n",
    "                {\n",
    "                    \"value_out\": ye_va,                              # (val_batch, output_dim) mask\n",
    "                    \"exists_out\": np.ones((len(Xva),), dtype=\"float32\"),\n",
    "                },\n",
    "            ),\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=TRAIN_BATCH_SIZE,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "        # grab best epoch by val_loss\n",
    "        val_loss_hist = np.asarray(history.history[\"val_loss\"], dtype=float)\n",
    "        best_i = int(np.argmin(val_loss_hist))\n",
    "        best_epoch = best_i + 1\n",
    "\n",
    "        out = {\n",
    "            \"train_frac\": float(train_frac),\n",
    "            \"train_n\": int(n_sub),\n",
    "            \"seed\": int(seed),\n",
    "            \"neurons_per_layer\": str(list(neurons_per_layer)),\n",
    "            \"total_params\": int(model.count_params()),\n",
    "            \"best_val_loss\": float(val_loss_hist[best_i]),\n",
    "            \"best_val_value_out_loss\": float(np.asarray(history.history.get(\"val_value_out_loss\"))[best_i]),\n",
    "            \"best_val_exists_out_loss\": float(np.asarray(history.history.get(\"val_exists_out_loss\"))[best_i]),\n",
    "            \"best_epoch\": int(best_epoch),\n",
    "        }\n",
    "        return out\n",
    "\n",
    "    results = []\n",
    "    for frac in TRAIN_FRACTIONS:\n",
    "        for seed in SWEEP_SEEDS:\n",
    "            out = train_one_fraction(FIXED_NEURONS, train_frac=frac, seed=seed)\n",
    "            results.append(out)\n",
    "            print(out)\n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "\n",
    "    df = pd.DataFrame(results).sort_values([\"train_frac\", \"seed\"]).reset_index(drop=True)\n",
    "\n",
    "    summary = (\n",
    "        df.groupby([\"train_frac\", \"train_n\", \"total_params\"], as_index=False)\n",
    "          .agg(\n",
    "              best_val_loss_mean=(\"best_val_loss\", \"mean\"),\n",
    "              best_val_loss_std=(\"best_val_loss\", \"std\"),\n",
    "              best_epoch_mean=(\"best_epoch\", \"mean\"),\n",
    "              best_val_value_out_loss_mean=(\"best_val_value_out_loss\", \"mean\"),\n",
    "              best_val_exists_out_loss_mean=(\"best_val_exists_out_loss\", \"mean\"),\n",
    "          )\n",
    "          .sort_values(\"train_frac\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # save to csv\n",
    "    run_id = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_dir = os.path.join(\"sweeps\", f\"data_fraction_sweep_{run_id}\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    df_path = os.path.join(out_dir, \"sweep_raw.csv\")\n",
    "    summary_path = os.path.join(out_dir, \"sweep_summary.csv\")\n",
    "    meta_path = os.path.join(out_dir, \"metadata.json\")\n",
    "    fig_path = os.path.join(out_dir, \"val_loss_vs_train_fraction.png\")\n",
    "\n",
    "    df.to_csv(df_path, index=False)\n",
    "    summary.to_csv(summary_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b458abc-b03a-4910-9dfd-d92637c4cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SWEEP_DATA_AMOUNT:\n",
    "    def _jsonify(o):\n",
    "        if isinstance(o, np.ndarray): return o.tolist()\n",
    "        if isinstance(o, (np.integer,)): return int(o)\n",
    "        if isinstance(o, (np.floating,)): return float(o)\n",
    "        return o\n",
    "\n",
    "    metadata = {\n",
    "        \"run_id\": run_id,\n",
    "        \"fixed_depth\": FIXED_DEPTH,\n",
    "        \"fixed_width\": FIXED_WIDTH,\n",
    "        \"fixed_neurons\": FIXED_NEURONS,\n",
    "        \"train_fractions\": TRAIN_FRACTIONS,\n",
    "        \"seeds\": SWEEP_SEEDS,\n",
    "        \"batch_size\": int(TRAIN_BATCH_SIZE),\n",
    "        \"early_stopping_patience\": int(TRAIN_EARLY_STOPPING_PATIENCE),\n",
    "        \"notes\": \"Subset sampling only on TRAIN split. value_out loss is masked using y_exists via sample_weight.\",\n",
    "    }\n",
    "\n",
    "    with open(meta_path, \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2, default=_jsonify)\n",
    "\n",
    "    plt.figure()\n",
    "    y = summary[\"best_val_loss_mean\"].to_numpy()\n",
    "    x = summary[\"train_frac\"].to_numpy()\n",
    "    if len(SWEEP_SEEDS) > 1:\n",
    "        yerr = summary[\"best_val_loss_std\"].fillna(0.0).to_numpy()\n",
    "        plt.errorbar(x, y, yerr=yerr, marker=\"o\")\n",
    "        plt.ylabel(\"Best val loss (mean  std)\")\n",
    "    else:\n",
    "        plt.plot(x, y, marker=\"o\")\n",
    "        plt.ylabel(\"Best val loss\")\n",
    "\n",
    "    plt.xlabel(\"Training data fraction\")\n",
    "    plt.title(f\"Val loss vs training fraction (depth={FIXED_DEPTH}, width={FIXED_WIDTH})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_path, dpi=200)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nSaved:\\n- {df_path}\\n- {summary_path}\\n- {meta_path}\\n- {fig_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d39c98-db42-48fe-9e64-8c153c978203",
   "metadata": {},
   "source": [
    "### Keras Tuner to Find Best Hyperparameters and Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efe2de3-ba0a-4103-bfd1-466d73bd9b8c",
   "metadata": {},
   "source": [
    "Run this if you want to use keras tuner to make the model rather than doing it by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d804553-ae82-4f8f-9182-806c7f22b531",
   "metadata": {},
   "outputs": [],
   "source": [
    "if KERAS_TUNER:\n",
    "    from tensorflow.keras import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "    from tensorflow.keras.regularizers import l2\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "    from keras_tuner import HyperModel, RandomSearch\n",
    "    from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "081aa70b-c54e-48af-b895-5e9ddd2746cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if KERAS_TUNER:\n",
    "    if 'Try Both' not in ENCODING_TYPE:\n",
    "        def build_hypermodel(hp):\n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "            # Hyperparameters to tune\n",
    "            neurons_per_layer = [hp.Int(f'neurons_{i}', min_value=200, max_value=700,  step=10) for i in range(5)]\n",
    "            dropout_rate = hp.Float('dropout_rate', TRAIN_DROPOUT_RATE, 0.5, step=0.1)\n",
    "            \n",
    "            # Create Model in the same way that we do by hand\n",
    "            inputs = Input(shape=(len(X_test[0]),), name='input1')\n",
    "            x = inputs\n",
    "        \n",
    "            for i, n in enumerate(neurons_per_layer):\n",
    "                x = Dense(n, name=f'fc{i}', kernel_initializer='lecun_uniform', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "                x = LeakyReLU(negative_slope=0.01, name=f'leaky_relu{i}')(x)\n",
    "                x = Dropout(rate=dropout_rate, name=f'dropout{i}')(x)\n",
    "        \n",
    "            # multi-output heads: value_out (regression) and exists_out (existence classification)\n",
    "            value_out = Dense(len(y_value_train[0]), name='value_out', activation='linear', kernel_initializer='lecun_uniform')(x)\n",
    "            exists_out = Dense(len(y_value_train[0]), name='exists_out', activation='sigmoid', kernel_initializer='lecun_uniform')(x)\n",
    "            model = tf.keras.Model(inputs=inputs, outputs={\"value_out\": value_out, \"exists_out\": exists_out})\n",
    "        \n",
    "            # Learning rate configuration\n",
    "            lr_initial = hp.Float('learning_rate', 2e-3, 5e-3, sampling='LOG', default=0.0001)\n",
    "            lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=lr_initial, decay_steps=LR_DECAY_STEPS, decay_rate=LR_DECAY_RATE, staircase=LR_STAIRCASE)\n",
    "        \n",
    "            model.compile(\n",
    "                optimizer=tf.optimizers.Adam(learning_rate=lr_schedule),\n",
    "                loss={'value_out': VALUE_LOSS_FN, 'exists_out': 'binary_crossentropy'},\n",
    "                loss_weights={'value_out': 1.0, 'exists_out': 1.0},\n",
    "                metrics={'value_out': [VALUE_LOSS_FN], 'exists_out': ['accuracy']}\n",
    "            )\n",
    "\n",
    "            return model\n",
    "    else:\n",
    "        def build_hypermodel_one_hot_encoding(hp):\n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "            # Hyperparameters to tune\n",
    "            neurons_per_layer = [hp.Int(f'neurons_{i}', min_value=200, max_value=700,  step=10) for i in range(5)]\n",
    "            dropout_rate = hp.Float('dropout_rate', TRAIN_DROPOUT_RATE, 0.5, step=0.1)\n",
    "            \n",
    "            #----------------------------------------------one hot-------------------------------------------\n",
    "            # Create Model in the same way that we do by hand\n",
    "            inputs = Input(shape=(len(X_test_one_hot_encoding[0]),), name='input1')\n",
    "            x = inputs\n",
    "        \n",
    "            for i, n in enumerate(neurons_per_layer):\n",
    "                x = Dense(n, name=f'fc{i}', kernel_initializer='lecun_uniform', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "                x = LeakyReLU(negative_slope=0.01, name=f'leaky_relu{i}')(x)\n",
    "                x = Dropout(rate=dropout_rate, name=f'dropout{i}')(x)\n",
    "        \n",
    "            value_out = Dense(len(y_value_train_one_hot_encoding[0]), name='value_out', activation='linear', kernel_initializer='lecun_uniform')(x)\n",
    "            exists_out = Dense(len(y_value_train_one_hot_encoding[0]), name='exists_out', activation='sigmoid', kernel_initializer='lecun_uniform')(x)\n",
    "            model_one_hot_encoding = tf.keras.Model(inputs=inputs, outputs=[value_out, exists_out])\n",
    "            \n",
    "            # Learning rate configuration\n",
    "            lr_initial = hp.Float('learning_rate', 2e-3, 5e-3, sampling='LOG', default=0.0001)\n",
    "            lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=lr_initial, decay_steps=LR_DECAY_STEPS, decay_rate=LR_DECAY_RATE, staircase=LR_STAIRCASE)\n",
    "            optimizer = tf.optimizers.Adam(learning_rate=lr_schedule)\n",
    "            model_one_hot_encoding.compile(\n",
    "                optimizer=optimizer,\n",
    "                loss={'value_out': VALUE_LOSS_FN, 'exists_out': 'binary_crossentropy'},\n",
    "                loss_weights={'value_out': 1.0, 'exists_out': 1.0},\n",
    "                metrics={'value_out': [VALUE_LOSS_FN], 'exists_out': ['accuracy']}\n",
    "            )\n",
    "\n",
    "            return model_one_hot_encoding\n",
    "\n",
    "        def build_hypermodel_linear_encoding(hp):\n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "            \n",
    "            # Hyperparameters to tune\n",
    "            neurons_per_layer = [hp.Int(f'neurons_{i}', min_value=200, max_value=700, step=50) for i in range(5)]\n",
    "            dropout_rate = hp.Float('dropout_rate', TRAIN_DROPOUT_RATE, 0.5, step=0.1)\n",
    "            \n",
    "            #----------------------------------------------linear------------------------------------------- \n",
    "            # Create Model in the same way that we do by hand\n",
    "            inputs = Input(shape=(len(X_test_linear_encoding[0]),), name='input1')\n",
    "            x = inputs\n",
    "        \n",
    "            for i, n in enumerate(neurons_per_layer):\n",
    "                x = Dense(n, name=f'fc{i}', kernel_initializer='lecun_uniform', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "                x = LeakyReLU(negative_slope=0.01, name=f'leaky_relu{i}')(x)\n",
    "                x = Dropout(rate=dropout_rate, name=f'dropout{i}')(x)\n",
    "        \n",
    "            value_out = Dense(len(y_value_train_linear_encoding[0]), name='value_out', activation='linear', kernel_initializer='lecun_uniform')(x)\n",
    "            exists_out = Dense(len(y_value_train_linear_encoding[0]), name='exists_out', activation='sigmoid', kernel_initializer='lecun_uniform')(x)\n",
    "            model_linear_encoding = tf.keras.Model(inputs=inputs, outputs=[value_out, exists_out])\n",
    "            #----------------------------------------------continue-------------------------------------------\n",
    "\n",
    "            # Learning rate configuration\n",
    "            lr_initial = hp.Float('learning_rate', 2e-3, 5e-3, sampling='LOG', default=0.0001)\n",
    "            lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=lr_initial, decay_steps=LR_DECAY_STEPS, decay_rate=LR_DECAY_RATE, staircase=LR_STAIRCASE)\n",
    "            optimizer = tf.optimizers.Adam(learning_rate=lr_schedule)\n",
    "            model_linear_encoding.compile(\n",
    "                optimizer=optimizer,\n",
    "                loss={'value_out': VALUE_LOSS_FN, 'exists_out': 'binary_crossentropy'},\n",
    "                loss_weights={'value_out': 1.0, 'exists_out': 1.0},\n",
    "                metrics={'value_out': [VALUE_LOSS_FN], 'exists_out': ['accuracy']}\n",
    "            )\n",
    "\n",
    "            return model_linear_encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2cfea157-b0b4-4148-8d0a-e37c877877a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from keras/hyper_tuning_one_hot_encoding/mlp_tuning_one_hot_encoding/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "if 'Try Both' not in ENCODING_TYPE:\n",
    "    if KERAS_TUNER:\n",
    "        tuner = RandomSearch(\n",
    "            build_hypermodel,\n",
    "            objective='val_loss',\n",
    "            max_trials=KERAS_TUNER_TRIALS,\n",
    "            executions_per_trial=1,\n",
    "            directory=KERAS_DIR + f'/hyper_tuning_{encoding}_encoding',\n",
    "            project_name=f'mlp_tuning_{encoding}_encoding'\n",
    "        )\n",
    "else:\n",
    "    if KERAS_TUNER:\n",
    "        # Start tuning linear encoding\n",
    "        tuner_linear_encoding = RandomSearch(\n",
    "            build_hypermodel_linear_encoding,\n",
    "            objective='val_loss',\n",
    "            max_trials=KERAS_TUNER_TRIALS,\n",
    "            executions_per_trial=1,\n",
    "            directory=KERAS_DIR + '/hyper_tuning_linear_encoding',\n",
    "            project_name='mlp_tuning_linear_encoding'\n",
    "        )\n",
    "\n",
    "        # Start tuning one hot encoding\n",
    "        tuner_one_hot_encoding = RandomSearch(\n",
    "            build_hypermodel_one_hot_encoding,\n",
    "            objective='val_loss',\n",
    "            max_trials=KERAS_TUNER_TRIALS,\n",
    "            executions_per_trial=1,\n",
    "            directory=KERAS_DIR + '/hyper_tuning_one_hot_encoding',\n",
    "            project_name='mlp_tuning_one_hot_encoding'\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c11608cb-5b05-45af-af22-0bcab2827734",
   "metadata": {},
   "outputs": [],
   "source": [
    "if KERAS_TUNER:\n",
    "    # Setup Callbacks\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_value_out_loss',\n",
    "        mode='min',\n",
    "        patience=TRAIN_EARLY_STOPPING_PATIENCE,\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15440869-6be6-4005-9db4-3b37695adc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 42 Complete [00h 01m 28s]\n",
      "val_loss: 0.09623625874519348\n",
      "\n",
      "Best val_loss So Far: 0.08712401986122131\n",
      "Total elapsed time: 00h 52m 33s\n"
     ]
    }
   ],
   "source": [
    "if KERAS_TUNER:\n",
    "    if 'Try Both' not in ENCODING_TYPE:\n",
    "        # Perform hyperparameter tuning\n",
    "        tuner.search(\n",
    "            np.asarray(X_train),\n",
    "            {'value_out': np.asarray(y_value_train), 'exists_out': np.asarray(y_exists_train)},\n",
    "            sample_weight={\n",
    "                \"value_out\": np.asarray(y_exists_train).astype(\"float32\"),  # (N,16)\n",
    "                \"exists_out\": np.ones((len(y_exists_train),), dtype=\"float32\"),\n",
    "            },\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=TRAIN_BATCH_SIZE,\n",
    "            validation_data=(\n",
    "                np.asarray(X_val),\n",
    "                {'value_out': np.asarray(y_value_val), 'exists_out': np.asarray(y_exists_val)},\n",
    "                {\n",
    "                    \"value_out\": np.asarray(y_exists_val).astype(\"float32\"),  # (N,16)\n",
    "                    \"exists_out\": np.ones((len(y_exists_val),), dtype=\"float32\"),\n",
    "                },\n",
    "            ),\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # one-hot encoding branch\n",
    "        tuner_one_hot_encoding.search(\n",
    "            np.asarray(X_train_one_hot_encoding),\n",
    "            {'value_out': np.asarray(y_value_train_one_hot_encoding), 'exists_out': np.asarray(y_exists_train_one_hot_encoding)},\n",
    "            sample_weight={\n",
    "                \"value_out\": np.asarray(y_exists_train_one_hot_encoding).astype(\"float32\"),\n",
    "                \"exists_out\": np.ones((len(y_exists_train_one_hot_encoding),), dtype=\"float32\"),\n",
    "            },\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=TRAIN_BATCH_SIZE,\n",
    "            validation_data=(\n",
    "                np.asarray(X_val_one_hot_encoding),\n",
    "                {'value_out': np.asarray(y_value_val_one_hot_encoding), 'exists_out': np.asarray(y_exists_val_one_hot_encoding)},\n",
    "                {\n",
    "                    \"value_out\": np.asarray(y_exists_val_one_hot_encoding).astype(\"float32\"),\n",
    "                    \"exists_out\": np.ones((len(y_exists_val_one_hot_encoding),), dtype=\"float32\"),\n",
    "                },\n",
    "            ),\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "\n",
    "        # linear encoding branch\n",
    "        tuner_linear_encoding.search(\n",
    "            np.asarray(X_train_linear_encoding),\n",
    "            {'value_out': np.asarray(y_value_train_linear_encoding),\n",
    "             'exists_out': np.asarray(y_exists_train_linear_encoding)},\n",
    "            sample_weight={\n",
    "                \"value_out\": np.asarray(y_exists_train_linear_encoding).astype(\"float32\"),\n",
    "                \"exists_out\": np.ones((len(y_exists_train_linear_encoding),), dtype=\"float32\"),\n",
    "            },\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=TRAIN_BATCH_SIZE,\n",
    "            validation_data=(\n",
    "                np.asarray(X_val_linear_encoding),\n",
    "                {'value_out': np.asarray(y_value_val_linear_encoding),\n",
    "                 'exists_out': np.asarray(y_exists_val_linear_encoding)},\n",
    "                {\n",
    "                    \"value_out\": np.asarray(y_exists_val_linear_encoding).astype(\"float32\"),\n",
    "                    \"exists_out\": np.ones((len(y_exists_val_linear_encoding),), dtype=\"float32\"),\n",
    "                },\n",
    "            ),\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "598b1b58-8413-4573-a02d-fe8363165edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olivias/.local/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'adam', because it has 1 variables whereas the saved optimizer has 29 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "if KERAS_TUNER:\n",
    "    if 'Try Both' not in ENCODING_TYPE:\n",
    "        os.makedirs('model', exist_ok=True)\n",
    "        best_model_file= f'model/best_keras_model_{encoding}_encoding.keras'\n",
    "\n",
    "        best_model = tuner.get_best_models(1)[0]\n",
    "        best_model.save(best_model_file)\n",
    "\n",
    "        #gpu is thowing errors, lets try to clear its memory\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "        \n",
    "        #lets not compile to help with the memory bug\n",
    "        with tf.device('/CPU:0'):\n",
    "            loaded_model = load_model(best_model_file, compile=False)\n",
    "    else:\n",
    "        os.makedirs('model', exist_ok=True)\n",
    "        best_model_file_linear = 'model/best_keras_model_linear_encoding.keras'\n",
    "        best_model_file_onehot = 'model/best_keras_model_one_hot_encoding.keras'\n",
    "        \n",
    "        best_linear_model = tuner_linear_encoding.get_best_models(1)[0]\n",
    "        best_onehot_model = tuner_one_hot_encoding.get_best_models(1)[0]\n",
    "        \n",
    "        best_linear_model.save(best_model_file_linear)\n",
    "        best_onehot_model.save(best_model_file_onehot)\n",
    "        \n",
    "        #gpu is thowing errors, lets try to clear its memory\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "        \n",
    "        #lets not compile to help with the memory bug\n",
    "        with tf.device('/CPU:0'):\n",
    "            loaded_linear_model = load_model(best_model_file_linear, compile=False)\n",
    "            loaded_onehot_model = load_model(best_model_file_onehot, compile=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4291953b-7ce8-498d-88ce-20974516af77",
   "metadata": {},
   "source": [
    "### View the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "137d2a50-a4ef-4ccc-8cb9-04e380f0fcfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
       "\n",
       " input1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
       "\n",
       " fc0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">570</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,710</span>  input1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
       "\n",
       " leaky_relu0          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">570</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  fc0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)                                                           \n",
       "\n",
       " dropout0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">570</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  leaky_relu0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       "\n",
       " fc1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">510</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">291,210</span>  dropout0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
       "\n",
       " leaky_relu1          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">510</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  fc1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)                                                           \n",
       "\n",
       " dropout1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">510</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  leaky_relu1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       "\n",
       " fc2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">127,750</span>  dropout1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
       "\n",
       " leaky_relu2          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  fc2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)                                                           \n",
       "\n",
       " dropout2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  leaky_relu2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       "\n",
       " fc3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">440</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">110,440</span>  dropout2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
       "\n",
       " leaky_relu3          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">440</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  fc3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)                                                           \n",
       "\n",
       " dropout3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">440</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  leaky_relu3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       "\n",
       " fc4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">480</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">211,680</span>  dropout3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
       "\n",
       " leaky_relu4          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">480</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  fc4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)                                                           \n",
       "\n",
       " dropout4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">480</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  leaky_relu4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       "\n",
       " exists_out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">7,696</span>  dropout4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
       "\n",
       " value_out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">7,696</span>  dropout4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " input1 (\u001b[38;5;33mInputLayer\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  -                 \n",
       "\n",
       " fc0 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m570\u001b[0m)             \u001b[38;5;34m1,710\u001b[0m  input1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
       "\n",
       " leaky_relu0          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m570\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  fc0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
       " (\u001b[38;5;33mLeakyReLU\u001b[0m)                                                           \n",
       "\n",
       " dropout0 (\u001b[38;5;33mDropout\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m570\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  leaky_relu0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
       "\n",
       " fc1 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m510\u001b[0m)           \u001b[38;5;34m291,210\u001b[0m  dropout0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
       "\n",
       " leaky_relu1          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m510\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  fc1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
       " (\u001b[38;5;33mLeakyReLU\u001b[0m)                                                           \n",
       "\n",
       " dropout1 (\u001b[38;5;33mDropout\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m510\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  leaky_relu1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
       "\n",
       " fc2 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m)           \u001b[38;5;34m127,750\u001b[0m  dropout1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
       "\n",
       " leaky_relu2          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  fc2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
       " (\u001b[38;5;33mLeakyReLU\u001b[0m)                                                           \n",
       "\n",
       " dropout2 (\u001b[38;5;33mDropout\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  leaky_relu2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
       "\n",
       " fc3 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m440\u001b[0m)           \u001b[38;5;34m110,440\u001b[0m  dropout2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
       "\n",
       " leaky_relu3          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m440\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  fc3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
       " (\u001b[38;5;33mLeakyReLU\u001b[0m)                                                           \n",
       "\n",
       " dropout3 (\u001b[38;5;33mDropout\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m440\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  leaky_relu3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
       "\n",
       " fc4 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m480\u001b[0m)           \u001b[38;5;34m211,680\u001b[0m  dropout3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
       "\n",
       " leaky_relu4          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m480\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  fc4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
       " (\u001b[38;5;33mLeakyReLU\u001b[0m)                                                           \n",
       "\n",
       " dropout4 (\u001b[38;5;33mDropout\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m480\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  leaky_relu4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
       "\n",
       " exists_out (\u001b[38;5;33mDense\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)              \u001b[38;5;34m7,696\u001b[0m  dropout4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
       "\n",
       " value_out (\u001b[38;5;33mDense\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)              \u001b[38;5;34m7,696\u001b[0m  dropout4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">758,182</span> (2.89 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m758,182\u001b[0m (2.89 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">758,182</span> (2.89 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m758,182\u001b[0m (2.89 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if KERAS_TUNER:\n",
    "    if 'Try Both' not in ENCODING_TYPE:\n",
    "        best_model.summary()\n",
    "    else:\n",
    "        best_onehot_model.summary()\n",
    "        best_linear_model.summary()\n",
    "        \n",
    "if not KERAS_TUNER  and not SWEEP_PARAM_NUM and not SWEEP_DATA_AMOUNT:\n",
    "    if 'Try Both' not in ENCODING_TYPE:\n",
    "        print(\"\\n---- Model Summary ----\")\n",
    "        model.summary()\n",
    "    else:\n",
    "        print(\"\\n---- Linear Encoding Model Summary ----\")\n",
    "        model_linear_encoding.summary()\n",
    "        \n",
    "        print(\"\\n---- One-Hot Encoding Model Summary ----\")\n",
    "        model_one_hot_encoding.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f05d0a25-2890-4ddc-b888-57b849fa2a0f",
   "metadata": {},
   "source": [
    "if KERAS_TUNER:\n",
    "    keras2ascii(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e57cb40-ca2c-4ac7-905c-e13beaf71a51",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5556f8-c81b-4522-b7be-d847b6f20452",
   "metadata": {},
   "source": [
    "Although we may plot and print many metrics, we focus only on **Mean Squared Error (MSE).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dc9acc-ad33-4df3-ac56-8ed3913ce16d",
   "metadata": {},
   "source": [
    "Plot training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "523cc6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib ipympl\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a721cc-cc18-4bc6-96b9-520b736e2382",
   "metadata": {},
   "source": [
    "### Visualize gradients for best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7d64ca5e-a7e8-49fe-8734-becc35d4998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Try Both' not in ENCODING_TYPE:\n",
    "    if KERAS_TUNER and not SWEEP_PARAM_NUM and VISUALIZE_GRADIENTS:\n",
    "        class GradientNormLogger(tf.keras.callbacks.Callback):\n",
    "            \"\"\"\n",
    "            Logs gradient norms on a fixed probe batch at epoch end.\n",
    "            Works for:\n",
    "              - single-output models (y_true array)\n",
    "              - multi-output dict models (e.g., {\"value_out\":..., \"exists_out\":...})\n",
    "            Supports passing sample_weight (incl. per-output dict), which is required for masked losses.\n",
    "            \"\"\"\n",
    "            def __init__(\n",
    "                self,\n",
    "                x_probe,\n",
    "                y_probe,\n",
    "                sample_weight_probe=None,\n",
    "                layer_name_prefixes=(\"fc\", \"value_out\", \"exists_out\"),\n",
    "                log_every=1,\n",
    "                verbose=1\n",
    "            ):\n",
    "                super().__init__()\n",
    "                self.x_probe = tf.convert_to_tensor(x_probe)\n",
    "                self.y_probe = y_probe  # keep as-is; we will convert lazily (dict vs array)\n",
    "                self.sample_weight_probe = sample_weight_probe\n",
    "                self.layer_name_prefixes = tuple(layer_name_prefixes)\n",
    "                self.log_every = int(log_every)\n",
    "                self.verbose = int(verbose)\n",
    "                self.records = []\n",
    "        \n",
    "            def _to_tensor_tree(self, obj):\n",
    "                # converts arrays (or dict of arrays) to tensors\n",
    "                if obj is None:\n",
    "                    return None\n",
    "                if isinstance(obj, dict):\n",
    "                    return {k: tf.convert_to_tensor(v) for k, v in obj.items()}\n",
    "                return tf.convert_to_tensor(obj)\n",
    "        \n",
    "            def _want_var(self, var_name: str) -> bool:\n",
    "                # var_name like \"fc0/kernel:0\", \"value_out/bias:0\", etc.\n",
    "                base = var_name.split(\":\")[0]\n",
    "                return any(base.startswith(pfx) for pfx in self.layer_name_prefixes)\n",
    "        \n",
    "            def on_epoch_end(self, epoch, logs=None):\n",
    "                logs = logs or {}\n",
    "                if (epoch + 1) % self.log_every != 0:\n",
    "                    return\n",
    "        \n",
    "                y_probe_t = self._to_tensor_tree(self.y_probe)\n",
    "                sw_probe_t = self._to_tensor_tree(self.sample_weight_probe)\n",
    "        \n",
    "                with tf.GradientTape() as tape:\n",
    "                    y_pred = self.model(self.x_probe, training=True)\n",
    "                    # IMPORTANT: use compiled_loss the same way training does\n",
    "                    loss = self.model.compiled_loss(\n",
    "                        y_probe_t,\n",
    "                        y_pred,\n",
    "                        sample_weight=sw_probe_t,\n",
    "                        regularization_losses=self.model.losses\n",
    "                    )\n",
    "        \n",
    "                grads = tape.gradient(loss, self.model.trainable_weights)\n",
    "        \n",
    "                rec = {\"epoch\": int(epoch + 1), \"probe_loss\": float(loss.numpy())}\n",
    "        \n",
    "                per_layer = {}\n",
    "                for w, g in zip(self.model.trainable_weights, grads):\n",
    "                    if g is None:\n",
    "                        continue\n",
    "        \n",
    "                    wname = w.name  # includes :0\n",
    "                    if not self._want_var(wname):\n",
    "                        continue\n",
    "        \n",
    "                    wbase = wname.split(\":\")[0]\n",
    "                    g_norm = float(tf.linalg.global_norm([g]).numpy().item())\n",
    "                    rec[f\"grad_norm__{wbase}\"] = g_norm\n",
    "        \n",
    "                    layer_key = wbase.split(\"/\")[0]\n",
    "                    per_layer.setdefault(layer_key, []).append(g_norm)\n",
    "        \n",
    "                for layer_key, norms in per_layer.items():\n",
    "                    rec[f\"grad_mean__{layer_key}\"] = float(np.mean(norms))\n",
    "                    rec[f\"grad_max__{layer_key}\"] = float(np.max(norms))\n",
    "        \n",
    "                g_all = [g for g in grads if g is not None]\n",
    "                rec[\"grad_global_norm\"] = float(tf.linalg.global_norm(g_all).numpy().item()) if g_all else float(\"nan\")\n",
    "        \n",
    "                self.records.append(rec)\n",
    "        \n",
    "                # push scalars into History\n",
    "                for k, v in rec.items():\n",
    "                    if k != \"epoch\":\n",
    "                        logs[k] = v\n",
    "        \n",
    "                if self.verbose:\n",
    "                    msg = f\"[Grad] epoch={rec['epoch']} probe_loss={rec['probe_loss']:.6g} global={rec['grad_global_norm']:.3g}\"\n",
    "                    # show a couple common layers if present\n",
    "                    for lk in (\"fc0\", \"value_out\", \"exists_out\"):\n",
    "                        mk = f\"grad_mean__{lk}\"\n",
    "                        xk = f\"grad_max__{lk}\"\n",
    "                        if mk in rec:\n",
    "                            msg += f\" | {lk}:mean={rec[mk]:.3g} max={rec[xk]:.3g}\"\n",
    "                    print(msg)\n",
    "        \n",
    "            def to_csv(self, path: str):\n",
    "                import csv, os\n",
    "                os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "                if not self.records:\n",
    "                    return\n",
    "                keys = sorted({k for r in self.records for k in r.keys()})\n",
    "                with open(path, \"w\", newline=\"\") as f:\n",
    "                    w = csv.DictWriter(f, fieldnames=keys)\n",
    "                    w.writeheader()\n",
    "                    w.writerows(self.records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9969df9e-8593-4cb0-9051-2a085a051d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Try Both' not in ENCODING_TYPE:\n",
    "    if KERAS_TUNER and not SWEEP_PARAM_NUM and VISUALIZE_GRADIENTS:\n",
    "        probe_n = min(256, len(X_train))\n",
    "\n",
    "        x_probe = np.asarray(X_train[:probe_n])\n",
    "\n",
    "        # y_true dict for the notebook model\n",
    "        y_probe = {\n",
    "            \"value_out\": np.asarray(y_value_train[:probe_n]),\n",
    "            \"exists_out\": np.asarray(y_exists_train[:probe_n]).astype(\"float32\"),\n",
    "        }\n",
    "\n",
    "        # sample_weight dict: mask value_out by exists_out, neutral weight for exists_out\n",
    "        sw_probe = {\n",
    "            \"value_out\": y_probe[\"exists_out\"],  # (probe_n, output_dim)\n",
    "            \"exists_out\": np.ones((probe_n,), dtype=\"float32\"),\n",
    "        }\n",
    "\n",
    "        grad_logger = GradientNormLogger(\n",
    "            x_probe=x_probe,\n",
    "            y_probe=y_probe,\n",
    "            sample_weight_probe=sw_probe,\n",
    "            layer_name_prefixes=(\"fc0\", \"value_out\", \"exists_out\"),  # adjust as you like\n",
    "            log_every=1,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "        model = tuner.hypermodel.build(best_hp)\n",
    "\n",
    "        lr_monitor = LearningRateMonitor()\n",
    "\n",
    "        history = model.fit(\n",
    "            np.asarray(X_train),\n",
    "            {\"value_out\": np.asarray(y_value_train), \"exists_out\": np.asarray(y_exists_train).astype(\"float32\")},\n",
    "            sample_weight={\n",
    "                \"value_out\": np.asarray(y_exists_train).astype(\"float32\"),\n",
    "                \"exists_out\": np.ones((len(X_train),), dtype=\"float32\"),\n",
    "            },\n",
    "            epochs=400,\n",
    "            batch_size=TRAIN_BATCH_SIZE,\n",
    "            validation_data=(\n",
    "                np.asarray(X_val),\n",
    "                {\"value_out\": np.asarray(y_value_val), \"exists_out\": np.asarray(y_exists_val).astype(\"float32\")},\n",
    "                {\n",
    "                    \"value_out\": np.asarray(y_exists_val).astype(\"float32\"),\n",
    "                    \"exists_out\": np.ones((len(X_val),), dtype=\"float32\"),\n",
    "                },\n",
    "            ),\n",
    "            callbacks=[early_stopping, lr_monitor, grad_logger],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        grad_logger.to_csv(f\"plots/{encoding}_gradients.csv\")\n",
    "\n",
    "        del model\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc30d21b-458b-4518-bdae-04f505e45c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Try Both' not in ENCODING_TYPE:\n",
    "    if KERAS_TUNER and not SWEEP_PARAM_NUM and VISUALIZE_GRADIENTS:\n",
    "        dfg = pd.DataFrame(grad_logger.records)\n",
    "        \n",
    "        # Plot global grad norm\n",
    "        plt.figure()\n",
    "        plt.plot(dfg[\"epoch\"], dfg[\"grad_global_norm\"])\n",
    "        plt.yscale(\"log\")  # very helpful to see vanishing/exploding\n",
    "        plt.title(\"Gradient tracking for a single batch across epochs\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Gradient magnitude (log scale)\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"plots/{encoding}_grad_global_norm.pdf\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot fc0 and output mean norms if present\n",
    "        for lk in [\"fc0\", \"output\"]:\n",
    "            col = f\"grad_mean__{lk}\"\n",
    "            if col in dfg.columns:\n",
    "                plt.figure()\n",
    "                plt.plot(dfg[\"epoch\"], dfg[col])\n",
    "                plt.yscale(\"log\")\n",
    "                plt.title(f\"Gradient Mean Norm: {lk} (probe batch)\")\n",
    "                plt.xlabel(\"Epoch\")\n",
    "                plt.ylabel(\"Mean grad norm (log scale)\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f\"plots/{encoding}_grad_mean_{lk}.pdf\")\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aa8eb5-11a2-4ca0-832f-e0cfb057a75a",
   "metadata": {},
   "source": [
    "### Look at the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "347263a8-570e-4a04-bbae-3ea0662e4012",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2s/step - exists_out_accuracy: 0.0012 - exists_out_loss: 0.3774 - loss: 2.6126 - value_out__loss: 0.4059 - value_out_loss: 0.3554 - val_exists_out_accuracy: 0.0055 - val_exists_out_loss: 0.1099 - val_loss: 1.8520 - val_value_out__loss: 0.4175 - val_value_out_loss: 0.3367\n",
      "Epoch 2/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - exists_out_accuracy: 0.0035 - exists_out_loss: 0.0857 - loss: 1.5602 - value_out__loss: 0.3406 - value_out_loss: 0.2761 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0616 - val_loss: 1.2273 - val_value_out__loss: 0.2660 - val_value_out_loss: 0.2002\n",
      "Epoch 3/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0012 - exists_out_loss: 0.0629 - loss: 1.1177 - value_out__loss: 0.2343 - value_out_loss: 0.1842 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0444 - val_loss: 0.9868 - val_value_out__loss: 0.2263 - val_value_out_loss: 0.1721\n",
      "Epoch 4/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0457 - loss: 0.9337 - value_out__loss: 0.1964 - value_out_loss: 0.1562 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0419 - val_loss: 0.8328 - val_value_out__loss: 0.1433 - val_value_out_loss: 0.1130\n",
      "Epoch 5/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0382 - loss: 0.8169 - value_out__loss: 0.1638 - value_out_loss: 0.1267 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0345 - val_loss: 0.7279 - val_value_out__loss: 0.1215 - val_value_out_loss: 0.0803\n",
      "Epoch 6/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0361 - loss: 0.7380 - value_out__loss: 0.1568 - value_out_loss: 0.1134 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0342 - val_loss: 0.6647 - val_value_out__loss: 0.1255 - val_value_out_loss: 0.0767\n",
      "Epoch 7/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0366 - loss: 0.6746 - value_out__loss: 0.1526 - value_out_loss: 0.1055 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0354 - val_loss: 0.6179 - val_value_out__loss: 0.1283 - val_value_out_loss: 0.0797\n",
      "Epoch 8/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0357 - loss: 0.6218 - value_out__loss: 0.1488 - value_out_loss: 0.1002 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0344 - val_loss: 0.5719 - val_value_out__loss: 0.1311 - val_value_out_loss: 0.0766\n",
      "Epoch 9/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0351 - loss: 0.5797 - value_out__loss: 0.1492 - value_out_loss: 0.0977 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0345 - val_loss: 0.5344 - val_value_out__loss: 0.1269 - val_value_out_loss: 0.0752\n",
      "Epoch 10/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0349 - loss: 0.5425 - value_out__loss: 0.1464 - value_out_loss: 0.0946 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0326 - val_loss: 0.4993 - val_value_out__loss: 0.1281 - val_value_out_loss: 0.0729\n",
      "Epoch 11/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0012 - exists_out_loss: 0.0340 - loss: 0.5090 - value_out__loss: 0.1438 - value_out_loss: 0.0922 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0330 - val_loss: 0.4648 - val_value_out__loss: 0.1208 - val_value_out_loss: 0.0664\n",
      "Epoch 12/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0335 - loss: 0.4790 - value_out__loss: 0.1407 - value_out_loss: 0.0897 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0326 - val_loss: 0.4432 - val_value_out__loss: 0.1209 - val_value_out_loss: 0.0710\n",
      "Epoch 13/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0333 - loss: 0.4507 - value_out__loss: 0.1363 - value_out_loss: 0.0869 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0320 - val_loss: 0.4171 - val_value_out__loss: 0.1197 - val_value_out_loss: 0.0697\n",
      "Epoch 14/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0012 - exists_out_loss: 0.0329 - loss: 0.4256 - value_out__loss: 0.1347 - value_out_loss: 0.0852 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0319 - val_loss: 0.3935 - val_value_out__loss: 0.1186 - val_value_out_loss: 0.0677\n",
      "Epoch 15/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0325 - loss: 0.4036 - value_out__loss: 0.1329 - value_out_loss: 0.0842 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0323 - val_loss: 0.3799 - val_value_out__loss: 0.1222 - val_value_out_loss: 0.0739\n",
      "Epoch 16/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0331 - loss: 0.3869 - value_out__loss: 0.1319 - value_out_loss: 0.0857 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0309 - val_loss: 0.3665 - val_value_out__loss: 0.1244 - val_value_out_loss: 0.0790\n",
      "Epoch 17/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0012 - exists_out_loss: 0.0323 - loss: 0.3677 - value_out__loss: 0.1276 - value_out_loss: 0.0842 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0307 - val_loss: 0.3376 - val_value_out__loss: 0.1089 - val_value_out_loss: 0.0663\n",
      "Epoch 18/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0320 - loss: 0.3462 - value_out__loss: 0.1204 - value_out_loss: 0.0796 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0295 - val_loss: 0.3164 - val_value_out__loss: 0.1035 - val_value_out_loss: 0.0624\n",
      "Epoch 19/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0311 - loss: 0.3269 - value_out__loss: 0.1178 - value_out_loss: 0.0769 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0299 - val_loss: 0.3013 - val_value_out__loss: 0.1033 - val_value_out_loss: 0.0619\n",
      "Epoch 20/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0312 - loss: 0.3138 - value_out__loss: 0.1198 - value_out_loss: 0.0782 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0296 - val_loss: 0.2900 - val_value_out__loss: 0.1044 - val_value_out_loss: 0.0647\n",
      "Epoch 21/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - exists_out_accuracy: 0.0012 - exists_out_loss: 0.0315 - loss: 0.3020 - value_out__loss: 0.1187 - value_out_loss: 0.0790 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0292 - val_loss: 0.2803 - val_value_out__loss: 0.1065 - val_value_out_loss: 0.0670\n",
      "Epoch 22/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0305 - loss: 0.2857 - value_out__loss: 0.1132 - value_out_loss: 0.0751 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0295 - val_loss: 0.2630 - val_value_out__loss: 0.0978 - val_value_out_loss: 0.0613\n",
      "Epoch 23/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0308 - loss: 0.2744 - value_out__loss: 0.1124 - value_out_loss: 0.0755 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0292 - val_loss: 0.2491 - val_value_out__loss: 0.0960 - val_value_out_loss: 0.0589\n",
      "Epoch 24/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0303 - loss: 0.2628 - value_out__loss: 0.1119 - value_out_loss: 0.0749 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0293 - val_loss: 0.2471 - val_value_out__loss: 0.1024 - val_value_out_loss: 0.0664\n",
      "Epoch 25/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0305 - loss: 0.2530 - value_out__loss: 0.1108 - value_out_loss: 0.0736 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0287 - val_loss: 0.2320 - val_value_out__loss: 0.0961 - val_value_out_loss: 0.0597\n",
      "Epoch 26/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0302 - loss: 0.2440 - value_out__loss: 0.1101 - value_out_loss: 0.0730 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0281 - val_loss: 0.2228 - val_value_out__loss: 0.0965 - val_value_out_loss: 0.0595\n",
      "Epoch 27/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0299 - loss: 0.2333 - value_out__loss: 0.1086 - value_out_loss: 0.0712 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0287 - val_loss: 0.2169 - val_value_out__loss: 0.0981 - val_value_out_loss: 0.0618\n",
      "Epoch 28/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0303 - loss: 0.2254 - value_out__loss: 0.1083 - value_out_loss: 0.0710 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0281 - val_loss: 0.2040 - val_value_out__loss: 0.0949 - val_value_out_loss: 0.0561\n",
      "Epoch 29/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0298 - loss: 0.2179 - value_out__loss: 0.1088 - value_out_loss: 0.0707 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0284 - val_loss: 0.2021 - val_value_out__loss: 0.0988 - val_value_out_loss: 0.0612\n",
      "Epoch 30/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0304 - loss: 0.2123 - value_out__loss: 0.1095 - value_out_loss: 0.0707 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0283 - val_loss: 0.2029 - val_value_out__loss: 0.1076 - val_value_out_loss: 0.0669\n",
      "Epoch 31/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0297 - loss: 0.2074 - value_out__loss: 0.1109 - value_out_loss: 0.0718 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0279 - val_loss: 0.1862 - val_value_out__loss: 0.0961 - val_value_out_loss: 0.0562\n",
      "Epoch 32/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0300 - loss: 0.1993 - value_out__loss: 0.1085 - value_out_loss: 0.0695 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0280 - val_loss: 0.1792 - val_value_out__loss: 0.0937 - val_value_out_loss: 0.0550\n",
      "Epoch 33/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0298 - loss: 0.1932 - value_out__loss: 0.1060 - value_out_loss: 0.0687 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0278 - val_loss: 0.1736 - val_value_out__loss: 0.0925 - val_value_out_loss: 0.0546\n",
      "Epoch 34/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0298 - loss: 0.1882 - value_out__loss: 0.1058 - value_out_loss: 0.0688 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0278 - val_loss: 0.1707 - val_value_out__loss: 0.0933 - val_value_out_loss: 0.0565\n",
      "Epoch 35/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0303 - loss: 0.1875 - value_out__loss: 0.1082 - value_out_loss: 0.0712 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0279 - val_loss: 0.1786 - val_value_out__loss: 0.1059 - val_value_out_loss: 0.0672\n",
      "Epoch 36/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0291 - loss: 0.1810 - value_out__loss: 0.1075 - value_out_loss: 0.0692 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0277 - val_loss: 0.1622 - val_value_out__loss: 0.0940 - val_value_out_loss: 0.0555\n",
      "Epoch 37/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0295 - loss: 0.1746 - value_out__loss: 0.1059 - value_out_loss: 0.0680 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0276 - val_loss: 0.1575 - val_value_out__loss: 0.0944 - val_value_out_loss: 0.0556\n",
      "Epoch 38/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0302 - loss: 0.1747 - value_out__loss: 0.1085 - value_out_loss: 0.0709 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0284 - val_loss: 0.1672 - val_value_out__loss: 0.1041 - val_value_out_loss: 0.0671\n",
      "Epoch 39/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0300 - loss: 0.1705 - value_out__loss: 0.1052 - value_out_loss: 0.0686 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0273 - val_loss: 0.1523 - val_value_out__loss: 0.0923 - val_value_out_loss: 0.0552\n",
      "Epoch 40/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0292 - loss: 0.1630 - value_out__loss: 0.1007 - value_out_loss: 0.0651 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0275 - val_loss: 0.1518 - val_value_out__loss: 0.0928 - val_value_out_loss: 0.0580\n",
      "Epoch 41/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0293 - loss: 0.1622 - value_out__loss: 0.1019 - value_out_loss: 0.0671 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0273 - val_loss: 0.1530 - val_value_out__loss: 0.0979 - val_value_out_loss: 0.0616\n",
      "Epoch 42/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0294 - loss: 0.1614 - value_out__loss: 0.1028 - value_out_loss: 0.0686 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0273 - val_loss: 0.1421 - val_value_out__loss: 0.0868 - val_value_out_loss: 0.0536\n",
      "Epoch 43/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0291 - loss: 0.1545 - value_out__loss: 0.0983 - value_out_loss: 0.0647 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0273 - val_loss: 0.1422 - val_value_out__loss: 0.0895 - val_value_out_loss: 0.0561\n",
      "Epoch 44/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0298 - loss: 0.1569 - value_out__loss: 0.1008 - value_out_loss: 0.0688 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0271 - val_loss: 0.1391 - val_value_out__loss: 0.0876 - val_value_out_loss: 0.0550\n",
      "Epoch 45/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0296 - loss: 0.1539 - value_out__loss: 0.0994 - value_out_loss: 0.0673 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0278 - val_loss: 0.1447 - val_value_out__loss: 0.0946 - val_value_out_loss: 0.0608\n",
      "Epoch 46/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0292 - loss: 0.1487 - value_out__loss: 0.0971 - value_out_loss: 0.0636 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0270 - val_loss: 0.1331 - val_value_out__loss: 0.0859 - val_value_out_loss: 0.0524\n",
      "Epoch 47/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0292 - loss: 0.1468 - value_out__loss: 0.0974 - value_out_loss: 0.0641 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0273 - val_loss: 0.1309 - val_value_out__loss: 0.0849 - val_value_out_loss: 0.0519\n",
      "Epoch 48/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0289 - loss: 0.1444 - value_out__loss: 0.0970 - value_out_loss: 0.0641 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0270 - val_loss: 0.1323 - val_value_out__loss: 0.0887 - val_value_out_loss: 0.0552\n",
      "Epoch 49/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0303 - loss: 0.1499 - value_out__loss: 0.1021 - value_out_loss: 0.0695 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0275 - val_loss: 0.1408 - val_value_out__loss: 0.0970 - val_value_out_loss: 0.0637\n",
      "Epoch 50/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0289 - loss: 0.1488 - value_out__loss: 0.1028 - value_out_loss: 0.0703 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0268 - val_loss: 0.1361 - val_value_out__loss: 0.0948 - val_value_out_loss: 0.0610\n",
      "Epoch 51/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0288 - loss: 0.1437 - value_out__loss: 0.0996 - value_out_loss: 0.0668 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0268 - val_loss: 0.1289 - val_value_out__loss: 0.0885 - val_value_out_loss: 0.0558\n",
      "Epoch 52/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0292 - loss: 0.1414 - value_out__loss: 0.0981 - value_out_loss: 0.0658 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0268 - val_loss: 0.1241 - val_value_out__loss: 0.0846 - val_value_out_loss: 0.0519\n",
      "Epoch 53/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0288 - loss: 0.1393 - value_out__loss: 0.0971 - value_out_loss: 0.0649 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0272 - val_loss: 0.1225 - val_value_out__loss: 0.0835 - val_value_out_loss: 0.0509\n",
      "Epoch 54/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0291 - loss: 0.1401 - value_out__loss: 0.0980 - value_out_loss: 0.0659 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0280 - val_loss: 0.1427 - val_value_out__loss: 0.1015 - val_value_out_loss: 0.0695\n",
      "Epoch 55/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0292 - loss: 0.1426 - value_out__loss: 0.0996 - value_out_loss: 0.0671 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0265 - val_loss: 0.1227 - val_value_out__loss: 0.0839 - val_value_out_loss: 0.0506\n",
      "Epoch 56/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0283 - loss: 0.1354 - value_out__loss: 0.0954 - value_out_loss: 0.0622 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0266 - val_loss: 0.1240 - val_value_out__loss: 0.0883 - val_value_out_loss: 0.0548\n",
      "Epoch 57/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0288 - loss: 0.1356 - value_out__loss: 0.0974 - value_out_loss: 0.0644 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0268 - val_loss: 0.1223 - val_value_out__loss: 0.0870 - val_value_out_loss: 0.0546\n",
      "Epoch 58/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0294 - loss: 0.1396 - value_out__loss: 0.1005 - value_out_loss: 0.0678 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0279 - val_loss: 0.1379 - val_value_out__loss: 0.1011 - val_value_out_loss: 0.0672\n",
      "Epoch 59/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0292 - loss: 0.1389 - value_out__loss: 0.0991 - value_out_loss: 0.0667 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0263 - val_loss: 0.1218 - val_value_out__loss: 0.0868 - val_value_out_loss: 0.0538\n",
      "Epoch 60/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0285 - loss: 0.1343 - value_out__loss: 0.0968 - value_out_loss: 0.0643 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0265 - val_loss: 0.1221 - val_value_out__loss: 0.0898 - val_value_out_loss: 0.0555\n",
      "Epoch 61/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0289 - loss: 0.1340 - value_out__loss: 0.0976 - value_out_loss: 0.0649 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0266 - val_loss: 0.1235 - val_value_out__loss: 0.0904 - val_value_out_loss: 0.0579\n",
      "Epoch 62/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0284 - loss: 0.1312 - value_out__loss: 0.0955 - value_out_loss: 0.0632 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0264 - val_loss: 0.1180 - val_value_out__loss: 0.0853 - val_value_out_loss: 0.0530\n",
      "Epoch 63/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0291 - loss: 0.1330 - value_out__loss: 0.0971 - value_out_loss: 0.0649 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0266 - val_loss: 0.1227 - val_value_out__loss: 0.0904 - val_value_out_loss: 0.0570\n",
      "Epoch 64/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0283 - loss: 0.1296 - value_out__loss: 0.0944 - value_out_loss: 0.0614 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0264 - val_loss: 0.1173 - val_value_out__loss: 0.0852 - val_value_out_loss: 0.0517\n",
      "Epoch 65/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0283 - loss: 0.1286 - value_out__loss: 0.0941 - value_out_loss: 0.0609 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0265 - val_loss: 0.1162 - val_value_out__loss: 0.0856 - val_value_out_loss: 0.0518\n",
      "Epoch 66/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0285 - loss: 0.1297 - value_out__loss: 0.0967 - value_out_loss: 0.0631 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0264 - val_loss: 0.1242 - val_value_out__loss: 0.0946 - val_value_out_loss: 0.0607\n",
      "Epoch 67/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0286 - loss: 0.1301 - value_out__loss: 0.0970 - value_out_loss: 0.0638 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0263 - val_loss: 0.1193 - val_value_out__loss: 0.0897 - val_value_out_loss: 0.0557\n",
      "Epoch 68/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0283 - loss: 0.1280 - value_out__loss: 0.0954 - value_out_loss: 0.0623 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0261 - val_loss: 0.1147 - val_value_out__loss: 0.0866 - val_value_out_loss: 0.0519\n",
      "Epoch 69/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0285 - loss: 0.1296 - value_out__loss: 0.0978 - value_out_loss: 0.0643 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0260 - val_loss: 0.1146 - val_value_out__loss: 0.0872 - val_value_out_loss: 0.0526\n",
      "Epoch 70/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0287 - loss: 0.1308 - value_out__loss: 0.0994 - value_out_loss: 0.0655 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0265 - val_loss: 0.1231 - val_value_out__loss: 0.0951 - val_value_out_loss: 0.0601\n",
      "Epoch 71/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0287 - loss: 0.1310 - value_out__loss: 0.0998 - value_out_loss: 0.0654 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0259 - val_loss: 0.1181 - val_value_out__loss: 0.0903 - val_value_out_loss: 0.0564\n",
      "Epoch 72/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0282 - loss: 0.1275 - value_out__loss: 0.0972 - value_out_loss: 0.0633 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0260 - val_loss: 0.1144 - val_value_out__loss: 0.0881 - val_value_out_loss: 0.0534\n",
      "Epoch 73/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0284 - loss: 0.1279 - value_out__loss: 0.0980 - value_out_loss: 0.0639 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0260 - val_loss: 0.1161 - val_value_out__loss: 0.0896 - val_value_out_loss: 0.0548\n",
      "Epoch 74/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0283 - loss: 0.1262 - value_out__loss: 0.0959 - value_out_loss: 0.0626 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0262 - val_loss: 0.1180 - val_value_out__loss: 0.0906 - val_value_out_loss: 0.0577\n",
      "Epoch 75/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0282 - loss: 0.1243 - value_out__loss: 0.0937 - value_out_loss: 0.0608 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0259 - val_loss: 0.1132 - val_value_out__loss: 0.0854 - val_value_out_loss: 0.0520\n",
      "Epoch 76/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0281 - loss: 0.1258 - value_out__loss: 0.0951 - value_out_loss: 0.0620 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0258 - val_loss: 0.1127 - val_value_out__loss: 0.0853 - val_value_out_loss: 0.0523\n",
      "Epoch 77/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0282 - loss: 0.1267 - value_out__loss: 0.0958 - value_out_loss: 0.0636 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0258 - val_loss: 0.1160 - val_value_out__loss: 0.0893 - val_value_out_loss: 0.0563\n",
      "Epoch 78/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0280 - loss: 0.1267 - value_out__loss: 0.0967 - value_out_loss: 0.0642 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0257 - val_loss: 0.1135 - val_value_out__loss: 0.0866 - val_value_out_loss: 0.0537\n",
      "Epoch 79/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0282 - loss: 0.1257 - value_out__loss: 0.0959 - value_out_loss: 0.0631 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0256 - val_loss: 0.1128 - val_value_out__loss: 0.0875 - val_value_out_loss: 0.0535\n",
      "Epoch 80/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0279 - loss: 0.1247 - value_out__loss: 0.0960 - value_out_loss: 0.0628 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0257 - val_loss: 0.1187 - val_value_out__loss: 0.0935 - val_value_out_loss: 0.0602\n",
      "Epoch 81/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0282 - loss: 0.1287 - value_out__loss: 0.1000 - value_out_loss: 0.0664 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0258 - val_loss: 0.1157 - val_value_out__loss: 0.0910 - val_value_out_loss: 0.0559\n",
      "Epoch 82/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0284 - loss: 0.1271 - value_out__loss: 0.0978 - value_out_loss: 0.0642 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0258 - val_loss: 0.1159 - val_value_out__loss: 0.0906 - val_value_out_loss: 0.0564\n",
      "Epoch 83/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0280 - loss: 0.1246 - value_out__loss: 0.0961 - value_out_loss: 0.0625 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0257 - val_loss: 0.1127 - val_value_out__loss: 0.0878 - val_value_out_loss: 0.0534\n",
      "Epoch 84/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0282 - loss: 0.1251 - value_out__loss: 0.0963 - value_out_loss: 0.0629 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0259 - val_loss: 0.1136 - val_value_out__loss: 0.0880 - val_value_out_loss: 0.0542\n",
      "Epoch 85/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0279 - loss: 0.1233 - value_out__loss: 0.0940 - value_out_loss: 0.0608 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0258 - val_loss: 0.1094 - val_value_out__loss: 0.0828 - val_value_out_loss: 0.0495\n",
      "Epoch 86/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0281 - loss: 0.1237 - value_out__loss: 0.0944 - value_out_loss: 0.0614 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0261 - val_loss: 0.1142 - val_value_out__loss: 0.0886 - val_value_out_loss: 0.0549\n",
      "Epoch 87/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0291 - loss: 0.1297 - value_out__loss: 0.1002 - value_out_loss: 0.0667 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0260 - val_loss: 0.1292 - val_value_out__loss: 0.1038 - val_value_out_loss: 0.0696\n",
      "Epoch 88/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0278 - loss: 0.1276 - value_out__loss: 0.0987 - value_out_loss: 0.0652 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0254 - val_loss: 0.1126 - val_value_out__loss: 0.0870 - val_value_out_loss: 0.0538\n",
      "Epoch 89/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0280 - loss: 0.1262 - value_out__loss: 0.0977 - value_out_loss: 0.0647 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0256 - val_loss: 0.1152 - val_value_out__loss: 0.0920 - val_value_out_loss: 0.0575\n",
      "Epoch 90/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0278 - loss: 0.1248 - value_out__loss: 0.0976 - value_out_loss: 0.0647 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0256 - val_loss: 0.1135 - val_value_out__loss: 0.0897 - val_value_out_loss: 0.0565\n",
      "Epoch 91/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0278 - loss: 0.1239 - value_out__loss: 0.0962 - value_out_loss: 0.0637 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0254 - val_loss: 0.1129 - val_value_out__loss: 0.0886 - val_value_out_loss: 0.0552\n",
      "Epoch 92/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0276 - loss: 0.1230 - value_out__loss: 0.0955 - value_out_loss: 0.0626 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0257 - val_loss: 0.1108 - val_value_out__loss: 0.0856 - val_value_out_loss: 0.0533\n",
      "Epoch 93/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0281 - loss: 0.1241 - value_out__loss: 0.0962 - value_out_loss: 0.0639 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0255 - val_loss: 0.1133 - val_value_out__loss: 0.0891 - val_value_out_loss: 0.0563\n",
      "Epoch 94/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0286 - loss: 0.1273 - value_out__loss: 0.0981 - value_out_loss: 0.0659 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0254 - val_loss: 0.1135 - val_value_out__loss: 0.0878 - val_value_out_loss: 0.0555\n",
      "Epoch 95/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0275 - loss: 0.1234 - value_out__loss: 0.0948 - value_out_loss: 0.0628 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0257 - val_loss: 0.1115 - val_value_out__loss: 0.0862 - val_value_out_loss: 0.0539\n",
      "Epoch 96/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0284 - loss: 0.1239 - value_out__loss: 0.0961 - value_out_loss: 0.0639 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0256 - val_loss: 0.1174 - val_value_out__loss: 0.0939 - val_value_out_loss: 0.0610\n",
      "Epoch 97/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0279 - loss: 0.1251 - value_out__loss: 0.0975 - value_out_loss: 0.0651 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0254 - val_loss: 0.1099 - val_value_out__loss: 0.0850 - val_value_out_loss: 0.0532\n",
      "Epoch 98/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0275 - loss: 0.1219 - value_out__loss: 0.0941 - value_out_loss: 0.0624 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0252 - val_loss: 0.1074 - val_value_out__loss: 0.0835 - val_value_out_loss: 0.0513\n",
      "Epoch 99/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0279 - loss: 0.1226 - value_out__loss: 0.0953 - value_out_loss: 0.0637 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0253 - val_loss: 0.1082 - val_value_out__loss: 0.0851 - val_value_out_loss: 0.0521\n",
      "Epoch 100/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0277 - loss: 0.1235 - value_out__loss: 0.0967 - value_out_loss: 0.0642 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0250 - val_loss: 0.1120 - val_value_out__loss: 0.0895 - val_value_out_loss: 0.0562\n",
      "Epoch 101/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0276 - loss: 0.1209 - value_out__loss: 0.0945 - value_out_loss: 0.0616 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0251 - val_loss: 0.1049 - val_value_out__loss: 0.0826 - val_value_out_loss: 0.0493\n",
      "Epoch 102/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0277 - loss: 0.1194 - value_out__loss: 0.0938 - value_out_loss: 0.0610 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0252 - val_loss: 0.1115 - val_value_out__loss: 0.0890 - val_value_out_loss: 0.0565\n",
      "Epoch 103/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0278 - loss: 0.1209 - value_out__loss: 0.0951 - value_out_loss: 0.0626 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0249 - val_loss: 0.1093 - val_value_out__loss: 0.0872 - val_value_out_loss: 0.0541\n",
      "Epoch 104/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0278 - loss: 0.1206 - value_out__loss: 0.0949 - value_out_loss: 0.0619 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0248 - val_loss: 0.1052 - val_value_out__loss: 0.0837 - val_value_out_loss: 0.0502\n",
      "Epoch 105/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0275 - loss: 0.1204 - value_out__loss: 0.0953 - value_out_loss: 0.0623 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0249 - val_loss: 0.1082 - val_value_out__loss: 0.0875 - val_value_out_loss: 0.0535\n",
      "Epoch 106/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0275 - loss: 0.1189 - value_out__loss: 0.0944 - value_out_loss: 0.0610 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0248 - val_loss: 0.1049 - val_value_out__loss: 0.0839 - val_value_out_loss: 0.0501\n",
      "Epoch 107/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0274 - loss: 0.1173 - value_out__loss: 0.0927 - value_out_loss: 0.0594 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0248 - val_loss: 0.1049 - val_value_out__loss: 0.0835 - val_value_out_loss: 0.0503\n",
      "Epoch 108/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0273 - loss: 0.1177 - value_out__loss: 0.0926 - value_out_loss: 0.0601 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0248 - val_loss: 0.1038 - val_value_out__loss: 0.0823 - val_value_out_loss: 0.0495\n",
      "Epoch 109/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0274 - loss: 0.1186 - value_out__loss: 0.0938 - value_out_loss: 0.0615 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0249 - val_loss: 0.1083 - val_value_out__loss: 0.0865 - val_value_out_loss: 0.0539\n",
      "Epoch 110/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0276 - loss: 0.1199 - value_out__loss: 0.0940 - value_out_loss: 0.0625 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0247 - val_loss: 0.1052 - val_value_out__loss: 0.0836 - val_value_out_loss: 0.0511\n",
      "Epoch 111/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0272 - loss: 0.1191 - value_out__loss: 0.0938 - value_out_loss: 0.0619 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0248 - val_loss: 0.1065 - val_value_out__loss: 0.0846 - val_value_out_loss: 0.0526\n",
      "Epoch 112/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0276 - loss: 0.1193 - value_out__loss: 0.0934 - value_out_loss: 0.0615 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0246 - val_loss: 0.1068 - val_value_out__loss: 0.0850 - val_value_out_loss: 0.0521\n",
      "Epoch 113/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0273 - loss: 0.1182 - value_out__loss: 0.0928 - value_out_loss: 0.0608 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0247 - val_loss: 0.1046 - val_value_out__loss: 0.0829 - val_value_out_loss: 0.0507\n",
      "Epoch 114/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0276 - loss: 0.1207 - value_out__loss: 0.0951 - value_out_loss: 0.0632 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0248 - val_loss: 0.1080 - val_value_out__loss: 0.0860 - val_value_out_loss: 0.0533\n",
      "Epoch 115/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0272 - loss: 0.1187 - value_out__loss: 0.0933 - value_out_loss: 0.0612 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0244 - val_loss: 0.1086 - val_value_out__loss: 0.0876 - val_value_out_loss: 0.0549\n",
      "Epoch 116/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0275 - loss: 0.1199 - value_out__loss: 0.0944 - value_out_loss: 0.0624 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0246 - val_loss: 0.1056 - val_value_out__loss: 0.0844 - val_value_out_loss: 0.0522\n",
      "Epoch 117/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0273 - loss: 0.1181 - value_out__loss: 0.0933 - value_out_loss: 0.0613 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0250 - val_loss: 0.1057 - val_value_out__loss: 0.0838 - val_value_out_loss: 0.0517\n",
      "Epoch 118/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0274 - loss: 0.1205 - value_out__loss: 0.0952 - value_out_loss: 0.0634 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0244 - val_loss: 0.1041 - val_value_out__loss: 0.0832 - val_value_out_loss: 0.0506\n",
      "Epoch 119/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0272 - loss: 0.1182 - value_out__loss: 0.0936 - value_out_loss: 0.0615 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0243 - val_loss: 0.1049 - val_value_out__loss: 0.0842 - val_value_out_loss: 0.0516\n",
      "Epoch 120/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0274 - loss: 0.1181 - value_out__loss: 0.0932 - value_out_loss: 0.0611 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0245 - val_loss: 0.1025 - val_value_out__loss: 0.0817 - val_value_out_loss: 0.0493\n",
      "Epoch 121/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0273 - loss: 0.1175 - value_out__loss: 0.0932 - value_out_loss: 0.0611 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0245 - val_loss: 0.1061 - val_value_out__loss: 0.0856 - val_value_out_loss: 0.0536\n",
      "Epoch 122/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0276 - loss: 0.1193 - value_out__loss: 0.0949 - value_out_loss: 0.0626 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0245 - val_loss: 0.1074 - val_value_out__loss: 0.0875 - val_value_out_loss: 0.0545\n",
      "Epoch 123/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0277 - loss: 0.1205 - value_out__loss: 0.0958 - value_out_loss: 0.0637 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0244 - val_loss: 0.1027 - val_value_out__loss: 0.0822 - val_value_out_loss: 0.0500\n",
      "Epoch 124/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0273 - loss: 0.1168 - value_out__loss: 0.0923 - value_out_loss: 0.0608 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0245 - val_loss: 0.1033 - val_value_out__loss: 0.0827 - val_value_out_loss: 0.0505\n",
      "Epoch 125/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0272 - loss: 0.1172 - value_out__loss: 0.0932 - value_out_loss: 0.0615 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0243 - val_loss: 0.1064 - val_value_out__loss: 0.0870 - val_value_out_loss: 0.0542\n",
      "Epoch 126/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0272 - loss: 0.1182 - value_out__loss: 0.0945 - value_out_loss: 0.0625 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0244 - val_loss: 0.1081 - val_value_out__loss: 0.0890 - val_value_out_loss: 0.0560\n",
      "Epoch 127/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0272 - loss: 0.1168 - value_out__loss: 0.0936 - value_out_loss: 0.0613 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0248 - val_loss: 0.1045 - val_value_out__loss: 0.0849 - val_value_out_loss: 0.0518\n",
      "Epoch 128/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0275 - loss: 0.1192 - value_out__loss: 0.0957 - value_out_loss: 0.0631 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0243 - val_loss: 0.1045 - val_value_out__loss: 0.0847 - val_value_out_loss: 0.0516\n",
      "Epoch 129/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0272 - loss: 0.1171 - value_out__loss: 0.0933 - value_out_loss: 0.0612 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0244 - val_loss: 0.1014 - val_value_out__loss: 0.0815 - val_value_out_loss: 0.0493\n",
      "Epoch 130/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0276 - loss: 0.1196 - value_out__loss: 0.0951 - value_out_loss: 0.0633 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0241 - val_loss: 0.1071 - val_value_out__loss: 0.0872 - val_value_out_loss: 0.0545\n",
      "Epoch 131/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0272 - loss: 0.1185 - value_out__loss: 0.0942 - value_out_loss: 0.0624 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0243 - val_loss: 0.1045 - val_value_out__loss: 0.0843 - val_value_out_loss: 0.0524\n",
      "Epoch 132/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0274 - loss: 0.1172 - value_out__loss: 0.0940 - value_out_loss: 0.0620 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0255 - val_loss: 0.1073 - val_value_out__loss: 0.0871 - val_value_out_loss: 0.0548\n",
      "Epoch 133/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0279 - loss: 0.1197 - value_out__loss: 0.0953 - value_out_loss: 0.0637 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0243 - val_loss: 0.1054 - val_value_out__loss: 0.0842 - val_value_out_loss: 0.0528\n",
      "Epoch 134/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0270 - loss: 0.1156 - value_out__loss: 0.0911 - value_out_loss: 0.0597 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0244 - val_loss: 0.1042 - val_value_out__loss: 0.0833 - val_value_out_loss: 0.0519\n",
      "Epoch 135/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0270 - loss: 0.1152 - value_out__loss: 0.0911 - value_out_loss: 0.0601 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0243 - val_loss: 0.1030 - val_value_out__loss: 0.0828 - val_value_out_loss: 0.0518\n",
      "Epoch 136/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0273 - loss: 0.1166 - value_out__loss: 0.0927 - value_out_loss: 0.0618 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0241 - val_loss: 0.1022 - val_value_out__loss: 0.0830 - val_value_out_loss: 0.0514\n",
      "Epoch 137/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0272 - loss: 0.1153 - value_out__loss: 0.0928 - value_out_loss: 0.0610 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0240 - val_loss: 0.1032 - val_value_out__loss: 0.0855 - val_value_out_loss: 0.0530\n",
      "Epoch 138/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0271 - loss: 0.1136 - value_out__loss: 0.0917 - value_out_loss: 0.0600 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0241 - val_loss: 0.0989 - val_value_out__loss: 0.0808 - val_value_out_loss: 0.0488\n",
      "Epoch 139/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0280 - loss: 0.1184 - value_out__loss: 0.0947 - value_out_loss: 0.0634 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0244 - val_loss: 0.1076 - val_value_out__loss: 0.0872 - val_value_out_loss: 0.0558\n",
      "Epoch 140/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0269 - loss: 0.1182 - value_out__loss: 0.0940 - value_out_loss: 0.0626 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0238 - val_loss: 0.1067 - val_value_out__loss: 0.0867 - val_value_out_loss: 0.0550\n",
      "Epoch 141/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0271 - loss: 0.1165 - value_out__loss: 0.0926 - value_out_loss: 0.0611 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0239 - val_loss: 0.0995 - val_value_out__loss: 0.0806 - val_value_out_loss: 0.0487\n",
      "Epoch 142/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0274 - loss: 0.1149 - value_out__loss: 0.0920 - value_out_loss: 0.0606 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0241 - val_loss: 0.0992 - val_value_out__loss: 0.0812 - val_value_out_loss: 0.0494\n",
      "Epoch 143/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0278 - loss: 0.1167 - value_out__loss: 0.0943 - value_out_loss: 0.0628 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0241 - val_loss: 0.0994 - val_value_out__loss: 0.0812 - val_value_out_loss: 0.0497\n",
      "Epoch 144/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0270 - loss: 0.1147 - value_out__loss: 0.0922 - value_out_loss: 0.0611 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0239 - val_loss: 0.1049 - val_value_out__loss: 0.0855 - val_value_out_loss: 0.0543\n",
      "Epoch 145/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0269 - loss: 0.1141 - value_out__loss: 0.0907 - value_out_loss: 0.0601 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0240 - val_loss: 0.0969 - val_value_out__loss: 0.0777 - val_value_out_loss: 0.0466\n",
      "Epoch 146/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0270 - loss: 0.1135 - value_out__loss: 0.0908 - value_out_loss: 0.0600 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0241 - val_loss: 0.1020 - val_value_out__loss: 0.0831 - val_value_out_loss: 0.0528\n",
      "Epoch 147/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0272 - loss: 0.1153 - value_out__loss: 0.0934 - value_out_loss: 0.0620 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0240 - val_loss: 0.0976 - val_value_out__loss: 0.0805 - val_value_out_loss: 0.0484\n",
      "Epoch 148/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0270 - loss: 0.1129 - value_out__loss: 0.0917 - value_out_loss: 0.0602 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0240 - val_loss: 0.0993 - val_value_out__loss: 0.0811 - val_value_out_loss: 0.0497\n",
      "Epoch 149/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0268 - loss: 0.1119 - value_out__loss: 0.0902 - value_out_loss: 0.0593 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0239 - val_loss: 0.0981 - val_value_out__loss: 0.0804 - val_value_out_loss: 0.0495\n",
      "Epoch 150/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0276 - loss: 0.1173 - value_out__loss: 0.0952 - value_out_loss: 0.0639 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0238 - val_loss: 0.1029 - val_value_out__loss: 0.0836 - val_value_out_loss: 0.0535\n",
      "Epoch 151/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0270 - loss: 0.1145 - value_out__loss: 0.0911 - value_out_loss: 0.0608 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0240 - val_loss: 0.1020 - val_value_out__loss: 0.0821 - val_value_out_loss: 0.0524\n",
      "Epoch 152/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0270 - loss: 0.1162 - value_out__loss: 0.0929 - value_out_loss: 0.0630 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0237 - val_loss: 0.1070 - val_value_out__loss: 0.0897 - val_value_out_loss: 0.0570\n",
      "Epoch 153/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0268 - loss: 0.1163 - value_out__loss: 0.0946 - value_out_loss: 0.0633 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0241 - val_loss: 0.1047 - val_value_out__loss: 0.0862 - val_value_out_loss: 0.0557\n",
      "Epoch 154/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0270 - loss: 0.1142 - value_out__loss: 0.0926 - value_out_loss: 0.0621 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0239 - val_loss: 0.1028 - val_value_out__loss: 0.0859 - val_value_out_loss: 0.0542\n",
      "Epoch 155/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0273 - loss: 0.1165 - value_out__loss: 0.0946 - value_out_loss: 0.0639 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0243 - val_loss: 0.1053 - val_value_out__loss: 0.0856 - val_value_out_loss: 0.0561\n",
      "Epoch 156/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0270 - loss: 0.1160 - value_out__loss: 0.0930 - value_out_loss: 0.0629 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0239 - val_loss: 0.0999 - val_value_out__loss: 0.0808 - val_value_out_loss: 0.0503\n",
      "Epoch 157/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0270 - loss: 0.1149 - value_out__loss: 0.0924 - value_out_loss: 0.0619 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0240 - val_loss: 0.0980 - val_value_out__loss: 0.0792 - val_value_out_loss: 0.0492\n",
      "Epoch 158/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0273 - loss: 0.1149 - value_out__loss: 0.0925 - value_out_loss: 0.0624 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0241 - val_loss: 0.1036 - val_value_out__loss: 0.0850 - val_value_out_loss: 0.0546\n",
      "Epoch 159/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0269 - loss: 0.1144 - value_out__loss: 0.0921 - value_out_loss: 0.0620 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0236 - val_loss: 0.1030 - val_value_out__loss: 0.0842 - val_value_out_loss: 0.0541\n",
      "Epoch 160/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0270 - loss: 0.1147 - value_out__loss: 0.0917 - value_out_loss: 0.0621 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0238 - val_loss: 0.0982 - val_value_out__loss: 0.0794 - val_value_out_loss: 0.0495\n",
      "Epoch 161/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0271 - loss: 0.1142 - value_out__loss: 0.0918 - value_out_loss: 0.0618 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0242 - val_loss: 0.1028 - val_value_out__loss: 0.0847 - val_value_out_loss: 0.0534\n",
      "Epoch 162/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0272 - loss: 0.1134 - value_out__loss: 0.0912 - value_out_loss: 0.0611 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0237 - val_loss: 0.0971 - val_value_out__loss: 0.0788 - val_value_out_loss: 0.0485\n",
      "Epoch 163/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0269 - loss: 0.1133 - value_out__loss: 0.0911 - value_out_loss: 0.0610 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0238 - val_loss: 0.1011 - val_value_out__loss: 0.0836 - val_value_out_loss: 0.0527\n",
      "Epoch 164/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0270 - loss: 0.1132 - value_out__loss: 0.0919 - value_out_loss: 0.0608 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0238 - val_loss: 0.1009 - val_value_out__loss: 0.0839 - val_value_out_loss: 0.0523\n",
      "Epoch 165/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0268 - loss: 0.1123 - value_out__loss: 0.0915 - value_out_loss: 0.0603 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0236 - val_loss: 0.0992 - val_value_out__loss: 0.0830 - val_value_out_loss: 0.0509\n",
      "Epoch 166/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0273 - loss: 0.1138 - value_out__loss: 0.0925 - value_out_loss: 0.0613 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0238 - val_loss: 0.1008 - val_value_out__loss: 0.0843 - val_value_out_loss: 0.0523\n",
      "Epoch 167/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0271 - loss: 0.1147 - value_out__loss: 0.0941 - value_out_loss: 0.0625 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0240 - val_loss: 0.1009 - val_value_out__loss: 0.0847 - val_value_out_loss: 0.0519\n",
      "Epoch 168/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0269 - loss: 0.1131 - value_out__loss: 0.0927 - value_out_loss: 0.0609 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0236 - val_loss: 0.0948 - val_value_out__loss: 0.0777 - val_value_out_loss: 0.0461\n",
      "Epoch 169/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0269 - loss: 0.1129 - value_out__loss: 0.0921 - value_out_loss: 0.0609 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0237 - val_loss: 0.0957 - val_value_out__loss: 0.0791 - val_value_out_loss: 0.0478\n",
      "Epoch 170/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0269 - loss: 0.1118 - value_out__loss: 0.0913 - value_out_loss: 0.0603 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0239 - val_loss: 0.1020 - val_value_out__loss: 0.0863 - val_value_out_loss: 0.0544\n",
      "Epoch 171/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0275 - loss: 0.1150 - value_out__loss: 0.0941 - value_out_loss: 0.0624 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0237 - val_loss: 0.0986 - val_value_out__loss: 0.0810 - val_value_out_loss: 0.0493\n",
      "Epoch 172/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0269 - loss: 0.1128 - value_out__loss: 0.0909 - value_out_loss: 0.0596 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0237 - val_loss: 0.0991 - val_value_out__loss: 0.0804 - val_value_out_loss: 0.0492\n",
      "Epoch 173/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0268 - loss: 0.1128 - value_out__loss: 0.0909 - value_out_loss: 0.0597 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0237 - val_loss: 0.1006 - val_value_out__loss: 0.0843 - val_value_out_loss: 0.0522\n",
      "Epoch 174/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0273 - loss: 0.1148 - value_out__loss: 0.0941 - value_out_loss: 0.0622 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0236 - val_loss: 0.0944 - val_value_out__loss: 0.0780 - val_value_out_loss: 0.0460\n",
      "Epoch 175/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0269 - loss: 0.1117 - value_out__loss: 0.0913 - value_out_loss: 0.0597 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0235 - val_loss: 0.0955 - val_value_out__loss: 0.0798 - val_value_out_loss: 0.0474\n",
      "Epoch 176/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0268 - loss: 0.1119 - value_out__loss: 0.0926 - value_out_loss: 0.0603 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0236 - val_loss: 0.0981 - val_value_out__loss: 0.0829 - val_value_out_loss: 0.0503\n",
      "Epoch 177/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0267 - loss: 0.1106 - value_out__loss: 0.0915 - value_out_loss: 0.0590 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0237 - val_loss: 0.0955 - val_value_out__loss: 0.0810 - val_value_out_loss: 0.0478\n",
      "Epoch 178/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0272 - loss: 0.1133 - value_out__loss: 0.0942 - value_out_loss: 0.0615 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0236 - val_loss: 0.0983 - val_value_out__loss: 0.0836 - val_value_out_loss: 0.0501\n",
      "Epoch 179/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0270 - loss: 0.1126 - value_out__loss: 0.0932 - value_out_loss: 0.0603 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0235 - val_loss: 0.0950 - val_value_out__loss: 0.0799 - val_value_out_loss: 0.0467\n",
      "Epoch 180/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0266 - loss: 0.1102 - value_out__loss: 0.0913 - value_out_loss: 0.0587 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0235 - val_loss: 0.0949 - val_value_out__loss: 0.0805 - val_value_out_loss: 0.0473\n",
      "Epoch 181/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0270 - loss: 0.1117 - value_out__loss: 0.0927 - value_out_loss: 0.0601 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0237 - val_loss: 0.0964 - val_value_out__loss: 0.0821 - val_value_out_loss: 0.0491\n",
      "Epoch 182/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0268 - loss: 0.1104 - value_out__loss: 0.0917 - value_out_loss: 0.0590 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0236 - val_loss: 0.0964 - val_value_out__loss: 0.0811 - val_value_out_loss: 0.0485\n",
      "Epoch 183/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0269 - loss: 0.1115 - value_out__loss: 0.0915 - value_out_loss: 0.0597 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0234 - val_loss: 0.0985 - val_value_out__loss: 0.0831 - val_value_out_loss: 0.0508\n",
      "Epoch 184/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0268 - loss: 0.1128 - value_out__loss: 0.0940 - value_out_loss: 0.0613 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0240 - val_loss: 0.0947 - val_value_out__loss: 0.0803 - val_value_out_loss: 0.0475\n",
      "Epoch 185/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0269 - loss: 0.1123 - value_out__loss: 0.0940 - value_out_loss: 0.0613 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0237 - val_loss: 0.1004 - val_value_out__loss: 0.0852 - val_value_out_loss: 0.0525\n",
      "Epoch 186/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0270 - loss: 0.1128 - value_out__loss: 0.0931 - value_out_loss: 0.0614 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0235 - val_loss: 0.0991 - val_value_out__loss: 0.0838 - val_value_out_loss: 0.0507\n",
      "Epoch 187/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0267 - loss: 0.1115 - value_out__loss: 0.0919 - value_out_loss: 0.0601 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0240 - val_loss: 0.0958 - val_value_out__loss: 0.0801 - val_value_out_loss: 0.0481\n",
      "Epoch 188/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0271 - loss: 0.1138 - value_out__loss: 0.0946 - value_out_loss: 0.0621 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0239 - val_loss: 0.1032 - val_value_out__loss: 0.0882 - val_value_out_loss: 0.0556\n",
      "Epoch 189/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0268 - loss: 0.1115 - value_out__loss: 0.0928 - value_out_loss: 0.0605 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0241 - val_loss: 0.0974 - val_value_out__loss: 0.0820 - val_value_out_loss: 0.0498\n",
      "Epoch 190/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0268 - loss: 0.1114 - value_out__loss: 0.0924 - value_out_loss: 0.0607 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0236 - val_loss: 0.0972 - val_value_out__loss: 0.0825 - val_value_out_loss: 0.0496\n",
      "Epoch 191/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0269 - loss: 0.1110 - value_out__loss: 0.0920 - value_out_loss: 0.0600 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0236 - val_loss: 0.0946 - val_value_out__loss: 0.0794 - val_value_out_loss: 0.0475\n",
      "Epoch 192/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0269 - loss: 0.1103 - value_out__loss: 0.0907 - value_out_loss: 0.0595 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0236 - val_loss: 0.0958 - val_value_out__loss: 0.0798 - val_value_out_loss: 0.0477\n",
      "Epoch 193/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0268 - loss: 0.1109 - value_out__loss: 0.0908 - value_out_loss: 0.0596 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0234 - val_loss: 0.0978 - val_value_out__loss: 0.0826 - val_value_out_loss: 0.0502\n",
      "Epoch 194/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0268 - loss: 0.1121 - value_out__loss: 0.0929 - value_out_loss: 0.0606 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0237 - val_loss: 0.0946 - val_value_out__loss: 0.0800 - val_value_out_loss: 0.0477\n",
      "Epoch 195/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0268 - loss: 0.1096 - value_out__loss: 0.0912 - value_out_loss: 0.0591 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0235 - val_loss: 0.0949 - val_value_out__loss: 0.0803 - val_value_out_loss: 0.0480\n",
      "Epoch 196/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0267 - loss: 0.1092 - value_out__loss: 0.0908 - value_out_loss: 0.0587 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0235 - val_loss: 0.1001 - val_value_out__loss: 0.0853 - val_value_out_loss: 0.0530\n",
      "Epoch 197/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0269 - loss: 0.1122 - value_out__loss: 0.0937 - value_out_loss: 0.0616 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0235 - val_loss: 0.0938 - val_value_out__loss: 0.0796 - val_value_out_loss: 0.0467\n",
      "Epoch 198/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0265 - loss: 0.1093 - value_out__loss: 0.0911 - value_out_loss: 0.0589 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0236 - val_loss: 0.0989 - val_value_out__loss: 0.0844 - val_value_out_loss: 0.0521\n",
      "Epoch 199/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0273 - loss: 0.1129 - value_out__loss: 0.0937 - value_out_loss: 0.0618 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0234 - val_loss: 0.0944 - val_value_out__loss: 0.0795 - val_value_out_loss: 0.0477\n",
      "Epoch 200/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0266 - loss: 0.1112 - value_out__loss: 0.0920 - value_out_loss: 0.0606 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0234 - val_loss: 0.0949 - val_value_out__loss: 0.0788 - val_value_out_loss: 0.0475\n",
      "Epoch 201/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0268 - loss: 0.1095 - value_out__loss: 0.0895 - value_out_loss: 0.0586 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0237 - val_loss: 0.0935 - val_value_out__loss: 0.0772 - val_value_out_loss: 0.0465\n",
      "Epoch 202/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0272 - loss: 0.1113 - value_out__loss: 0.0915 - value_out_loss: 0.0604 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0236 - val_loss: 0.0977 - val_value_out__loss: 0.0820 - val_value_out_loss: 0.0505\n",
      "Epoch 203/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0270 - loss: 0.1108 - value_out__loss: 0.0905 - value_out_loss: 0.0595 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0236 - val_loss: 0.0982 - val_value_out__loss: 0.0824 - val_value_out_loss: 0.0509\n",
      "Epoch 204/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0277 - loss: 0.1146 - value_out__loss: 0.0941 - value_out_loss: 0.0623 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0234 - val_loss: 0.0963 - val_value_out__loss: 0.0811 - val_value_out_loss: 0.0484\n",
      "Epoch 205/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0269 - loss: 0.1134 - value_out__loss: 0.0930 - value_out_loss: 0.0612 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0238 - val_loss: 0.1044 - val_value_out__loss: 0.0881 - val_value_out_loss: 0.0561\n",
      "Epoch 206/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0265 - loss: 0.1110 - value_out__loss: 0.0915 - value_out_loss: 0.0599 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0235 - val_loss: 0.0950 - val_value_out__loss: 0.0806 - val_value_out_loss: 0.0479\n",
      "Epoch 207/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0269 - loss: 0.1103 - value_out__loss: 0.0919 - value_out_loss: 0.0601 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0240 - val_loss: 0.0961 - val_value_out__loss: 0.0812 - val_value_out_loss: 0.0497\n",
      "Epoch 208/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0268 - loss: 0.1110 - value_out__loss: 0.0924 - value_out_loss: 0.0609 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0234 - val_loss: 0.1006 - val_value_out__loss: 0.0856 - val_value_out_loss: 0.0539\n",
      "Epoch 209/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0267 - loss: 0.1119 - value_out__loss: 0.0935 - value_out_loss: 0.0615 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0235 - val_loss: 0.1031 - val_value_out__loss: 0.0887 - val_value_out_loss: 0.0560\n",
      "Epoch 210/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0268 - loss: 0.1125 - value_out__loss: 0.0937 - value_out_loss: 0.0616 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0236 - val_loss: 0.0943 - val_value_out__loss: 0.0800 - val_value_out_loss: 0.0472\n",
      "Epoch 211/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0271 - loss: 0.1118 - value_out__loss: 0.0938 - value_out_loss: 0.0612 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0236 - val_loss: 0.0969 - val_value_out__loss: 0.0831 - val_value_out_loss: 0.0504\n",
      "Epoch 212/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0273 - loss: 0.1141 - value_out__loss: 0.0954 - value_out_loss: 0.0630 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0233 - val_loss: 0.0942 - val_value_out__loss: 0.0796 - val_value_out_loss: 0.0474\n",
      "Epoch 213/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0265 - loss: 0.1099 - value_out__loss: 0.0915 - value_out_loss: 0.0592 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0237 - val_loss: 0.0961 - val_value_out__loss: 0.0817 - val_value_out_loss: 0.0491\n",
      "Epoch 214/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0267 - loss: 0.1104 - value_out__loss: 0.0923 - value_out_loss: 0.0599 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0237 - val_loss: 0.0956 - val_value_out__loss: 0.0822 - val_value_out_loss: 0.0487\n",
      "Epoch 215/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0266 - loss: 0.1098 - value_out__loss: 0.0926 - value_out_loss: 0.0599 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0236 - val_loss: 0.0964 - val_value_out__loss: 0.0832 - val_value_out_loss: 0.0505\n",
      "Epoch 216/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0267 - loss: 0.1102 - value_out__loss: 0.0927 - value_out_loss: 0.0603 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0234 - val_loss: 0.0979 - val_value_out__loss: 0.0854 - val_value_out_loss: 0.0517\n",
      "Epoch 217/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0269 - loss: 0.1128 - value_out__loss: 0.0949 - value_out_loss: 0.0627 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0242 - val_loss: 0.1010 - val_value_out__loss: 0.0860 - val_value_out_loss: 0.0535\n",
      "Epoch 218/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0278 - loss: 0.1154 - value_out__loss: 0.0957 - value_out_loss: 0.0635 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0242 - val_loss: 0.1024 - val_value_out__loss: 0.0872 - val_value_out_loss: 0.0538\n",
      "Epoch 219/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0267 - loss: 0.1110 - value_out__loss: 0.0918 - value_out_loss: 0.0591 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0233 - val_loss: 0.0936 - val_value_out__loss: 0.0779 - val_value_out_loss: 0.0455\n",
      "Epoch 220/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0265 - loss: 0.1084 - value_out__loss: 0.0890 - value_out_loss: 0.0573 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0242 - val_loss: 0.0978 - val_value_out__loss: 0.0818 - val_value_out_loss: 0.0505\n",
      "Epoch 221/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0269 - loss: 0.1116 - value_out__loss: 0.0926 - value_out_loss: 0.0612 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0234 - val_loss: 0.0965 - val_value_out__loss: 0.0829 - val_value_out_loss: 0.0505\n",
      "Epoch 222/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0271 - loss: 0.1109 - value_out__loss: 0.0924 - value_out_loss: 0.0607 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0233 - val_loss: 0.0954 - val_value_out__loss: 0.0817 - val_value_out_loss: 0.0492\n",
      "Epoch 223/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0266 - loss: 0.1096 - value_out__loss: 0.0914 - value_out_loss: 0.0597 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0236 - val_loss: 0.0977 - val_value_out__loss: 0.0832 - val_value_out_loss: 0.0519\n",
      "Epoch 224/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0266 - loss: 0.1114 - value_out__loss: 0.0930 - value_out_loss: 0.0611 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0234 - val_loss: 0.0952 - val_value_out__loss: 0.0801 - val_value_out_loss: 0.0485\n",
      "Epoch 225/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0269 - loss: 0.1108 - value_out__loss: 0.0921 - value_out_loss: 0.0601 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0236 - val_loss: 0.0947 - val_value_out__loss: 0.0799 - val_value_out_loss: 0.0475\n",
      "Epoch 226/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0266 - loss: 0.1093 - value_out__loss: 0.0909 - value_out_loss: 0.0591 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0234 - val_loss: 0.0959 - val_value_out__loss: 0.0819 - val_value_out_loss: 0.0499\n",
      "Epoch 227/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0267 - loss: 0.1109 - value_out__loss: 0.0927 - value_out_loss: 0.0608 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0236 - val_loss: 0.0986 - val_value_out__loss: 0.0844 - val_value_out_loss: 0.0522\n",
      "Epoch 228/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0267 - loss: 0.1092 - value_out__loss: 0.0912 - value_out_loss: 0.0595 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0916 - val_value_out__loss: 0.0770 - val_value_out_loss: 0.0454\n",
      "Epoch 229/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0267 - loss: 0.1090 - value_out__loss: 0.0904 - value_out_loss: 0.0587 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0233 - val_loss: 0.0922 - val_value_out__loss: 0.0776 - val_value_out_loss: 0.0455\n",
      "Epoch 230/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0266 - loss: 0.1091 - value_out__loss: 0.0907 - value_out_loss: 0.0587 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0234 - val_loss: 0.0970 - val_value_out__loss: 0.0824 - val_value_out_loss: 0.0501\n",
      "Epoch 231/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0266 - loss: 0.1087 - value_out__loss: 0.0901 - value_out_loss: 0.0583 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0235 - val_loss: 0.0978 - val_value_out__loss: 0.0835 - val_value_out_loss: 0.0517\n",
      "Epoch 232/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0267 - loss: 0.1088 - value_out__loss: 0.0907 - value_out_loss: 0.0591 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0239 - val_loss: 0.0983 - val_value_out__loss: 0.0842 - val_value_out_loss: 0.0518\n",
      "Epoch 233/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0272 - loss: 0.1129 - value_out__loss: 0.0943 - value_out_loss: 0.0627 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0234 - val_loss: 0.0935 - val_value_out__loss: 0.0789 - val_value_out_loss: 0.0471\n",
      "Epoch 234/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0267 - loss: 0.1105 - value_out__loss: 0.0917 - value_out_loss: 0.0597 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0236 - val_loss: 0.0943 - val_value_out__loss: 0.0797 - val_value_out_loss: 0.0477\n",
      "Epoch 235/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0266 - loss: 0.1089 - value_out__loss: 0.0906 - value_out_loss: 0.0586 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0233 - val_loss: 0.0950 - val_value_out__loss: 0.0805 - val_value_out_loss: 0.0489\n",
      "Epoch 236/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0270 - loss: 0.1090 - value_out__loss: 0.0913 - value_out_loss: 0.0593 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0233 - val_loss: 0.0924 - val_value_out__loss: 0.0793 - val_value_out_loss: 0.0466\n",
      "Epoch 237/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0266 - loss: 0.1074 - value_out__loss: 0.0903 - value_out_loss: 0.0583 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0236 - val_loss: 0.0944 - val_value_out__loss: 0.0812 - val_value_out_loss: 0.0489\n",
      "Epoch 238/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0262 - loss: 0.1071 - value_out__loss: 0.0907 - value_out_loss: 0.0584 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0910 - val_value_out__loss: 0.0776 - val_value_out_loss: 0.0455\n",
      "Epoch 239/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0266 - loss: 0.1078 - value_out__loss: 0.0903 - value_out_loss: 0.0588 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0238 - val_loss: 0.0935 - val_value_out__loss: 0.0796 - val_value_out_loss: 0.0479\n",
      "Epoch 240/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0275 - loss: 0.1130 - value_out__loss: 0.0941 - value_out_loss: 0.0625 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0238 - val_loss: 0.1026 - val_value_out__loss: 0.0871 - val_value_out_loss: 0.0548\n",
      "Epoch 241/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0264 - loss: 0.1114 - value_out__loss: 0.0919 - value_out_loss: 0.0598 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0231 - val_loss: 0.0949 - val_value_out__loss: 0.0783 - val_value_out_loss: 0.0461\n",
      "Epoch 242/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0266 - loss: 0.1103 - value_out__loss: 0.0898 - value_out_loss: 0.0584 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0233 - val_loss: 0.0922 - val_value_out__loss: 0.0764 - val_value_out_loss: 0.0453\n",
      "Epoch 243/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0269 - loss: 0.1101 - value_out__loss: 0.0910 - value_out_loss: 0.0600 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0234 - val_loss: 0.0951 - val_value_out__loss: 0.0810 - val_value_out_loss: 0.0498\n",
      "Epoch 244/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0264 - loss: 0.1087 - value_out__loss: 0.0912 - value_out_loss: 0.0597 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0235 - val_loss: 0.0966 - val_value_out__loss: 0.0823 - val_value_out_loss: 0.0517\n",
      "Epoch 245/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0268 - loss: 0.1101 - value_out__loss: 0.0920 - value_out_loss: 0.0609 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0953 - val_value_out__loss: 0.0812 - val_value_out_loss: 0.0494\n",
      "Epoch 246/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0266 - loss: 0.1098 - value_out__loss: 0.0914 - value_out_loss: 0.0601 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0969 - val_value_out__loss: 0.0809 - val_value_out_loss: 0.0508\n",
      "Epoch 247/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0263 - loss: 0.1078 - value_out__loss: 0.0894 - value_out_loss: 0.0581 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0235 - val_loss: 0.0922 - val_value_out__loss: 0.0780 - val_value_out_loss: 0.0465\n",
      "Epoch 248/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0273 - loss: 0.1119 - value_out__loss: 0.0933 - value_out_loss: 0.0618 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0233 - val_loss: 0.0960 - val_value_out__loss: 0.0815 - val_value_out_loss: 0.0502\n",
      "Epoch 249/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0264 - loss: 0.1086 - value_out__loss: 0.0906 - value_out_loss: 0.0592 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0233 - val_loss: 0.0916 - val_value_out__loss: 0.0778 - val_value_out_loss: 0.0459\n",
      "Epoch 250/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0265 - loss: 0.1097 - value_out__loss: 0.0927 - value_out_loss: 0.0604 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0234 - val_loss: 0.0917 - val_value_out__loss: 0.0783 - val_value_out_loss: 0.0467\n",
      "Epoch 251/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0269 - loss: 0.1099 - value_out__loss: 0.0926 - value_out_loss: 0.0609 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0233 - val_loss: 0.0938 - val_value_out__loss: 0.0811 - val_value_out_loss: 0.0486\n",
      "Epoch 252/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0266 - loss: 0.1083 - value_out__loss: 0.0911 - value_out_loss: 0.0588 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0233 - val_loss: 0.0938 - val_value_out__loss: 0.0803 - val_value_out_loss: 0.0478\n",
      "Epoch 253/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0269 - loss: 0.1100 - value_out__loss: 0.0926 - value_out_loss: 0.0604 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0233 - val_loss: 0.0928 - val_value_out__loss: 0.0800 - val_value_out_loss: 0.0470\n",
      "Epoch 254/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0269 - loss: 0.1104 - value_out__loss: 0.0928 - value_out_loss: 0.0602 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0233 - val_loss: 0.0939 - val_value_out__loss: 0.0801 - val_value_out_loss: 0.0474\n",
      "Epoch 255/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0265 - loss: 0.1084 - value_out__loss: 0.0905 - value_out_loss: 0.0582 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0233 - val_loss: 0.0926 - val_value_out__loss: 0.0785 - val_value_out_loss: 0.0465\n",
      "Epoch 256/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0267 - loss: 0.1093 - value_out__loss: 0.0918 - value_out_loss: 0.0600 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0234 - val_loss: 0.0971 - val_value_out__loss: 0.0835 - val_value_out_loss: 0.0523\n",
      "Epoch 257/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0267 - loss: 0.1087 - value_out__loss: 0.0917 - value_out_loss: 0.0599 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0233 - val_loss: 0.0948 - val_value_out__loss: 0.0816 - val_value_out_loss: 0.0493\n",
      "Epoch 258/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0262 - loss: 0.1076 - value_out__loss: 0.0906 - value_out_loss: 0.0587 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0234 - val_loss: 0.0935 - val_value_out__loss: 0.0804 - val_value_out_loss: 0.0480\n",
      "Epoch 259/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0266 - loss: 0.1087 - value_out__loss: 0.0918 - value_out_loss: 0.0600 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0912 - val_value_out__loss: 0.0787 - val_value_out_loss: 0.0461\n",
      "Epoch 260/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0266 - loss: 0.1071 - value_out__loss: 0.0906 - value_out_loss: 0.0586 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0236 - val_loss: 0.0952 - val_value_out__loss: 0.0820 - val_value_out_loss: 0.0496\n",
      "Epoch 261/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0269 - loss: 0.1081 - value_out__loss: 0.0912 - value_out_loss: 0.0590 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0920 - val_value_out__loss: 0.0790 - val_value_out_loss: 0.0468\n",
      "Epoch 262/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0264 - loss: 0.1074 - value_out__loss: 0.0906 - value_out_loss: 0.0585 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0234 - val_loss: 0.0930 - val_value_out__loss: 0.0805 - val_value_out_loss: 0.0477\n",
      "Epoch 263/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0265 - loss: 0.1087 - value_out__loss: 0.0921 - value_out_loss: 0.0599 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0926 - val_value_out__loss: 0.0797 - val_value_out_loss: 0.0475\n",
      "Epoch 264/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0264 - loss: 0.1074 - value_out__loss: 0.0909 - value_out_loss: 0.0585 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0234 - val_loss: 0.0931 - val_value_out__loss: 0.0808 - val_value_out_loss: 0.0478\n",
      "Epoch 265/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0274 - loss: 0.1106 - value_out__loss: 0.0933 - value_out_loss: 0.0609 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0231 - val_loss: 0.0945 - val_value_out__loss: 0.0818 - val_value_out_loss: 0.0494\n",
      "Epoch 266/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0263 - loss: 0.1076 - value_out__loss: 0.0910 - value_out_loss: 0.0588 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0233 - val_loss: 0.0950 - val_value_out__loss: 0.0820 - val_value_out_loss: 0.0498\n",
      "Epoch 267/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0264 - loss: 0.1063 - value_out__loss: 0.0893 - value_out_loss: 0.0575 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0939 - val_value_out__loss: 0.0808 - val_value_out_loss: 0.0490\n",
      "Epoch 268/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0266 - loss: 0.1074 - value_out__loss: 0.0902 - value_out_loss: 0.0586 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0233 - val_loss: 0.0933 - val_value_out__loss: 0.0810 - val_value_out_loss: 0.0485\n",
      "Epoch 269/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0264 - loss: 0.1081 - value_out__loss: 0.0923 - value_out_loss: 0.0596 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0234 - val_loss: 0.0936 - val_value_out__loss: 0.0806 - val_value_out_loss: 0.0486\n",
      "Epoch 270/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0267 - loss: 0.1075 - value_out__loss: 0.0903 - value_out_loss: 0.0585 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0902 - val_value_out__loss: 0.0771 - val_value_out_loss: 0.0451\n",
      "Epoch 271/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0267 - loss: 0.1077 - value_out__loss: 0.0905 - value_out_loss: 0.0589 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0231 - val_loss: 0.0931 - val_value_out__loss: 0.0801 - val_value_out_loss: 0.0484\n",
      "Epoch 272/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0263 - loss: 0.1075 - value_out__loss: 0.0909 - value_out_loss: 0.0589 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0932 - val_value_out__loss: 0.0805 - val_value_out_loss: 0.0487\n",
      "Epoch 273/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0265 - loss: 0.1078 - value_out__loss: 0.0915 - value_out_loss: 0.0591 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0230 - val_loss: 0.0904 - val_value_out__loss: 0.0776 - val_value_out_loss: 0.0452\n",
      "Epoch 274/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0266 - loss: 0.1092 - value_out__loss: 0.0921 - value_out_loss: 0.0601 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0999 - val_value_out__loss: 0.0869 - val_value_out_loss: 0.0547\n",
      "Epoch 275/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0266 - loss: 0.1101 - value_out__loss: 0.0938 - value_out_loss: 0.0612 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0234 - val_loss: 0.0992 - val_value_out__loss: 0.0874 - val_value_out_loss: 0.0549\n",
      "Epoch 276/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0269 - loss: 0.1106 - value_out__loss: 0.0941 - value_out_loss: 0.0614 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0234 - val_loss: 0.0954 - val_value_out__loss: 0.0823 - val_value_out_loss: 0.0506\n",
      "Epoch 277/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0263 - loss: 0.1076 - value_out__loss: 0.0912 - value_out_loss: 0.0589 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0236 - val_loss: 0.0959 - val_value_out__loss: 0.0836 - val_value_out_loss: 0.0509\n",
      "Epoch 278/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0270 - loss: 0.1100 - value_out__loss: 0.0935 - value_out_loss: 0.0612 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0958 - val_value_out__loss: 0.0833 - val_value_out_loss: 0.0510\n",
      "Epoch 279/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0265 - loss: 0.1069 - value_out__loss: 0.0910 - value_out_loss: 0.0583 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0902 - val_value_out__loss: 0.0782 - val_value_out_loss: 0.0452\n",
      "Epoch 280/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0269 - loss: 0.1092 - value_out__loss: 0.0928 - value_out_loss: 0.0603 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0926 - val_value_out__loss: 0.0807 - val_value_out_loss: 0.0476\n",
      "Epoch 281/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0264 - loss: 0.1084 - value_out__loss: 0.0920 - value_out_loss: 0.0593 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0923 - val_value_out__loss: 0.0787 - val_value_out_loss: 0.0467\n",
      "Epoch 282/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0266 - loss: 0.1077 - value_out__loss: 0.0905 - value_out_loss: 0.0583 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0233 - val_loss: 0.0921 - val_value_out__loss: 0.0795 - val_value_out_loss: 0.0471\n",
      "Epoch 283/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0265 - loss: 0.1077 - value_out__loss: 0.0919 - value_out_loss: 0.0593 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0916 - val_value_out__loss: 0.0795 - val_value_out_loss: 0.0469\n",
      "Epoch 284/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0262 - loss: 0.1070 - value_out__loss: 0.0908 - value_out_loss: 0.0587 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0955 - val_value_out__loss: 0.0829 - val_value_out_loss: 0.0501\n",
      "Epoch 285/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0265 - loss: 0.1082 - value_out__loss: 0.0909 - value_out_loss: 0.0595 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0923 - val_value_out__loss: 0.0784 - val_value_out_loss: 0.0474\n",
      "Epoch 286/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0265 - loss: 0.1081 - value_out__loss: 0.0907 - value_out_loss: 0.0593 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0937 - val_value_out__loss: 0.0801 - val_value_out_loss: 0.0488\n",
      "Epoch 287/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0265 - loss: 0.1069 - value_out__loss: 0.0899 - value_out_loss: 0.0582 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0233 - val_loss: 0.0912 - val_value_out__loss: 0.0784 - val_value_out_loss: 0.0462\n",
      "Epoch 288/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0263 - loss: 0.1067 - value_out__loss: 0.0904 - value_out_loss: 0.0585 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0920 - val_value_out__loss: 0.0788 - val_value_out_loss: 0.0477\n",
      "Epoch 289/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0266 - loss: 0.1060 - value_out__loss: 0.0892 - value_out_loss: 0.0579 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0927 - val_value_out__loss: 0.0802 - val_value_out_loss: 0.0487\n",
      "Epoch 290/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0263 - loss: 0.1067 - value_out__loss: 0.0901 - value_out_loss: 0.0588 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0929 - val_value_out__loss: 0.0806 - val_value_out_loss: 0.0488\n",
      "Epoch 291/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0267 - loss: 0.1085 - value_out__loss: 0.0918 - value_out_loss: 0.0602 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0233 - val_loss: 0.0923 - val_value_out__loss: 0.0792 - val_value_out_loss: 0.0475\n",
      "Epoch 292/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0263 - loss: 0.1075 - value_out__loss: 0.0905 - value_out_loss: 0.0590 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0233 - val_loss: 0.0959 - val_value_out__loss: 0.0823 - val_value_out_loss: 0.0513\n",
      "Epoch 293/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0266 - loss: 0.1087 - value_out__loss: 0.0914 - value_out_loss: 0.0600 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0245 - val_loss: 0.0993 - val_value_out__loss: 0.0836 - val_value_out_loss: 0.0538\n",
      "Epoch 294/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0270 - loss: 0.1107 - value_out__loss: 0.0926 - value_out_loss: 0.0618 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0231 - val_loss: 0.0955 - val_value_out__loss: 0.0816 - val_value_out_loss: 0.0502\n",
      "Epoch 295/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0263 - loss: 0.1090 - value_out__loss: 0.0912 - value_out_loss: 0.0601 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0233 - val_loss: 0.0931 - val_value_out__loss: 0.0791 - val_value_out_loss: 0.0484\n",
      "Epoch 296/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0264 - loss: 0.1065 - value_out__loss: 0.0889 - value_out_loss: 0.0581 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0233 - val_loss: 0.0911 - val_value_out__loss: 0.0771 - val_value_out_loss: 0.0465\n",
      "Epoch 297/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0263 - loss: 0.1055 - value_out__loss: 0.0881 - value_out_loss: 0.0576 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0911 - val_value_out__loss: 0.0777 - val_value_out_loss: 0.0471\n",
      "Epoch 298/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0266 - loss: 0.1061 - value_out__loss: 0.0894 - value_out_loss: 0.0585 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0242 - val_loss: 0.0915 - val_value_out__loss: 0.0781 - val_value_out_loss: 0.0474\n",
      "Epoch 299/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0273 - loss: 0.1115 - value_out__loss: 0.0936 - value_out_loss: 0.0627 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0971 - val_value_out__loss: 0.0824 - val_value_out_loss: 0.0520\n",
      "Epoch 300/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0265 - loss: 0.1093 - value_out__loss: 0.0906 - value_out_loss: 0.0593 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0946 - val_value_out__loss: 0.0802 - val_value_out_loss: 0.0487\n",
      "Epoch 301/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0266 - loss: 0.1079 - value_out__loss: 0.0898 - value_out_loss: 0.0586 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0233 - val_loss: 0.0920 - val_value_out__loss: 0.0785 - val_value_out_loss: 0.0472\n",
      "Epoch 302/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0265 - loss: 0.1065 - value_out__loss: 0.0895 - value_out_loss: 0.0583 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0231 - val_loss: 0.0912 - val_value_out__loss: 0.0785 - val_value_out_loss: 0.0474\n",
      "Epoch 303/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0269 - loss: 0.1082 - value_out__loss: 0.0913 - value_out_loss: 0.0601 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0231 - val_loss: 0.0879 - val_value_out__loss: 0.0744 - val_value_out_loss: 0.0437\n",
      "Epoch 304/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0264 - loss: 0.1055 - value_out__loss: 0.0882 - value_out_loss: 0.0577 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0231 - val_loss: 0.0897 - val_value_out__loss: 0.0759 - val_value_out_loss: 0.0458\n",
      "Epoch 305/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0263 - loss: 0.1061 - value_out__loss: 0.0892 - value_out_loss: 0.0584 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0231 - val_loss: 0.0918 - val_value_out__loss: 0.0792 - val_value_out_loss: 0.0479\n",
      "Epoch 306/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0265 - loss: 0.1064 - value_out__loss: 0.0895 - value_out_loss: 0.0587 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0939 - val_value_out__loss: 0.0811 - val_value_out_loss: 0.0500\n",
      "Epoch 307/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0266 - loss: 0.1088 - value_out__loss: 0.0914 - value_out_loss: 0.0607 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0233 - val_loss: 0.1013 - val_value_out__loss: 0.0880 - val_value_out_loss: 0.0571\n",
      "Epoch 308/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0266 - loss: 0.1101 - value_out__loss: 0.0928 - value_out_loss: 0.0617 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0231 - val_loss: 0.0932 - val_value_out__loss: 0.0802 - val_value_out_loss: 0.0485\n",
      "Epoch 309/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0268 - loss: 0.1065 - value_out__loss: 0.0892 - value_out_loss: 0.0582 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0234 - val_loss: 0.0904 - val_value_out__loss: 0.0771 - val_value_out_loss: 0.0463\n",
      "Epoch 310/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0263 - loss: 0.1055 - value_out__loss: 0.0889 - value_out_loss: 0.0580 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0233 - val_loss: 0.0899 - val_value_out__loss: 0.0773 - val_value_out_loss: 0.0463\n",
      "Epoch 311/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0263 - loss: 0.1072 - value_out__loss: 0.0911 - value_out_loss: 0.0599 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0231 - val_loss: 0.0907 - val_value_out__loss: 0.0779 - val_value_out_loss: 0.0471\n",
      "Epoch 312/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0263 - loss: 0.1067 - value_out__loss: 0.0895 - value_out_loss: 0.0590 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0230 - val_loss: 0.0934 - val_value_out__loss: 0.0801 - val_value_out_loss: 0.0490\n",
      "Epoch 313/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0266 - loss: 0.1068 - value_out__loss: 0.0894 - value_out_loss: 0.0584 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0939 - val_value_out__loss: 0.0808 - val_value_out_loss: 0.0497\n",
      "Epoch 314/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0264 - loss: 0.1063 - value_out__loss: 0.0895 - value_out_loss: 0.0583 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0235 - val_loss: 0.0932 - val_value_out__loss: 0.0804 - val_value_out_loss: 0.0495\n",
      "Epoch 315/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0264 - loss: 0.1064 - value_out__loss: 0.0902 - value_out_loss: 0.0591 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0234 - val_loss: 0.0915 - val_value_out__loss: 0.0794 - val_value_out_loss: 0.0482\n",
      "Epoch 316/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0266 - loss: 0.1065 - value_out__loss: 0.0907 - value_out_loss: 0.0591 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0231 - val_loss: 0.0896 - val_value_out__loss: 0.0768 - val_value_out_loss: 0.0457\n",
      "Epoch 317/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0264 - loss: 0.1054 - value_out__loss: 0.0885 - value_out_loss: 0.0574 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0230 - val_loss: 0.0921 - val_value_out__loss: 0.0787 - val_value_out_loss: 0.0478\n",
      "Epoch 318/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0264 - loss: 0.1064 - value_out__loss: 0.0897 - value_out_loss: 0.0586 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0230 - val_loss: 0.0896 - val_value_out__loss: 0.0773 - val_value_out_loss: 0.0458\n",
      "Epoch 319/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0265 - loss: 0.1060 - value_out__loss: 0.0899 - value_out_loss: 0.0583 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0920 - val_value_out__loss: 0.0795 - val_value_out_loss: 0.0480\n",
      "Epoch 320/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0266 - loss: 0.1058 - value_out__loss: 0.0890 - value_out_loss: 0.0577 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0903 - val_value_out__loss: 0.0771 - val_value_out_loss: 0.0456\n",
      "Epoch 321/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0265 - loss: 0.1071 - value_out__loss: 0.0902 - value_out_loss: 0.0586 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0230 - val_loss: 0.0952 - val_value_out__loss: 0.0816 - val_value_out_loss: 0.0504\n",
      "Epoch 322/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0262 - loss: 0.1055 - value_out__loss: 0.0888 - value_out_loss: 0.0571 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0230 - val_loss: 0.0938 - val_value_out__loss: 0.0816 - val_value_out_loss: 0.0498\n",
      "Epoch 323/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0268 - loss: 0.1092 - value_out__loss: 0.0932 - value_out_loss: 0.0609 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0968 - val_value_out__loss: 0.0855 - val_value_out_loss: 0.0532\n",
      "Epoch 324/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0263 - loss: 0.1068 - value_out__loss: 0.0912 - value_out_loss: 0.0595 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0230 - val_loss: 0.0927 - val_value_out__loss: 0.0804 - val_value_out_loss: 0.0488\n",
      "Epoch 325/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0263 - loss: 0.1072 - value_out__loss: 0.0905 - value_out_loss: 0.0593 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0230 - val_loss: 0.0893 - val_value_out__loss: 0.0766 - val_value_out_loss: 0.0455\n",
      "Epoch 326/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0263 - loss: 0.1046 - value_out__loss: 0.0887 - value_out_loss: 0.0572 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0919 - val_value_out__loss: 0.0793 - val_value_out_loss: 0.0484\n",
      "Epoch 327/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0264 - loss: 0.1062 - value_out__loss: 0.0899 - value_out_loss: 0.0588 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0233 - val_loss: 0.0916 - val_value_out__loss: 0.0788 - val_value_out_loss: 0.0480\n",
      "Epoch 328/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0265 - loss: 0.1068 - value_out__loss: 0.0903 - value_out_loss: 0.0593 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0921 - val_value_out__loss: 0.0791 - val_value_out_loss: 0.0479\n",
      "Epoch 329/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0269 - loss: 0.1092 - value_out__loss: 0.0919 - value_out_loss: 0.0605 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0231 - val_loss: 0.0927 - val_value_out__loss: 0.0785 - val_value_out_loss: 0.0477\n",
      "Epoch 330/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0264 - loss: 0.1068 - value_out__loss: 0.0888 - value_out_loss: 0.0580 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0230 - val_loss: 0.0931 - val_value_out__loss: 0.0786 - val_value_out_loss: 0.0484\n",
      "Epoch 331/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0264 - loss: 0.1071 - value_out__loss: 0.0893 - value_out_loss: 0.0587 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0921 - val_value_out__loss: 0.0786 - val_value_out_loss: 0.0486\n",
      "Epoch 332/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0267 - loss: 0.1080 - value_out__loss: 0.0906 - value_out_loss: 0.0599 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0230 - val_loss: 0.0935 - val_value_out__loss: 0.0800 - val_value_out_loss: 0.0495\n",
      "Epoch 333/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0264 - loss: 0.1075 - value_out__loss: 0.0903 - value_out_loss: 0.0593 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0229 - val_loss: 0.0898 - val_value_out__loss: 0.0760 - val_value_out_loss: 0.0457\n",
      "Epoch 334/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0265 - loss: 0.1066 - value_out__loss: 0.0890 - value_out_loss: 0.0585 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0230 - val_loss: 0.0922 - val_value_out__loss: 0.0787 - val_value_out_loss: 0.0483\n",
      "Epoch 335/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0263 - loss: 0.1058 - value_out__loss: 0.0891 - value_out_loss: 0.0582 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0233 - val_loss: 0.1003 - val_value_out__loss: 0.0865 - val_value_out_loss: 0.0566\n",
      "Epoch 336/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0263 - loss: 0.1074 - value_out__loss: 0.0907 - value_out_loss: 0.0598 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0230 - val_loss: 0.0940 - val_value_out__loss: 0.0815 - val_value_out_loss: 0.0503\n",
      "Epoch 337/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0266 - loss: 0.1083 - value_out__loss: 0.0921 - value_out_loss: 0.0611 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0941 - val_value_out__loss: 0.0824 - val_value_out_loss: 0.0506\n",
      "Epoch 338/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0263 - loss: 0.1076 - value_out__loss: 0.0913 - value_out_loss: 0.0603 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0231 - val_loss: 0.0948 - val_value_out__loss: 0.0811 - val_value_out_loss: 0.0511\n",
      "Epoch 339/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0260 - loss: 0.1062 - value_out__loss: 0.0896 - value_out_loss: 0.0584 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0230 - val_loss: 0.0911 - val_value_out__loss: 0.0774 - val_value_out_loss: 0.0472\n",
      "Epoch 340/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0262 - loss: 0.1039 - value_out__loss: 0.0875 - value_out_loss: 0.0567 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0233 - val_loss: 0.0909 - val_value_out__loss: 0.0785 - val_value_out_loss: 0.0478\n",
      "Epoch 341/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0267 - loss: 0.1059 - value_out__loss: 0.0899 - value_out_loss: 0.0589 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0229 - val_loss: 0.0891 - val_value_out__loss: 0.0764 - val_value_out_loss: 0.0455\n",
      "Epoch 342/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0263 - loss: 0.1055 - value_out__loss: 0.0888 - value_out_loss: 0.0579 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0230 - val_loss: 0.0900 - val_value_out__loss: 0.0760 - val_value_out_loss: 0.0456\n",
      "Epoch 343/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0261 - loss: 0.1051 - value_out__loss: 0.0880 - value_out_loss: 0.0573 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0231 - val_loss: 0.0912 - val_value_out__loss: 0.0780 - val_value_out_loss: 0.0478\n",
      "Epoch 344/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0263 - loss: 0.1058 - value_out__loss: 0.0896 - value_out_loss: 0.0588 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0230 - val_loss: 0.0910 - val_value_out__loss: 0.0786 - val_value_out_loss: 0.0480\n",
      "Epoch 345/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0261 - loss: 0.1049 - value_out__loss: 0.0893 - value_out_loss: 0.0579 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0232 - val_loss: 0.0921 - val_value_out__loss: 0.0791 - val_value_out_loss: 0.0488\n",
      "Epoch 346/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0262 - loss: 0.1045 - value_out__loss: 0.0882 - value_out_loss: 0.0572 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0229 - val_loss: 0.0876 - val_value_out__loss: 0.0743 - val_value_out_loss: 0.0438\n",
      "Epoch 347/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0264 - loss: 0.1061 - value_out__loss: 0.0891 - value_out_loss: 0.0585 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0231 - val_loss: 0.0885 - val_value_out__loss: 0.0747 - val_value_out_loss: 0.0449\n",
      "Epoch 348/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0268 - loss: 0.1054 - value_out__loss: 0.0883 - value_out_loss: 0.0578 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0230 - val_loss: 0.0887 - val_value_out__loss: 0.0758 - val_value_out_loss: 0.0455\n",
      "Epoch 349/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0261 - loss: 0.1044 - value_out__loss: 0.0886 - value_out_loss: 0.0577 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0229 - val_loss: 0.0899 - val_value_out__loss: 0.0767 - val_value_out_loss: 0.0469\n",
      "Epoch 350/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0260 - loss: 0.1042 - value_out__loss: 0.0888 - value_out_loss: 0.0575 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0229 - val_loss: 0.0893 - val_value_out__loss: 0.0766 - val_value_out_loss: 0.0463\n",
      "Epoch 351/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0262 - loss: 0.1037 - value_out__loss: 0.0874 - value_out_loss: 0.0567 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0229 - val_loss: 0.0900 - val_value_out__loss: 0.0768 - val_value_out_loss: 0.0468\n",
      "Epoch 352/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0261 - loss: 0.1046 - value_out__loss: 0.0883 - value_out_loss: 0.0579 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0229 - val_loss: 0.0914 - val_value_out__loss: 0.0790 - val_value_out_loss: 0.0487\n",
      "Epoch 353/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0266 - loss: 0.1061 - value_out__loss: 0.0900 - value_out_loss: 0.0594 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0230 - val_loss: 0.0901 - val_value_out__loss: 0.0775 - val_value_out_loss: 0.0472\n",
      "Epoch 354/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0261 - loss: 0.1052 - value_out__loss: 0.0889 - value_out_loss: 0.0586 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0230 - val_loss: 0.0897 - val_value_out__loss: 0.0764 - val_value_out_loss: 0.0463\n",
      "Epoch 355/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0262 - loss: 0.1046 - value_out__loss: 0.0887 - value_out_loss: 0.0580 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0229 - val_loss: 0.0891 - val_value_out__loss: 0.0765 - val_value_out_loss: 0.0462\n",
      "Epoch 356/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0261 - loss: 0.1038 - value_out__loss: 0.0879 - value_out_loss: 0.0573 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0230 - val_loss: 0.0899 - val_value_out__loss: 0.0769 - val_value_out_loss: 0.0474\n",
      "Epoch 357/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0263 - loss: 0.1051 - value_out__loss: 0.0887 - value_out_loss: 0.0586 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0230 - val_loss: 0.0924 - val_value_out__loss: 0.0786 - val_value_out_loss: 0.0493\n",
      "Epoch 358/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0264 - loss: 0.1060 - value_out__loss: 0.0890 - value_out_loss: 0.0588 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0230 - val_loss: 0.0907 - val_value_out__loss: 0.0764 - val_value_out_loss: 0.0468\n",
      "Epoch 359/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0264 - loss: 0.1050 - value_out__loss: 0.0879 - value_out_loss: 0.0575 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0229 - val_loss: 0.0889 - val_value_out__loss: 0.0749 - val_value_out_loss: 0.0454\n",
      "Epoch 360/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0266 - loss: 0.1060 - value_out__loss: 0.0884 - value_out_loss: 0.0582 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0236 - val_loss: 0.0960 - val_value_out__loss: 0.0819 - val_value_out_loss: 0.0519\n",
      "Epoch 361/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0268 - loss: 0.1087 - value_out__loss: 0.0914 - value_out_loss: 0.0608 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0229 - val_loss: 0.0888 - val_value_out__loss: 0.0751 - val_value_out_loss: 0.0445\n",
      "Epoch 362/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0262 - loss: 0.1068 - value_out__loss: 0.0897 - value_out_loss: 0.0585 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0234 - val_loss: 0.0919 - val_value_out__loss: 0.0775 - val_value_out_loss: 0.0475\n",
      "Epoch 363/400\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - exists_out_accuracy: 0.0000e+00 - exists_out_loss: 0.0263 - loss: 0.1070 - value_out__loss: 0.0902 - value_out_loss: 0.0591 - val_exists_out_accuracy: 0.0000e+00 - val_exists_out_loss: 0.0227 - val_loss: 0.0899 - val_value_out__loss: 0.0770 - val_value_out_loss: 0.0463\n",
      "Epoch 363: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if 'Try Both' not in ENCODING_TYPE:\n",
    "    if KERAS_TUNER:\n",
    "        # now both searches should have completed, so lets get the best hyperparams \n",
    "        # and retrain with history saved so we can look at it\n",
    "        best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "        \n",
    "        # make the models again using the best hyperparams\n",
    "        model = tuner.hypermodel.build(best_hp)\n",
    "\n",
    "        lr_monitor = LearningRateMonitor()  # make learning rate monitor\n",
    "        \n",
    "        value_sw_train = np.asarray(y_exists_train).astype(\"float32\")  # keep (N,16)\n",
    "        value_sw_train_val   = np.asarray(y_exists_val).astype(\"float32\")    # keep (N,16)\n",
    "        value_sw_val = np.asarray(y_exists_val).astype(\"float32\")\n",
    "        \n",
    "        history = model.fit(\n",
    "            np.asarray(X_train),\n",
    "            {\n",
    "                \"value_out\": np.asarray(y_value_train),\n",
    "                \"exists_out\": np.asarray(y_exists_train),\n",
    "            },\n",
    "            sample_weight={\n",
    "                \"value_out\": np.asarray(y_exists_train).astype(\"float32\"),           # (N,16) if youre masking per-dim\n",
    "                \"exists_out\": np.ones((len(y_exists_train),), dtype=\"float32\"),      # (N,)\n",
    "            },\n",
    "            validation_data=(\n",
    "                np.asarray(X_val),\n",
    "                {\n",
    "                    \"value_out\": np.asarray(y_value_val),\n",
    "                    \"exists_out\": np.asarray(y_exists_val),\n",
    "                },\n",
    "                {\n",
    "                    \"value_out\": np.asarray(y_exists_val).astype(\"float32\"),\n",
    "                    \"exists_out\": np.ones((len(y_exists_val),), dtype=\"float32\"),\n",
    "                },\n",
    "            ),\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=TRAIN_BATCH_SIZE,\n",
    "            callbacks=[early_stopping, lr_monitor],\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "\n",
    "        \n",
    "        # keep getting a memory allocation error on EAF so lets free everything after the first \n",
    "        # model fit, before moving to the next one\n",
    "        \n",
    "        del model\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "    \n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/{encoding}_history.pdf')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(lr_monitor.learning_rates)\n",
    "    plt.title(\"Learning Rate over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Learning Rate\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/{encoding}_learning_rate.pdf')\n",
    "    plt.show()\n",
    "else:\n",
    "    if KERAS_TUNER:\n",
    "        # now both searches should have completed, so lets get the best hyperparams \n",
    "        # and retrain with history saved so we can look at it\n",
    "        best_hp_linear = tuner_linear_encoding.get_best_hyperparameters(1)[0]\n",
    "        \n",
    "        # make the models again using the best hyperparams\n",
    "        model_linear = tuner_linear_encoding.hypermodel.build(best_hp_linear)\n",
    "\n",
    "        lr_monitor_linear_encoding = LearningRateMonitor()  # make learning rate monitor\n",
    "        \n",
    "        # retrain with history so we can plot it\n",
    "        history_linear_encoding = model_linear.fit(\n",
    "            np.asarray(X_train_linear_encoding),\n",
    "            {'value_out': np.asarray(y_value_train_linear_encoding), 'exists_out': np.asarray(y_exists_train_linear_encoding)},\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=TRAIN_BATCH_SIZE,\n",
    "            sample_weight={\n",
    "                \"value_out\": np.asarray(y_exists_train_linear_encoding).astype(\"float32\"),\n",
    "                \"exists_out\": np.ones((len(y_exists_train_linear_encoding),), dtype=\"float32\"),\n",
    "            },\n",
    "            validation_data=(\n",
    "                np.asarray(X_val_linear_encoding),\n",
    "                {'value_out': np.asarray(y_value_val_linear_encoding), 'exists_out': np.asarray(y_exists_val_linear_encoding)},\n",
    "                {\n",
    "                    \"value_out\": np.asarray(y_exists_val_linear_encoding).astype(\"float32\"),\n",
    "                    \"exists_out\": np.ones((len(y_exists_val_linear_encoding),), dtype=\"float32\"),\n",
    "                },\n",
    "            ),\n",
    "\n",
    "            callbacks=[early_stopping, lr_monitor_linear_encoding],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # keep getting a memory allocation error on EAF so lets free everything after the first \n",
    "        # model fit, before moving to the next one\n",
    "        \n",
    "        del model_linear\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "    plt.plot(history_linear_encoding.history['loss'])\n",
    "    plt.plot(history_linear_encoding.history['val_loss'])\n",
    "    plt.title('Model loss linear encoding')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/linear_encoding_history.pdf')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(lr_monitor_linear_encoding.learning_rates)\n",
    "    plt.title(\"Learning Rate over Epochs linear encoding\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Learning Rate\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/linear_encoding_learning_rate.pdf')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "47150b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I needed to restart the kernal and skip the block above to run this block-otherwise i got memory allocation error. \n",
    "#theres probably a better way to do this--like clearing the memory before running this block so you can run both models sequentially\n",
    "\n",
    "if 'Try Both' in ENCODING_TYPE and KERAS_TUNER:\n",
    "    best_hp_onehot = tuner_one_hot_encoding.get_best_hyperparameters(1)[0]\n",
    "    model_onehot = tuner_one_hot_encoding.hypermodel.build(best_hp_onehot)\n",
    "\n",
    "    lr_monitor_one_hot_encoding = LearningRateMonitor()  # make learning rate monitor\n",
    "    \n",
    "    history_one_hot_encoding = model_onehot.fit(\n",
    "        np.asarray(X_train_one_hot_encoding),\n",
    "        {'value_out': np.asarray(y_value_train_one_hot_encoding),\n",
    "         'exists_out': np.asarray(y_exists_train_one_hot_encoding)},\n",
    "        sample_weight={\n",
    "            \"value_out\": np.asarray(y_exists_train_one_hot_encoding).astype(\"float32\"),\n",
    "            \"exists_out\": np.ones((len(y_exists_train_one_hot_encoding),), dtype=\"float32\"),\n",
    "        },\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        validation_data=(\n",
    "            np.asarray(X_val_one_hot_encoding),\n",
    "            {'value_out': np.asarray(y_value_val_one_hot_encoding),\n",
    "             'exists_out': np.asarray(y_exists_val_one_hot_encoding)},\n",
    "            {\n",
    "                \"value_out\": np.asarray(y_exists_val_one_hot_encoding).astype(\"float32\"),\n",
    "                \"exists_out\": np.ones((len(y_exists_val_one_hot_encoding),), dtype=\"float32\"),\n",
    "            },\n",
    "        ),\n",
    "        callbacks=[early_stopping, lr_monitor_one_hot_encoding],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "\n",
    "    plt.plot(history_one_hot_encoding.history['loss'])\n",
    "    plt.plot(history_one_hot_encoding.history['val_loss'])\n",
    "    plt.title('Model loss one hot encoding')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/one_hot_encoding_history.pdf')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(lr_monitor_one_hot_encoding.learning_rates)\n",
    "    plt.title(\"Learning Rate over Epochs one hot encoding\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Learning Rate\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/one_hot_encoding_learning_rate.pdf')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a331d8-70ea-4ea4-8f5b-256472850784",
   "metadata": {},
   "source": [
    "Measure and print metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "32ff7d16-5709-41d6-b3f7-e6c9a062b35c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss one_hot encoding mae: 0.09331876039505005\n",
      "   Encoding Type Train Loss Metric  Test Loss\n",
      "one hot Encoding               mae   0.093319\n"
     ]
    }
   ],
   "source": [
    "#take a look at the loss results and save them \n",
    "\n",
    "# clear everything so we start with a blank slate memory wise\n",
    "tf.keras.backend.clear_session()                         #clear tf backend\n",
    "gc.collect()                                             #just collect stray things\n",
    "#tf.config.experimental.reset_memory_stats('GPU:0')       #clear gpus\n",
    "\n",
    "\n",
    "def get_loss(eval_out):\n",
    "    # eval_out can be float, list/tuple/ndarray, or dict\n",
    "    if isinstance(eval_out, dict):\n",
    "        return float(eval_out.get('loss', list(eval_out.values())[0]))\n",
    "    if isinstance(eval_out, (list, tuple, np.ndarray)):\n",
    "        return float(eval_out[0])\n",
    "    return float(eval_out)\n",
    "\n",
    "def mlp_signature(m, prefix=\"mlp\"):\n",
    "    #get the model input sizze first\n",
    "    in_shape = m.input_shape\n",
    "    if isinstance(in_shape, list):     #just get the first one\n",
    "        in_shape = in_shape[0]\n",
    "    dims = [d for d in in_shape[1:] if d is not None]\n",
    "    input_size = int(np.prod(dims)) if dims else \"None\"\n",
    "\n",
    "    # now get the dense laysers in order\n",
    "    dense_units = [l.units for l in m.layers if isinstance(l, Dense)]\n",
    "\n",
    "    parts = [prefix, str(input_size)] + [str(u) for u in dense_units]\n",
    "    return \"_\".join(parts)\n",
    "\n",
    "    \n",
    "if 'Try Both' not in ENCODING_TYPE:\n",
    "    model = load_model(best_model_file, compile=False)\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss={'value_out': VALUE_LOSS_FN, 'exists_out': 'binary_crossentropy'},\n",
    "        loss_weights={'value_out': 1.0, 'exists_out': 1.0},\n",
    "        metrics={'value_out': [VALUE_LOSS_FN], 'exists_out': ['accuracy']}\n",
    "    )\n",
    "\n",
    "\n",
    "    test_loss_result = model.evaluate(\n",
    "        np.asarray(X_test),\n",
    "        {'value_out': np.asarray(y_value_test), 'exists_out': np.asarray(y_exists_test)},\n",
    "        sample_weight={\n",
    "            \"value_out\": np.asarray(y_exists_test).astype(\"float32\"),  # (N,16)\n",
    "            \"exists_out\": np.ones((len(y_exists_test),), dtype=\"float32\"),\n",
    "        },\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    test_loss_result = get_loss(test_loss_result)\n",
    "    \n",
    "    #save the shape for the next block so we can save it using the definition above\n",
    "    model_shape = mlp_signature(model)\n",
    "\n",
    "    print('Current loss {} encoding {}: {}'.format(encoding, TRAIN_LOSS, test_loss_result))\n",
    "    \n",
    "    results_df = pd.DataFrame([\n",
    "        {'Encoding Type': f'{ENCODING_TYPE} Encoding', 'Train Loss Metric': TRAIN_LOSS, 'Test Loss': test_loss_result},\n",
    "        ])\n",
    "\n",
    "else:\n",
    "    # we need to do some fancy allocation of recources if we are going to load both models\n",
    "    # do the first on one GPU\n",
    "    linear_model = load_model(best_model_file_linear, compile=False)\n",
    "    linear_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss={'value_out': VALUE_LOSS_FN, 'exists_out': 'binary_crossentropy'},\n",
    "        loss_weights={'value_out': 1.0, 'exists_out': 1.0}\n",
    "    )\n",
    "    test_loss_result_linear_encoding = linear_model.evaluate(\n",
    "        np.asarray(X_test_linear_encoding),\n",
    "        {'value_out': np.asarray(y_value_test_linear_encoding), 'exists_out': np.asarray(y_exists_test_linear_encoding)},\n",
    "        sample_weight={\n",
    "            \"value_out\": np.asarray(y_exists_test_linear_encoding).astype(\"float32\"),\n",
    "            \"exists_out\": np.ones((len(y_exists_test_linear_encoding),), dtype=\"float32\"),\n",
    "        },\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    test_loss_result_linear_encoding = get_loss(test_loss_result_linear_encoding)\n",
    "    \n",
    "    # now do on CPU to avoid GPU allocation\n",
    "    with tf.device('/GPU:0'):\n",
    "        onehot_model = load_model(best_model_file_onehot, compile=False)\n",
    "        onehot_model.compile(\n",
    "            optimizer='adam',\n",
    "            loss={'value_out': VALUE_LOSS_FN, 'exists_out': 'binary_crossentropy'},\n",
    "            loss_weights={'value_out': 1.0, 'exists_out': 1.0}\n",
    "        )\n",
    "        test_loss_result_one_hot_encoding = onehot_model.evaluate(\n",
    "            np.asarray(X_test_one_hot_encoding),\n",
    "            {'value_out': np.asarray(y_value_test_one_hot_encoding), 'exists_out': np.asarray(y_exists_test_one_hot_encoding)},\n",
    "            sample_weight={\n",
    "            \"value_out\": np.asarray(y_exists_test_one_hot_encoding).astype(\"float32\"),\n",
    "            \"exists_out\": np.ones((len(y_exists_test_one_hot_encoding),), dtype=\"float32\"),\n",
    "        },\n",
    "            verbose=0\n",
    "        )\n",
    "        test_loss_result_one_hot_encoding = get_loss(test_loss_result_one_hot_encoding)\n",
    "\n",
    "    #save the shape for the next block so we can save it using the definition above\n",
    "    model_shape_one_hot_encoding = mlp_signature(onehot_model)\n",
    "    model_shape_linear_encoding  = mlp_signature(linear_model)\n",
    "    \n",
    "    print('Current loss linear encoding {}: {}'.format(TRAIN_LOSS, test_loss_result_linear_encoding))\n",
    "    print('Current loss one hot encoding {}: {}'.format(TRAIN_LOSS, test_loss_result_one_hot_encoding))\n",
    "    \n",
    "    results_df = pd.DataFrame([\n",
    "        {'Encoding Type': 'linear Encoding', 'Train Loss Metric': TRAIN_LOSS, 'Test Loss': test_loss_result_linear_encoding},\n",
    "        {'Encoding Type': 'One Hot Encoding', 'Train Loss Metric': TRAIN_LOSS, 'Test Loss': test_loss_result_one_hot_encoding}\n",
    "    ])\n",
    "\n",
    "print(results_df.to_string(index=False))\n",
    "results_df.to_csv('test_loss_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7812af-2596-44d2-bb25-68741e0506ba",
   "metadata": {},
   "source": [
    "## Compare predictions vs. test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "60157266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_c53b7_row0_col3, #T_c53b7_row1_col3, #T_c53b7_row2_col3, #T_c53b7_row3_col3, #T_c53b7_row4_col3, #T_c53b7_row5_col3, #T_c53b7_row6_col3, #T_c53b7_row7_col3, #T_c53b7_row8_col3, #T_c53b7_row9_col3, #T_c53b7_row10_col3, #T_c53b7_row11_col3, #T_c53b7_row12_col3, #T_c53b7_row13_col3, #T_c53b7_row14_col3 {\n",
       "  color: red;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_c53b7\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_c53b7_level0_col0\" class=\"col_heading level0 col0\" >data_augmentation</th>\n",
       "      <th id=\"T_c53b7_level0_col1\" class=\"col_heading level0 col1\" >model_shape</th>\n",
       "      <th id=\"T_c53b7_level0_col2\" class=\"col_heading level0 col2\" >encoding_type</th>\n",
       "      <th id=\"T_c53b7_level0_col3\" class=\"col_heading level0 col3\" >test_loss</th>\n",
       "      <th id=\"T_c53b7_level0_col4\" class=\"col_heading level0 col4\" >train_loss</th>\n",
       "      <th id=\"T_c53b7_level0_col5\" class=\"col_heading level0 col5\" >train_dropout_rate</th>\n",
       "      <th id=\"T_c53b7_level0_col6\" class=\"col_heading level0 col6\" >train_early_stop_patience</th>\n",
       "      <th id=\"T_c53b7_level0_col7\" class=\"col_heading level0 col7\" >train_batch_size</th>\n",
       "      <th id=\"T_c53b7_level0_col8\" class=\"col_heading level0 col8\" >train_val_split</th>\n",
       "      <th id=\"T_c53b7_level0_col9\" class=\"col_heading level0 col9\" >lr_initial</th>\n",
       "      <th id=\"T_c53b7_level0_col10\" class=\"col_heading level0 col10\" >lr_decay_step</th>\n",
       "      <th id=\"T_c53b7_level0_col11\" class=\"col_heading level0 col11\" >lr_decay_rate</th>\n",
       "      <th id=\"T_c53b7_level0_col12\" class=\"col_heading level0 col12\" >lr_stair_case</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c53b7_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_c53b7_row0_col0\" class=\"data row0 col0\" >True</td>\n",
       "      <td id=\"T_c53b7_row0_col1\" class=\"data row0 col1\" >mlp_2_1500_2800_1300_2100_45</td>\n",
       "      <td id=\"T_c53b7_row0_col2\" class=\"data row0 col2\" >One Hot</td>\n",
       "      <td id=\"T_c53b7_row0_col3\" class=\"data row0 col3\" >0.026580</td>\n",
       "      <td id=\"T_c53b7_row0_col4\" class=\"data row0 col4\" >mean_squared_error</td>\n",
       "      <td id=\"T_c53b7_row0_col5\" class=\"data row0 col5\" >0.400000</td>\n",
       "      <td id=\"T_c53b7_row0_col6\" class=\"data row0 col6\" >50</td>\n",
       "      <td id=\"T_c53b7_row0_col7\" class=\"data row0 col7\" >32</td>\n",
       "      <td id=\"T_c53b7_row0_col8\" class=\"data row0 col8\" >0.15/0.15</td>\n",
       "      <td id=\"T_c53b7_row0_col9\" class=\"data row0 col9\" >0.001456</td>\n",
       "      <td id=\"T_c53b7_row0_col10\" class=\"data row0 col10\" >100</td>\n",
       "      <td id=\"T_c53b7_row0_col11\" class=\"data row0 col11\" >0.990000</td>\n",
       "      <td id=\"T_c53b7_row0_col12\" class=\"data row0 col12\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c53b7_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_c53b7_row1_col0\" class=\"data row1 col0\" >True</td>\n",
       "      <td id=\"T_c53b7_row1_col1\" class=\"data row1 col1\" >mlp_2_600_100_2600_5000_45</td>\n",
       "      <td id=\"T_c53b7_row1_col2\" class=\"data row1 col2\" >Linear</td>\n",
       "      <td id=\"T_c53b7_row1_col3\" class=\"data row1 col3\" >0.027197</td>\n",
       "      <td id=\"T_c53b7_row1_col4\" class=\"data row1 col4\" >mean_squared_error</td>\n",
       "      <td id=\"T_c53b7_row1_col5\" class=\"data row1 col5\" >0.400000</td>\n",
       "      <td id=\"T_c53b7_row1_col6\" class=\"data row1 col6\" >50</td>\n",
       "      <td id=\"T_c53b7_row1_col7\" class=\"data row1 col7\" >32</td>\n",
       "      <td id=\"T_c53b7_row1_col8\" class=\"data row1 col8\" >0.15/0.15</td>\n",
       "      <td id=\"T_c53b7_row1_col9\" class=\"data row1 col9\" >0.001456</td>\n",
       "      <td id=\"T_c53b7_row1_col10\" class=\"data row1 col10\" >100</td>\n",
       "      <td id=\"T_c53b7_row1_col11\" class=\"data row1 col11\" >0.990000</td>\n",
       "      <td id=\"T_c53b7_row1_col12\" class=\"data row1 col12\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c53b7_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_c53b7_row2_col0\" class=\"data row2 col0\" >True</td>\n",
       "      <td id=\"T_c53b7_row2_col1\" class=\"data row2 col1\" >mlp_2_1500_2800_1300_2100_45</td>\n",
       "      <td id=\"T_c53b7_row2_col2\" class=\"data row2 col2\" >one hot</td>\n",
       "      <td id=\"T_c53b7_row2_col3\" class=\"data row2 col3\" >0.026581</td>\n",
       "      <td id=\"T_c53b7_row2_col4\" class=\"data row2 col4\" >mean_squared_error</td>\n",
       "      <td id=\"T_c53b7_row2_col5\" class=\"data row2 col5\" >0.400000</td>\n",
       "      <td id=\"T_c53b7_row2_col6\" class=\"data row2 col6\" >50</td>\n",
       "      <td id=\"T_c53b7_row2_col7\" class=\"data row2 col7\" >32</td>\n",
       "      <td id=\"T_c53b7_row2_col8\" class=\"data row2 col8\" >0.15/0.15</td>\n",
       "      <td id=\"T_c53b7_row2_col9\" class=\"data row2 col9\" >0.001456</td>\n",
       "      <td id=\"T_c53b7_row2_col10\" class=\"data row2 col10\" >100</td>\n",
       "      <td id=\"T_c53b7_row2_col11\" class=\"data row2 col11\" >0.990000</td>\n",
       "      <td id=\"T_c53b7_row2_col12\" class=\"data row2 col12\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c53b7_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_c53b7_row3_col0\" class=\"data row3 col0\" >True</td>\n",
       "      <td id=\"T_c53b7_row3_col1\" class=\"data row3 col1\" >mlp_2_3300_200_4600_1700_45</td>\n",
       "      <td id=\"T_c53b7_row3_col2\" class=\"data row3 col2\" >one hot</td>\n",
       "      <td id=\"T_c53b7_row3_col3\" class=\"data row3 col3\" >0.023184</td>\n",
       "      <td id=\"T_c53b7_row3_col4\" class=\"data row3 col4\" >mean_squared_error</td>\n",
       "      <td id=\"T_c53b7_row3_col5\" class=\"data row3 col5\" >0.400000</td>\n",
       "      <td id=\"T_c53b7_row3_col6\" class=\"data row3 col6\" >50</td>\n",
       "      <td id=\"T_c53b7_row3_col7\" class=\"data row3 col7\" >32</td>\n",
       "      <td id=\"T_c53b7_row3_col8\" class=\"data row3 col8\" >0.15/0.15</td>\n",
       "      <td id=\"T_c53b7_row3_col9\" class=\"data row3 col9\" >0.001456</td>\n",
       "      <td id=\"T_c53b7_row3_col10\" class=\"data row3 col10\" >100</td>\n",
       "      <td id=\"T_c53b7_row3_col11\" class=\"data row3 col11\" >0.990000</td>\n",
       "      <td id=\"T_c53b7_row3_col12\" class=\"data row3 col12\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c53b7_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_c53b7_row4_col0\" class=\"data row4 col0\" >True</td>\n",
       "      <td id=\"T_c53b7_row4_col1\" class=\"data row4 col1\" >mlp_2_3300_200_4600_1700_45</td>\n",
       "      <td id=\"T_c53b7_row4_col2\" class=\"data row4 col2\" >one hot</td>\n",
       "      <td id=\"T_c53b7_row4_col3\" class=\"data row4 col3\" >0.023925</td>\n",
       "      <td id=\"T_c53b7_row4_col4\" class=\"data row4 col4\" >mean_squared_error</td>\n",
       "      <td id=\"T_c53b7_row4_col5\" class=\"data row4 col5\" >0.400000</td>\n",
       "      <td id=\"T_c53b7_row4_col6\" class=\"data row4 col6\" >50</td>\n",
       "      <td id=\"T_c53b7_row4_col7\" class=\"data row4 col7\" >32</td>\n",
       "      <td id=\"T_c53b7_row4_col8\" class=\"data row4 col8\" >0.15/0.15</td>\n",
       "      <td id=\"T_c53b7_row4_col9\" class=\"data row4 col9\" >0.001456</td>\n",
       "      <td id=\"T_c53b7_row4_col10\" class=\"data row4 col10\" >100</td>\n",
       "      <td id=\"T_c53b7_row4_col11\" class=\"data row4 col11\" >0.990000</td>\n",
       "      <td id=\"T_c53b7_row4_col12\" class=\"data row4 col12\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c53b7_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_c53b7_row5_col0\" class=\"data row5 col0\" >True</td>\n",
       "      <td id=\"T_c53b7_row5_col1\" class=\"data row5 col1\" >mlp_2_700_3800_400_1800_16</td>\n",
       "      <td id=\"T_c53b7_row5_col2\" class=\"data row5 col2\" >one hot</td>\n",
       "      <td id=\"T_c53b7_row5_col3\" class=\"data row5 col3\" >0.045630</td>\n",
       "      <td id=\"T_c53b7_row5_col4\" class=\"data row5 col4\" >mean_squared_error</td>\n",
       "      <td id=\"T_c53b7_row5_col5\" class=\"data row5 col5\" >0.400000</td>\n",
       "      <td id=\"T_c53b7_row5_col6\" class=\"data row5 col6\" >50</td>\n",
       "      <td id=\"T_c53b7_row5_col7\" class=\"data row5 col7\" >32</td>\n",
       "      <td id=\"T_c53b7_row5_col8\" class=\"data row5 col8\" >0.15/0.15</td>\n",
       "      <td id=\"T_c53b7_row5_col9\" class=\"data row5 col9\" >0.001456</td>\n",
       "      <td id=\"T_c53b7_row5_col10\" class=\"data row5 col10\" >100</td>\n",
       "      <td id=\"T_c53b7_row5_col11\" class=\"data row5 col11\" >0.990000</td>\n",
       "      <td id=\"T_c53b7_row5_col12\" class=\"data row5 col12\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c53b7_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_c53b7_row6_col0\" class=\"data row6 col0\" >True</td>\n",
       "      <td id=\"T_c53b7_row6_col1\" class=\"data row6 col1\" >mlp_2_700_3800_400_1800_16</td>\n",
       "      <td id=\"T_c53b7_row6_col2\" class=\"data row6 col2\" >one hot</td>\n",
       "      <td id=\"T_c53b7_row6_col3\" class=\"data row6 col3\" >0.041409</td>\n",
       "      <td id=\"T_c53b7_row6_col4\" class=\"data row6 col4\" >mean_squared_error</td>\n",
       "      <td id=\"T_c53b7_row6_col5\" class=\"data row6 col5\" >0.400000</td>\n",
       "      <td id=\"T_c53b7_row6_col6\" class=\"data row6 col6\" >50</td>\n",
       "      <td id=\"T_c53b7_row6_col7\" class=\"data row6 col7\" >32</td>\n",
       "      <td id=\"T_c53b7_row6_col8\" class=\"data row6 col8\" >0.15/0.15</td>\n",
       "      <td id=\"T_c53b7_row6_col9\" class=\"data row6 col9\" >0.002648</td>\n",
       "      <td id=\"T_c53b7_row6_col10\" class=\"data row6 col10\" >100</td>\n",
       "      <td id=\"T_c53b7_row6_col11\" class=\"data row6 col11\" >0.990000</td>\n",
       "      <td id=\"T_c53b7_row6_col12\" class=\"data row6 col12\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c53b7_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_c53b7_row7_col0\" class=\"data row7 col0\" >True</td>\n",
       "      <td id=\"T_c53b7_row7_col1\" class=\"data row7 col1\" >mlp_2_2000_400_1300_1700_16_16</td>\n",
       "      <td id=\"T_c53b7_row7_col2\" class=\"data row7 col2\" >one hot</td>\n",
       "      <td id=\"T_c53b7_row7_col3\" class=\"data row7 col3\" >0.087074</td>\n",
       "      <td id=\"T_c53b7_row7_col4\" class=\"data row7 col4\" >mean_squared_error</td>\n",
       "      <td id=\"T_c53b7_row7_col5\" class=\"data row7 col5\" >0.400000</td>\n",
       "      <td id=\"T_c53b7_row7_col6\" class=\"data row7 col6\" >50</td>\n",
       "      <td id=\"T_c53b7_row7_col7\" class=\"data row7 col7\" >32</td>\n",
       "      <td id=\"T_c53b7_row7_col8\" class=\"data row7 col8\" >0.15/0.15</td>\n",
       "      <td id=\"T_c53b7_row7_col9\" class=\"data row7 col9\" >0.002648</td>\n",
       "      <td id=\"T_c53b7_row7_col10\" class=\"data row7 col10\" >100</td>\n",
       "      <td id=\"T_c53b7_row7_col11\" class=\"data row7 col11\" >0.990000</td>\n",
       "      <td id=\"T_c53b7_row7_col12\" class=\"data row7 col12\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c53b7_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_c53b7_row8_col0\" class=\"data row8 col0\" >True</td>\n",
       "      <td id=\"T_c53b7_row8_col1\" class=\"data row8 col1\" >mlp_2_2000_400_1300_1700_16_16</td>\n",
       "      <td id=\"T_c53b7_row8_col2\" class=\"data row8 col2\" >one hot</td>\n",
       "      <td id=\"T_c53b7_row8_col3\" class=\"data row8 col3\" >0.080590</td>\n",
       "      <td id=\"T_c53b7_row8_col4\" class=\"data row8 col4\" >mean_squared_error</td>\n",
       "      <td id=\"T_c53b7_row8_col5\" class=\"data row8 col5\" >0.400000</td>\n",
       "      <td id=\"T_c53b7_row8_col6\" class=\"data row8 col6\" >50</td>\n",
       "      <td id=\"T_c53b7_row8_col7\" class=\"data row8 col7\" >32</td>\n",
       "      <td id=\"T_c53b7_row8_col8\" class=\"data row8 col8\" >0.15/0.15</td>\n",
       "      <td id=\"T_c53b7_row8_col9\" class=\"data row8 col9\" >0.002505</td>\n",
       "      <td id=\"T_c53b7_row8_col10\" class=\"data row8 col10\" >100</td>\n",
       "      <td id=\"T_c53b7_row8_col11\" class=\"data row8 col11\" >0.990000</td>\n",
       "      <td id=\"T_c53b7_row8_col12\" class=\"data row8 col12\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c53b7_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_c53b7_row9_col0\" class=\"data row9 col0\" >True</td>\n",
       "      <td id=\"T_c53b7_row9_col1\" class=\"data row9 col1\" >mlp_2_2000_2700_500_4000_16_16</td>\n",
       "      <td id=\"T_c53b7_row9_col2\" class=\"data row9 col2\" >one hot</td>\n",
       "      <td id=\"T_c53b7_row9_col3\" class=\"data row9 col3\" >0.086770</td>\n",
       "      <td id=\"T_c53b7_row9_col4\" class=\"data row9 col4\" >mean_squared_error</td>\n",
       "      <td id=\"T_c53b7_row9_col5\" class=\"data row9 col5\" >0.400000</td>\n",
       "      <td id=\"T_c53b7_row9_col6\" class=\"data row9 col6\" >50</td>\n",
       "      <td id=\"T_c53b7_row9_col7\" class=\"data row9 col7\" >32</td>\n",
       "      <td id=\"T_c53b7_row9_col8\" class=\"data row9 col8\" >0.15/0.15</td>\n",
       "      <td id=\"T_c53b7_row9_col9\" class=\"data row9 col9\" >0.002505</td>\n",
       "      <td id=\"T_c53b7_row9_col10\" class=\"data row9 col10\" >100</td>\n",
       "      <td id=\"T_c53b7_row9_col11\" class=\"data row9 col11\" >0.990000</td>\n",
       "      <td id=\"T_c53b7_row9_col12\" class=\"data row9 col12\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c53b7_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_c53b7_row10_col0\" class=\"data row10 col0\" >True</td>\n",
       "      <td id=\"T_c53b7_row10_col1\" class=\"data row10 col1\" >mlp_2_640_550_850_550_1410_16_16</td>\n",
       "      <td id=\"T_c53b7_row10_col2\" class=\"data row10 col2\" >one hot</td>\n",
       "      <td id=\"T_c53b7_row10_col3\" class=\"data row10 col3\" >0.129929</td>\n",
       "      <td id=\"T_c53b7_row10_col4\" class=\"data row10 col4\" >mae</td>\n",
       "      <td id=\"T_c53b7_row10_col5\" class=\"data row10 col5\" >0.000000</td>\n",
       "      <td id=\"T_c53b7_row10_col6\" class=\"data row10 col6\" >60</td>\n",
       "      <td id=\"T_c53b7_row10_col7\" class=\"data row10 col7\" >128</td>\n",
       "      <td id=\"T_c53b7_row10_col8\" class=\"data row10 col8\" >0.15/0.15</td>\n",
       "      <td id=\"T_c53b7_row10_col9\" class=\"data row10 col9\" >0.000982</td>\n",
       "      <td id=\"T_c53b7_row10_col10\" class=\"data row10 col10\" >140</td>\n",
       "      <td id=\"T_c53b7_row10_col11\" class=\"data row10 col11\" >0.990000</td>\n",
       "      <td id=\"T_c53b7_row10_col12\" class=\"data row10 col12\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c53b7_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_c53b7_row11_col0\" class=\"data row11 col0\" >True</td>\n",
       "      <td id=\"T_c53b7_row11_col1\" class=\"data row11 col1\" >mlp_2_500_1400_820_430_800_16_16</td>\n",
       "      <td id=\"T_c53b7_row11_col2\" class=\"data row11 col2\" >one hot</td>\n",
       "      <td id=\"T_c53b7_row11_col3\" class=\"data row11 col3\" >0.113140</td>\n",
       "      <td id=\"T_c53b7_row11_col4\" class=\"data row11 col4\" >mae</td>\n",
       "      <td id=\"T_c53b7_row11_col5\" class=\"data row11 col5\" >0.000000</td>\n",
       "      <td id=\"T_c53b7_row11_col6\" class=\"data row11 col6\" >60</td>\n",
       "      <td id=\"T_c53b7_row11_col7\" class=\"data row11 col7\" >128</td>\n",
       "      <td id=\"T_c53b7_row11_col8\" class=\"data row11 col8\" >0.15/0.15</td>\n",
       "      <td id=\"T_c53b7_row11_col9\" class=\"data row11 col9\" >0.000982</td>\n",
       "      <td id=\"T_c53b7_row11_col10\" class=\"data row11 col10\" >140</td>\n",
       "      <td id=\"T_c53b7_row11_col11\" class=\"data row11 col11\" >0.990000</td>\n",
       "      <td id=\"T_c53b7_row11_col12\" class=\"data row11 col12\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c53b7_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_c53b7_row12_col0\" class=\"data row12 col0\" >True</td>\n",
       "      <td id=\"T_c53b7_row12_col1\" class=\"data row12 col1\" >mlp_2_256_256_256_256_256_16_16</td>\n",
       "      <td id=\"T_c53b7_row12_col2\" class=\"data row12 col2\" >one hot</td>\n",
       "      <td id=\"T_c53b7_row12_col3\" class=\"data row12 col3\" >0.117758</td>\n",
       "      <td id=\"T_c53b7_row12_col4\" class=\"data row12 col4\" >mae</td>\n",
       "      <td id=\"T_c53b7_row12_col5\" class=\"data row12 col5\" >0.000000</td>\n",
       "      <td id=\"T_c53b7_row12_col6\" class=\"data row12 col6\" >60</td>\n",
       "      <td id=\"T_c53b7_row12_col7\" class=\"data row12 col7\" >128</td>\n",
       "      <td id=\"T_c53b7_row12_col8\" class=\"data row12 col8\" >0.15/0.15</td>\n",
       "      <td id=\"T_c53b7_row12_col9\" class=\"data row12 col9\" >0.000982</td>\n",
       "      <td id=\"T_c53b7_row12_col10\" class=\"data row12 col10\" >140</td>\n",
       "      <td id=\"T_c53b7_row12_col11\" class=\"data row12 col11\" >0.990000</td>\n",
       "      <td id=\"T_c53b7_row12_col12\" class=\"data row12 col12\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c53b7_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_c53b7_row13_col0\" class=\"data row13 col0\" >True</td>\n",
       "      <td id=\"T_c53b7_row13_col1\" class=\"data row13 col1\" >mlp_2_512_512_512_512_512_16_16</td>\n",
       "      <td id=\"T_c53b7_row13_col2\" class=\"data row13 col2\" >one hot</td>\n",
       "      <td id=\"T_c53b7_row13_col3\" class=\"data row13 col3\" >0.101876</td>\n",
       "      <td id=\"T_c53b7_row13_col4\" class=\"data row13 col4\" >mae</td>\n",
       "      <td id=\"T_c53b7_row13_col5\" class=\"data row13 col5\" >0.000000</td>\n",
       "      <td id=\"T_c53b7_row13_col6\" class=\"data row13 col6\" >60</td>\n",
       "      <td id=\"T_c53b7_row13_col7\" class=\"data row13 col7\" >128</td>\n",
       "      <td id=\"T_c53b7_row13_col8\" class=\"data row13 col8\" >0.15/0.15</td>\n",
       "      <td id=\"T_c53b7_row13_col9\" class=\"data row13 col9\" >0.000982</td>\n",
       "      <td id=\"T_c53b7_row13_col10\" class=\"data row13 col10\" >140</td>\n",
       "      <td id=\"T_c53b7_row13_col11\" class=\"data row13 col11\" >0.990000</td>\n",
       "      <td id=\"T_c53b7_row13_col12\" class=\"data row13 col12\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c53b7_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_c53b7_row14_col0\" class=\"data row14 col0\" >True</td>\n",
       "      <td id=\"T_c53b7_row14_col1\" class=\"data row14 col1\" >mlp_2_570_510_250_440_480_16_16</td>\n",
       "      <td id=\"T_c53b7_row14_col2\" class=\"data row14 col2\" >one hot</td>\n",
       "      <td id=\"T_c53b7_row14_col3\" class=\"data row14 col3\" >0.093319</td>\n",
       "      <td id=\"T_c53b7_row14_col4\" class=\"data row14 col4\" >mae</td>\n",
       "      <td id=\"T_c53b7_row14_col5\" class=\"data row14 col5\" >0.000000</td>\n",
       "      <td id=\"T_c53b7_row14_col6\" class=\"data row14 col6\" >60</td>\n",
       "      <td id=\"T_c53b7_row14_col7\" class=\"data row14 col7\" >128</td>\n",
       "      <td id=\"T_c53b7_row14_col8\" class=\"data row14 col8\" >0.15/0.15</td>\n",
       "      <td id=\"T_c53b7_row14_col9\" class=\"data row14 col9\" >0.000982</td>\n",
       "      <td id=\"T_c53b7_row14_col10\" class=\"data row14 col10\" >140</td>\n",
       "      <td id=\"T_c53b7_row14_col11\" class=\"data row14 col11\" >0.990000</td>\n",
       "      <td id=\"T_c53b7_row14_col12\" class=\"data row14 col12\" >False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f60c04b49d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import csv\n",
    "if 'Try Both' not in ENCODING_TYPE:\n",
    "    csv_data = [[\n",
    "        DATA_AUGMENTATION,\n",
    "        model_shape,\n",
    "        ENCODING_TYPE,\n",
    "        test_loss_result,\n",
    "        TRAIN_LOSS,\n",
    "        TRAIN_DROPOUT_RATE,\n",
    "        TRAIN_EARLY_STOPPING_PATIENCE,\n",
    "        TRAIN_BATCH_SIZE,\n",
    "        '0.15/0.15',\n",
    "        LR_INITIAL,\n",
    "        LR_DECAY_STEPS,\n",
    "        LR_DECAY_RATE,\n",
    "        LR_STAIRCASE\n",
    "        ]]\n",
    "    \n",
    "    csv_file = 'history_losses.csv'  #this doesnt reqrite this file so you need to delete this if you want something fresh\n",
    "    \n",
    "    if not os.path.exists(csv_file):\n",
    "        with open(csv_file, 'w') as file:\n",
    "            file.write('data_augmentation,model_shape,encoding_type,test_loss,train_loss,train_dropout_rate,train_early_stop_patience,'+\n",
    "                        'train_batch_size,train_val_split,lr_initial,lr_decay_step,lr_decay_rate,lr_stair_case\\n')\n",
    "    \n",
    "    with open(csv_file, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(csv_data)\n",
    "    \n",
    "    # Convert data to DataFrame for easier display\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    def color_red_column(s):\n",
    "        return ['color: red' if v else '' for v in s]\n",
    "    \n",
    "    styled_df = df.style.apply(color_red_column, subset=['test_loss'])\n",
    "    \n",
    "    # Display the DataFrame as a table\n",
    "    display(styled_df)\n",
    "    #qgrid_widget = qgrid.show_grid(df, show_toolbar=True)\n",
    "\n",
    "else:\n",
    "    #---------------------------------------------------one hot---------------------------------------\n",
    "    csv_data = [[\n",
    "        DATA_AUGMENTATION,\n",
    "        model_shape_one_hot_encoding,\n",
    "        'One Hot',\n",
    "        test_loss_result_one_hot_encoding,\n",
    "        TRAIN_LOSS,\n",
    "        TRAIN_DROPOUT_RATE,\n",
    "        TRAIN_EARLY_STOPPING_PATIENCE,\n",
    "        TRAIN_BATCH_SIZE,\n",
    "        '0.15/0.15',\n",
    "        LR_INITIAL,\n",
    "        LR_DECAY_STEPS,\n",
    "        LR_DECAY_RATE,\n",
    "        LR_STAIRCASE\n",
    "        ]]\n",
    "    \n",
    "    csv_file = 'history_losses.csv'  #this doesnt reqrite this file so you need to delete this if you want something fresh\n",
    "    \n",
    "    if not os.path.exists(csv_file):\n",
    "        with open(csv_file, 'w') as file:\n",
    "            file.write('data_augmentation,model_shape,encoding_type,test_loss,train_loss,train_dropout_rate,train_early_stop_patience,'+\n",
    "                        'train_batch_size,train_val_split,lr_initial,lr_decay_step,lr_decay_rate,lr_stair_case\\n')\n",
    "            \n",
    "    with open(csv_file, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(csv_data)\n",
    "    \n",
    "    # Convert data to DataFrame for easier display\n",
    "    df_one_hot_encoding = pd.read_csv(csv_file)\n",
    "    \n",
    "    def color_red_column(s):\n",
    "        return ['color: red' if v else '' for v in s]\n",
    "    \n",
    "    styled_df_one_hot_encoding = df_one_hot_encoding.style.apply(color_red_column, subset=['test_loss'])\n",
    "    #---------------------------------------------------linear---------------------------------------\n",
    "    csv_data_linear_encoding = [[\n",
    "        DATA_AUGMENTATION,\n",
    "        model_shape_linear_encoding,\n",
    "        'linear',\n",
    "        test_loss_result_linear_encoding,\n",
    "        TRAIN_LOSS,\n",
    "        TRAIN_DROPOUT_RATE,\n",
    "        TRAIN_EARLY_STOPPING_PATIENCE,\n",
    "        TRAIN_BATCH_SIZE,\n",
    "        '0.15/0.15',\n",
    "        LR_INITIAL,\n",
    "        LR_DECAY_STEPS,\n",
    "        LR_DECAY_RATE,\n",
    "        LR_STAIRCASE\n",
    "        ]]\n",
    "    \n",
    "    csv_file_linear_encoding = 'history_losses.csv'  #this doesnt reqrite this file so you need to delete this if you want something fresh\n",
    "    \n",
    "    with open(csv_file_linear_encoding, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(csv_data_linear_encoding)\n",
    "    \n",
    "    # Convert data to DataFrame for easier display\n",
    "    df_linear_encoding = pd.read_csv(csv_file_linear_encoding)\n",
    "    \n",
    "    def color_red_column(s):\n",
    "        return ['color: red' if v else '' for v in s]\n",
    "    \n",
    "    styled_df_linear_encoding = df_linear_encoding.style.apply(color_red_column, subset=['test_loss'])\n",
    "    \n",
    "    # Display the DataFrame as a table\n",
    "    display(styled_df_linear_encoding)\n",
    "    #qgrid_widget = qgrid.show_grid(df, show_toolbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9ea88851-ed24-42d4-b36c-e79175af6847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Decide which model file & test set to use\n",
    "if 'Try Both' not in ENCODING_TYPE:\n",
    "    chosen_path = best_model_file\n",
    "    X_test_cur = np.asarray(X_test)\n",
    "    y_value_test_cur = np.asarray(y_value_test)\n",
    "    y_exists_test_cur = np.asarray(y_exists_test)\n",
    "else:\n",
    "    if test_loss_result_linear_encoding < test_loss_result_one_hot_encoding:\n",
    "        chosen_path = best_model_file_linear\n",
    "        X_test_cur = np.asarray(X_test_linear_encoding)\n",
    "        y_value_test_cur = np.asarray(y_value_test_linear_encoding)\n",
    "        y_exists_test_cur = np.asarray(y_exists_test_linear_encoding)\n",
    "        y_encoding_format_name = 'linear'\n",
    "    else:\n",
    "        chosen_path = best_model_file_onehot\n",
    "        X_test_cur = np.asarray(X_test_one_hot_encoding)\n",
    "        y_value_test_cur = np.asarray(y_value_test_one_hot_encoding)\n",
    "        y_exists_test_cur = np.asarray(y_exists_test_one_hot_encoding)\n",
    "        y_encoding_format_name = 'one_hot'\n",
    "\n",
    "# clear everything again, then load & predict on one cpu device to avoid the memory thing again\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    chosen_model = load_model(chosen_path, compile=False)\n",
    "    pred = chosen_model.predict(X_test_cur, verbose=0)\n",
    "\n",
    "# Unpack model outputs into proper numeric arrays\n",
    "if isinstance(pred, dict):\n",
    "    y_value_pred = np.asarray(pred['value_out'])\n",
    "    y_exists_pred = np.asarray(pred['exists_out'])\n",
    "else:\n",
    "    y_value_pred, y_exists_pred = pred\n",
    "    y_value_pred = np.asarray(y_value_pred)\n",
    "    y_exists_pred = np.asarray(y_exists_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a6ffa127-c6a4-4a52-8a29-4c08ae0cca3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved CSV -> /home/olivias/ML_qubit_design/model_predict_cavity_claw_RouteMeander_eigenmode/predictions_and_errors_one_hot.csv\n",
      "\n",
      " Sample 0  X: cavity_frequency=0.0208384, kappa=0.16982\n",
      "                                                          param  exists      ref      pred  abs_error     sq_error\n",
      "   design_options.claw_opts.connection_pads.readout.claw_length     1.0 0.403941  0.409193   0.005252 2.758260e-05\n",
      "design_options.claw_opts.connection_pads.readout.ground_spacing     1.0 0.000000 -0.002957   0.002957 8.742485e-06\n",
      "                                 design_options.claw_opts.pos_x     1.0 0.000000 -0.005582   0.005582 3.115487e-05\n",
      "                          design_options.claw_opts.cross_length     1.0 0.000000  0.002558   0.002558 6.544872e-06\n",
      "                           design_options.claw_opts.cross_width     1.0 0.000000 -0.000420   0.000420 1.762846e-07\n",
      "                             design_options.claw_opts.cross_gap     1.0 0.000000 -0.000630   0.000630 3.969078e-07\n",
      "                           design_options.cpw_opts.total_length     1.0 0.428571  0.338296   0.090275 8.149627e-03\n",
      "                    design_options.cpw_opts.lead.start_straight     1.0 1.000000  0.987233   0.012767 1.630074e-04\n",
      "                      design_options.cpw_opts.meander.asymmetry     1.0 0.956522  0.889393   0.067129 4.506287e-03\n",
      "                       design_options.cplr_opts.coupling_length     1.0 0.700000  0.670904   0.029096 8.465506e-04\n",
      "                      design_options.cpw_opts.lead.end_straight     1.0 1.000000  0.997789   0.002211 4.889727e-06\n",
      "          design_options.cpw_opts.lead.start_jogged_extension.0     0.0 0.000000 -0.073937        NaN          NaN\n",
      "                                               coupler_type_CLT     1.0 1.000000  0.997852   0.002148 4.613018e-06\n",
      "                                              coupler_type_NCap     1.0 0.000000 -0.001914   0.001914 3.665229e-06\n",
      "                                            resonator_type_half     1.0 0.000000  0.004170   0.004170 1.738797e-05\n",
      "                                         resonator_type_quarter     1.0 1.000000  0.997970   0.002030 4.119267e-06\n",
      "\n",
      " Sample 1  X: cavity_frequency=0.147811, kappa=0.00035887\n",
      "                                                          param  exists      ref      pred  abs_error     sq_error\n",
      "   design_options.claw_opts.connection_pads.readout.claw_length     1.0 0.236453  0.128514   0.107939 1.165086e-02\n",
      "design_options.claw_opts.connection_pads.readout.ground_spacing     1.0 0.000000 -0.004580   0.004580 2.097878e-05\n",
      "                                 design_options.claw_opts.pos_x     1.0 0.000000 -0.003529   0.003529 1.245727e-05\n",
      "                          design_options.claw_opts.cross_length     1.0 1.000000  0.999734   0.000266 7.098642e-08\n",
      "                           design_options.claw_opts.cross_width     1.0 1.000000  0.995628   0.004372 1.911335e-05\n",
      "                             design_options.claw_opts.cross_gap     1.0 1.000000  0.994500   0.005500 3.025348e-05\n",
      "                           design_options.cpw_opts.total_length     1.0 0.714286  0.685780   0.028506 8.126036e-04\n",
      "                    design_options.cpw_opts.lead.start_straight     1.0 1.000000  0.995155   0.004845 2.347771e-05\n",
      "                      design_options.cpw_opts.meander.asymmetry     0.0 0.652174  0.598722        NaN          NaN\n",
      "                       design_options.cplr_opts.coupling_length     0.0 0.000000  0.378067        NaN          NaN\n",
      "                      design_options.cpw_opts.lead.end_straight     1.0 0.500000  0.499673   0.000327 1.070791e-07\n",
      "          design_options.cpw_opts.lead.start_jogged_extension.0     0.0 0.000000 -0.090683        NaN          NaN\n",
      "                                               coupler_type_CLT     1.0 0.000000  0.004356   0.004356 1.897445e-05\n",
      "                                              coupler_type_NCap     1.0 1.000000  0.994730   0.005270 2.777106e-05\n",
      "                                            resonator_type_half     1.0 1.000000  0.998182   0.001818 3.304912e-06\n",
      "                                         resonator_type_quarter     1.0 0.000000  0.003959   0.003959 1.567274e-05\n",
      "\n",
      " Sample 2  X: cavity_frequency=0.255023, kappa=0.00060377\n",
      "                                                          param  exists      ref      pred  abs_error     sq_error\n",
      "   design_options.claw_opts.connection_pads.readout.claw_length     1.0 0.019704  0.129605   0.109901 1.207814e-02\n",
      "design_options.claw_opts.connection_pads.readout.ground_spacing     1.0 0.000000 -0.004491   0.004491 2.016718e-05\n",
      "                                 design_options.claw_opts.pos_x     1.0 0.000000 -0.001706   0.001706 2.908804e-06\n",
      "                          design_options.claw_opts.cross_length     1.0 1.000000  1.001314   0.001314 1.727652e-06\n",
      "                           design_options.claw_opts.cross_width     1.0 1.000000  0.997063   0.002937 8.626430e-06\n",
      "                             design_options.claw_opts.cross_gap     1.0 1.000000  0.995804   0.004196 1.760932e-05\n",
      "                           design_options.cpw_opts.total_length     1.0 0.500000  0.493950   0.006050 3.659842e-05\n",
      "                    design_options.cpw_opts.lead.start_straight     1.0 1.000000  0.994166   0.005834 3.403258e-05\n",
      "                      design_options.cpw_opts.meander.asymmetry     0.0 0.652174  0.579884        NaN          NaN\n",
      "                       design_options.cplr_opts.coupling_length     0.0 0.000000  0.384524        NaN          NaN\n",
      "                      design_options.cpw_opts.lead.end_straight     1.0 0.500000  0.496742   0.003258 1.061741e-05\n",
      "          design_options.cpw_opts.lead.start_jogged_extension.0     0.0 0.000000 -0.094343        NaN          NaN\n",
      "                                               coupler_type_CLT     1.0 0.000000  0.000445   0.000445 1.976612e-07\n",
      "                                              coupler_type_NCap     1.0 1.000000  0.996556   0.003444 1.186166e-05\n",
      "                                            resonator_type_half     1.0 1.000000  0.999498   0.000502 2.516354e-07\n",
      "                                         resonator_type_quarter     1.0 0.000000 -0.000089   0.000089 8.004267e-09\n",
      "\n",
      "Global error stats (defined parameters only):\n",
      "  min abs_error: 5.960464477539063e-08\n",
      "  median abs_error: 0.0042505450546741486\n",
      "  max abs_error: 1.0033009187318385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\nHere onehot/linear encoding and the mlp which maps categorical data to 1s and 0s is probably \\nthrowing off the global average. These will be rounded in the future and will probably always \\nround to the right number to reconstruct the correct category-- but for now it might throw off \\nthe overall average error. In the future we might want to just have it consider the non categorical \\ndata when finding an overall average and reporting that number.\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now lets look at a specfic case to see how the model predicts things\n",
    "if 'Try Both' not in ENCODING_TYPE:\n",
    "    y_encoding_format_name = encoding\n",
    "    \n",
    "filename = f'y_characteristics_{y_encoding_format_name}_encoding.csv'\n",
    "with open(filename, 'r') as f:\n",
    "    headers = f.readline().strip().split(',')\n",
    "\n",
    "X_test_cur = np.asarray(X_test_cur)\n",
    "y_value_test_cur = np.asarray(y_value_test_cur)\n",
    "y_exists_test_cur = np.asarray(y_exists_test_cur)\n",
    "y_value_pred = np.asarray(y_value_pred)\n",
    "\n",
    "n_samples, n_params = y_value_test_cur.shape\n",
    "#change nsamples if you dont want to look at everything\n",
    "n_samples = 3\n",
    "\n",
    "# raw errors\n",
    "sq_errors = (y_value_test_cur - y_value_pred) ** 2\n",
    "abs_errors = np.abs(y_value_test_cur - y_value_pred)\n",
    "\n",
    "# mask out parameters that are \"not defined\" according to the ground-truth exists flag\n",
    "sq_errors_masked = np.where(y_exists_test_cur == 1.0, sq_errors, np.nan)\n",
    "abs_errors_masked = np.where(y_exists_test_cur == 1.0, abs_errors, np.nan)\n",
    "\n",
    "# make a nice dataframe so the output is comprehensible\n",
    "rows = []\n",
    "for i in range(n_samples):\n",
    "    cav_freq, kappa = X_test_cur[i, 0], X_test_cur[i, 1]\n",
    "    for j in range(n_params):\n",
    "        rows.append({\n",
    "            \"sample_idx\": i,\n",
    "            \"cavity_frequency\": cav_freq,\n",
    "            \"kappa\": kappa,\n",
    "            \"param\": headers[j],\n",
    "            \"exists\": y_exists_test_cur[i, j],\n",
    "            \"ref\": y_value_test_cur[i, j],\n",
    "            \"pred\": y_value_pred[i, j],\n",
    "            \"abs_error\": abs_errors_masked[i, j],\n",
    "            \"sq_error\": sq_errors_masked[i, j],\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "#save it incase we want to do stuff with this in the future\n",
    "out_csv = Path(f\"predictions_and_errors_{y_encoding_format_name}.csv\")\n",
    "df.to_csv(out_csv, index=False, float_format=\"%.6g\")\n",
    "print(f\"\\nSaved CSV -> {out_csv.resolve()}\\n\")\n",
    "\n",
    "# print it out nicely \n",
    "for i in range(n_samples):\n",
    "    sub = df[df[\"sample_idx\"] == i].copy()\n",
    "    sub = sub[[\"param\", \"exists\", \"ref\", \"pred\", \"abs_error\", \"sq_error\"]]\n",
    "    header_line = (\n",
    "        f\" Sample {i}  \"\n",
    "        f\"X: cavity_frequency={X_test_cur[i,0]:.6g}, kappa={X_test_cur[i,1]:.6g}\"\n",
    "    )\n",
    "    print(header_line)\n",
    "    print(sub.to_string(index=False))\n",
    "    print() \n",
    "\n",
    "# (Optional) quick global stats (only over defined parameters)\n",
    "print(\"Global error stats (defined parameters only):\")\n",
    "print(\"  min abs_error:\", float(np.nanmin(abs_errors_masked)))\n",
    "print(\"  median abs_error:\", float(np.nanmedian(abs_errors_masked)))\n",
    "print(\"  max abs_error:\", float(np.nanmax(abs_errors_masked)))\n",
    "\n",
    "''' \n",
    "Here onehot/linear encoding and the mlp which maps categorical data to 1s and 0s is probably \n",
    "throwing off the global average. These will be rounded in the future and will probably always \n",
    "round to the right number to reconstruct the correct category-- but for now it might throw off \n",
    "the overall average error. In the future we might want to just have it consider the non categorical \n",
    "data when finding an overall average and reporting that number.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5304c67f-9838-491f-869c-e51d86748217",
   "metadata": {},
   "source": [
    "### Unscaled test vs predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0e05f774-e63a-4325-86eb-c091a1c286d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved CSV -> /home/olivias/ML_qubit_design/model_predict_cavity_claw_RouteMeander_eigenmode/predictions_and_errors_unscaled_one_hot.csv\n",
      "\n",
      " Sample 0 (Unscaled)  X: cavity_frequency=5.24699e+09, kappa=168252\n",
      "                                                          param  exists  ref_unscaled  pred_unscaled  abs_error_unscaled  sq_error_unscaled\n",
      "   design_options.claw_opts.connection_pads.readout.claw_length     1.0      0.000275   2.776654e-04        2.665356e-06       7.104125e-12\n",
      "design_options.claw_opts.connection_pads.readout.ground_spacing     1.0      0.000004   4.082555e-06        1.744472e-08       3.043183e-16\n",
      "                                 design_options.claw_opts.pos_x     1.0     -0.001500  -1.502791e-03        2.790838e-06       7.788774e-12\n",
      "                          design_options.claw_opts.cross_length     1.0      0.000000   6.139907e-07        6.139907e-07       3.769846e-13\n",
      "                           design_options.claw_opts.cross_width     1.0      0.000000  -1.259588e-08        1.259588e-08       1.586561e-16\n",
      "                             design_options.claw_opts.cross_gap     1.0      0.000000  -1.890019e-08        1.890019e-08       3.572170e-16\n",
      "                           design_options.cpw_opts.total_length     1.0      0.004700   4.068073e-03        6.319271e-04       3.993319e-07\n",
      "                    design_options.cpw_opts.lead.start_straight     1.0      0.000100   9.936163e-05        6.383732e-07       4.075204e-13\n",
      "                      design_options.cpw_opts.meander.asymmetry     1.0      0.000117   9.093393e-05        2.573274e-05       6.621738e-10\n",
      "                       design_options.cplr_opts.coupling_length     1.0      0.000350   3.354522e-04        1.454777e-05       2.116375e-10\n",
      "                      design_options.cpw_opts.lead.end_straight     1.0      0.000100   9.977887e-05        2.211262e-07       4.889678e-14\n",
      "          design_options.cpw_opts.lead.start_jogged_extension.0     0.0      0.000000  -7.393702e-02                 NaN                NaN\n",
      "                                               coupler_type_CLT     1.0      1.000000   9.978522e-01        2.147794e-03       4.613018e-06\n",
      "                                              coupler_type_NCap     1.0      0.000000  -1.914479e-03        1.914479e-03       3.665229e-06\n",
      "                                            resonator_type_half     1.0      0.000000   4.169889e-03        4.169889e-03       1.738797e-05\n",
      "                                         resonator_type_quarter     1.0      1.000000   9.979704e-01        2.029598e-03       4.119267e-06\n",
      "\n",
      " Sample 1 (Unscaled)  X: cavity_frequency=8.36575e+09, kappa=768.707\n",
      "                                                          param  exists  ref_unscaled  pred_unscaled  abs_error_unscaled  sq_error_unscaled\n",
      "   design_options.claw_opts.connection_pads.readout.claw_length     1.0      0.000190       0.000135        5.477911e-05       3.000751e-09\n",
      "design_options.claw_opts.connection_pads.readout.ground_spacing     1.0      0.000004       0.000004        2.702352e-08       7.302706e-16\n",
      "                                 design_options.claw_opts.pos_x     1.0     -0.001500      -0.001502        1.764753e-06       3.114353e-12\n",
      "                          design_options.claw_opts.cross_length     1.0      0.000240       0.000240        6.394718e-08       4.089242e-15\n",
      "                           design_options.claw_opts.cross_width     1.0      0.000030       0.000030        1.311572e-07       1.720220e-14\n",
      "                             design_options.claw_opts.cross_gap     1.0      0.000030       0.000030        1.650104e-07       2.722843e-14\n",
      "                           design_options.cpw_opts.total_length     1.0      0.006700       0.006500        1.995435e-04       3.981762e-08\n",
      "                    design_options.cpw_opts.lead.start_straight     1.0      0.000100       0.000100        2.422701e-07       5.869480e-14\n",
      "                      design_options.cpw_opts.meander.asymmetry     0.0      0.000000      -0.000020                 NaN                NaN\n",
      "                       design_options.cplr_opts.coupling_length     0.0      0.000000       0.000189                 NaN                NaN\n",
      "                      design_options.cpw_opts.lead.end_straight     1.0      0.000050       0.000050        3.272124e-08       1.070680e-15\n",
      "          design_options.cpw_opts.lead.start_jogged_extension.0     0.0      0.000000      -0.090683                 NaN                NaN\n",
      "                                               coupler_type_CLT     1.0      0.000000       0.004356        4.355967e-03       1.897445e-05\n",
      "                                              coupler_type_NCap     1.0      1.000000       0.994730        5.269825e-03       2.777106e-05\n",
      "                                            resonator_type_half     1.0      1.000000       0.998182        1.817942e-03       3.304912e-06\n",
      "                                         resonator_type_quarter     1.0      0.000000       0.003959        3.958881e-03       1.567274e-05\n",
      "\n",
      " Sample 2 (Unscaled)  X: cavity_frequency=1.09991e+10, kappa=1010.75\n",
      "                                                          param  exists  ref_unscaled  pred_unscaled  abs_error_unscaled  sq_error_unscaled\n",
      "   design_options.claw_opts.connection_pads.readout.claw_length     1.0      0.000080       0.000136        5.577455e-05       3.110800e-09\n",
      "design_options.claw_opts.connection_pads.readout.ground_spacing     1.0      0.000004       0.000004        2.649556e-08       7.020146e-16\n",
      "                                 design_options.claw_opts.pos_x     1.0     -0.001500      -0.001501        8.527553e-07       7.271916e-13\n",
      "                          design_options.claw_opts.cross_length     1.0      0.000240       0.000240        3.154504e-07       9.950893e-14\n",
      "                           design_options.claw_opts.cross_width     1.0      0.000030       0.000030        8.811260e-08       7.763831e-15\n",
      "                             design_options.claw_opts.cross_gap     1.0      0.000030       0.000030        1.258912e-07       1.584859e-14\n",
      "                           design_options.cpw_opts.total_length     1.0      0.005200       0.005158        4.234769e-05       1.793327e-09\n",
      "                    design_options.cpw_opts.lead.start_straight     1.0      0.000100       0.000100        2.916884e-07       8.508212e-14\n",
      "                      design_options.cpw_opts.meander.asymmetry     0.0      0.000000      -0.000028                 NaN                NaN\n",
      "                       design_options.cplr_opts.coupling_length     0.0      0.000000       0.000192                 NaN                NaN\n",
      "                      design_options.cpw_opts.lead.end_straight     1.0      0.000050       0.000050        3.258441e-07       1.061744e-13\n",
      "          design_options.cpw_opts.lead.start_jogged_extension.0     0.0      0.000000      -0.094343                 NaN                NaN\n",
      "                                               coupler_type_CLT     1.0      0.000000       0.000445        4.445910e-04       1.976612e-07\n",
      "                                              coupler_type_NCap     1.0      1.000000       0.996556        3.444076e-03       1.186166e-05\n",
      "                                            resonator_type_half     1.0      1.000000       0.999498        5.016327e-04       2.516354e-07\n",
      "                                         resonator_type_quarter     1.0      0.000000      -0.000089        8.946657e-05       8.004267e-09\n",
      "\n",
      "Global unscaled error stats (defined parameters only):\n",
      "  min abs_error: 6.034970440049747e-11\n",
      "  median abs_error: 1.608349854592234e-05\n",
      "  max abs_error: 0.04484760761260986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nHere onehot/linear encoding and the MLP which maps categorical data to 1s and 0s is probably \\nthrowing off the global average. These will be rounded in the future and will probably always \\nround to the right number to reconstruct the correct category-- but for now it might throw off \\nthe overall average error. In the future we might want to just have it consider the non-categorical \\ndata when finding an overall average and reporting that number.\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unscale everything and look at errors again. \n",
    "#You can compare the unscaled actual values to the ml_00...py notebook to convice yourself that unscaling worked\n",
    "\n",
    "with open('X_names', 'r') as f:\n",
    "    X_index_names = f.read().splitlines()\n",
    "\n",
    "# unscaling x\n",
    "X_test_unscaled = np.asarray(X_test_cur.copy())\n",
    "for i in range(X_test_unscaled.shape[0]):\n",
    "    for j in range(X_test_unscaled.shape[1]):\n",
    "        scaler = joblib.load(f'scalers/scaler_X_{X_index_names[j]}.save')\n",
    "        X_test_unscaled[i, j] = scaler.inverse_transform([[X_test_unscaled[i, j]]])[0][0]\n",
    "\n",
    "# unscaling y (value head only)\n",
    "y_value_test_unscaled = np.asarray(y_value_test_cur.copy())\n",
    "for i in range(y_value_test_unscaled.shape[0]):\n",
    "    for j in range(y_value_test_unscaled.shape[1]):\n",
    "        scaler = joblib.load(f'scalers/scaler_y_value__{headers[j]}_{y_encoding_format_name}_encoding.save')\n",
    "        y_value_test_unscaled[i, j] = scaler.inverse_transform([[y_value_test_unscaled[i, j]]])[0][0]\n",
    "\n",
    "# unscaling y predictions (value head only)\n",
    "y_value_pred_unscaled = np.asarray(y_value_pred.copy())\n",
    "for i in range(y_value_pred_unscaled.shape[0]):\n",
    "    for j in range(y_value_pred_unscaled.shape[1]):\n",
    "        scaler = joblib.load(f'scalers/scaler_y_value__{headers[j]}_{y_encoding_format_name}_encoding.save')\n",
    "        y_value_pred_unscaled[i, j] = scaler.inverse_transform([[y_value_pred_unscaled[i, j]]])[0][0]\n",
    "\n",
    "n_samples, n_params = y_value_test_unscaled.shape\n",
    "n_samples = 3 \n",
    "\n",
    "# find how good or bad we did (the errors)\n",
    "sq_errors_unscaled = (y_value_test_unscaled - y_value_pred_unscaled) ** 2\n",
    "abs_errors_unscaled = np.abs(y_value_test_unscaled - y_value_pred_unscaled)\n",
    "\n",
    "# mask out parameters that are not defined according to ground-truth exists flag\n",
    "sq_errors_unscaled_masked = np.where(y_exists_test_cur == 1.0, sq_errors_unscaled, np.nan)\n",
    "abs_errors_unscaled_masked = np.where(y_exists_test_cur == 1.0, abs_errors_unscaled, np.nan)\n",
    "\n",
    "# making a nice fancy dataframe, we like fancy things\n",
    "rows_unscaled = []\n",
    "for i in range(n_samples):\n",
    "    cav_freq, kappa = X_test_unscaled[i, 0], X_test_unscaled[i, 1]\n",
    "    for j in range(n_params):\n",
    "        rows_unscaled.append({\n",
    "            \"sample_idx\": i,\n",
    "            \"cavity_frequency\": cav_freq,\n",
    "            \"kappa\": kappa,\n",
    "            \"param\": headers[j],\n",
    "            \"exists\": y_exists_test_cur[i, j],\n",
    "            \"ref_unscaled\": y_value_test_unscaled[i, j],\n",
    "            \"pred_unscaled\": y_value_pred_unscaled[i, j],\n",
    "            \"abs_error_unscaled\": abs_errors_unscaled_masked[i, j],\n",
    "            \"sq_error_unscaled\": sq_errors_unscaled_masked[i, j],\n",
    "        })\n",
    "\n",
    "df_unscaled = pd.DataFrame(rows_unscaled)\n",
    "\n",
    "# save csv of unscaled results uncase we lose this notebook due to github blowing up, ya never know\n",
    "out_csv_unscaled = Path(f\"predictions_and_errors_unscaled_{y_encoding_format_name}.csv\")\n",
    "df_unscaled.to_csv(out_csv_unscaled, index=False, float_format=\"%.6g\")\n",
    "print(f\"\\nSaved CSV -> {out_csv_unscaled.resolve()}\\n\")\n",
    "\n",
    "# print out stuff so you can see it here if you are to lazy like me to open a csv\n",
    "for i in range(n_samples):\n",
    "    sub = df_unscaled[df_unscaled[\"sample_idx\"] == i].copy()\n",
    "    sub = sub[[\"param\", \"exists\", \"ref_unscaled\", \"pred_unscaled\", \"abs_error_unscaled\", \"sq_error_unscaled\"]]\n",
    "    header_line = (\n",
    "        f\" Sample {i} (Unscaled)  \"\n",
    "        f\"X: cavity_frequency={X_test_unscaled[i,0]:.6g}, kappa={X_test_unscaled[i,1]:.6g}\"\n",
    "    )\n",
    "    print(header_line)\n",
    "    print(sub.to_string(index=False))\n",
    "    print()\n",
    "\n",
    "# look at overall stats, see below comment for a caviat \n",
    "print(\"Global unscaled error stats (defined parameters only):\")\n",
    "print(\"  min abs_error:\", float(np.nanmin(abs_errors_unscaled_masked)))\n",
    "print(\"  median abs_error:\", float(np.nanmedian(abs_errors_unscaled_masked)))\n",
    "print(\"  max abs_error:\", float(np.nanmax(abs_errors_unscaled_masked)))\n",
    "\n",
    "'''\n",
    "Here onehot/linear encoding and the MLP which maps categorical data to 1s and 0s is probably \n",
    "throwing off the global average. These will be rounded in the future and will probably always \n",
    "round to the right number to reconstruct the correct category-- but for now it might throw off \n",
    "the overall average error. In the future we might want to just have it consider the non-categorical \n",
    "data when finding an overall average and reporting that number.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2da89f-f7e8-4672-b054-bcf439f5b7b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a76783-7ab4-4458-b944-4ddb2526ce61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f91622-6756-4bdb-b440-2a0b844fa80e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b20419-4bce-427c-afe3-58004c5c0439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd6f856-dfb6-4216-a01d-11ec59931913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121b4acc-f517-4f14-95be-46d0c00c78f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb72d605-bb06-44e7-ba56-d920a7716ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81094893-d64f-4900-952a-c3c5ea8b7f5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508d4152-ebf4-4922-ab41-8fbfb1bc15be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
